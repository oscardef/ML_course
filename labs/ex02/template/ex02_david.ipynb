{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from helpers import *\n",
    "\n",
    "height, weight, gender = load_data(sub_sample=False, add_outlier=False)\n",
    "x, mean_x, std_x = standardize(height)\n",
    "y, tx = build_model_data(x, weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000,), (10000, 2))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape, tx.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NB: throughout this laboratory the data has the following format: \n",
    "  * there are **N = 10000** data entries\n",
    "  * **y** represents the column vector containing weight information -- that which we wish to predict/the output (see also the first page of $\\texttt{exercise02.pdf}$). Its **shape** is **(N,)**.\n",
    "  * **tx** represents the matrix $\\tilde{X}$ formed by laterally concatenating a column vector of 1s to the column vector of height information -- the input data (see also the first page of $\\texttt{exercise02.pdf}$). Its **shape** is **(N,2)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warm-up Questions\n",
    "\n",
    "### What does each column of X represent?\n",
    "- The first column is a vector of ones (corresponding to the intercept term, w0​).\n",
    "- The second column contains the feature values (in this case, heights xn1xn1​).\n",
    "\n",
    "### What does each row of X represent?\n",
    "- Each row represents the input features for a single data point. For a person nn, the row will contain:\n",
    "        [1,xn1], [1,xn1​] where xn1xn1​ is the height of person nn.\n",
    "\n",
    "### Why do we have 1’s in X?\n",
    "- The ones are there to account for the bias term w0w in the linear model. Without this, the model would be forced through the origin, which is rarely desirable.\n",
    "\n",
    "### If we have heights and weights of 3 people, what would be the size of yy and X? What would X_{3,2} represent?\n",
    "        The size of y would be 3×1 (a column vector).\n",
    "        The size of X would be 3×2. Each row contains a \"1\" and a height value.\n",
    "        X_{3,2} represents the second feature (height) of the third person."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Computing the Cost Function\n",
    "Fill in the `compute_loss` function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(y, tx, w):\n",
    "    \"\"\"Calculate the loss using either MSE or MAE.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        w: numpy array of shape=(2,). The vector of model parameters.\n",
    "\n",
    "    Returns:\n",
    "        the value of the loss (a scalar), corresponding to the input parameters w.\n",
    "    \"\"\"\n",
    "    N = y.shape[0]\n",
    "    error = y - np.dot(tx,w)\n",
    "    mse = (1 / (2 * N)) * np.dot(error.T, error)\n",
    "    return mse\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2694.483365887078\n"
     ]
    }
   ],
   "source": [
    "params = np.array([1,2])\n",
    "\n",
    "loss = compute_loss(y,tx,params)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in the function `grid_search()` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from costs import *\n",
    "\n",
    "\n",
    "def grid_search(y, tx, grid_w0, grid_w1):\n",
    "    \"\"\"Algorithm for grid search.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        grid_w0: numpy array of shape=(num_grid_pts_w0, ). A 1D array containing num_grid_pts_w0 values of parameter w0 to be tested in the grid search.\n",
    "        grid_w1: numpy array of shape=(num_grid_pts_w1, ). A 1D array containing num_grid_pts_w1 values of parameter w1 to be tested in the grid search.\n",
    "\n",
    "    Returns:\n",
    "        losses: numpy array of shape=(num_grid_pts_w0, num_grid_pts_w1). A 2D array containing the loss value for each combination of w0 and w1\n",
    "    \"\"\"\n",
    "    losses = np.zeros((len(grid_w0), len(grid_w1)))\n",
    "    for i, w0 in enumerate(grid_w0):\n",
    "        for j, w1 in enumerate(grid_w1):\n",
    "            w = np.array([w0,w1])\n",
    "            loss = compute_loss(y,tx,w)\n",
    "            losses[i,j] = loss\n",
    "            return losses\n",
    "            raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us play with the grid search demo now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid Search: loss*=0.0, w0*=-100.0, w1*=-116.66666666666666, execution time=0.004 seconds\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from grid_search import generate_w, get_best_parameters\n",
    "from plots import grid_visualization\n",
    "\n",
    "# Generate the grid of parameters to be swept\n",
    "grid_w0, grid_w1 = generate_w(num_intervals=10)\n",
    "\n",
    "# Start the grid search\n",
    "start_time = datetime.datetime.now()\n",
    "grid_losses = grid_search(y, tx, grid_w0, grid_w1)\n",
    "\n",
    "# Select the best combinaison\n",
    "loss_star, w0_star, w1_star = get_best_parameters(grid_w0, grid_w1, grid_losses)\n",
    "end_time = datetime.datetime.now()\n",
    "execution_time = (end_time - start_time).total_seconds()\n",
    "\n",
    "# Print the results\n",
    "print(\n",
    "    \"Grid Search: loss*={l}, w0*={w0}, w1*={w1}, execution time={t:.3f} seconds\".format(\n",
    "        l=loss_star, w0=w0_star, w1=w1_star, t=execution_time\n",
    "    )\n",
    ")\n",
    "\n",
    "# Plot the results\n",
    "fig = grid_visualization(grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight)\n",
    "fig.set_size_inches(10.0, 6.0)\n",
    "fig.savefig(\"grid_plot\")  # Optional saving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, please fill in the functions `compute_gradient` below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please fill in the functions `gradient_descent` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(y, tx, w):\n",
    "    \"\"\"Computes the gradient at w.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        w: numpy array of shape=(2, ). The vector of model parameters.\n",
    "\n",
    "    Returns:\n",
    "        An numpy array of shape (2, ) (same shape as w), containing the gradient of the loss at w.\n",
    "    \"\"\"\n",
    "    N = y.shape[0]\n",
    "    error = y - np.dot(tx,w)\n",
    "    gradient = -1/N*np.dot(tx.T,error)\n",
    "    return gradient\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[636.5 895. ]\n",
      "[316.5 445. ]\n"
     ]
    }
   ],
   "source": [
    "y_test = np.array([1,2])\n",
    "tx_test = np.array([[1,2],[3,4]])\n",
    "w_test = np.array([100,20])\n",
    "w2_test = np.array([50,10])\n",
    "\n",
    "gradient_test = compute_gradient(y_test,tx_test,w_test)\n",
    "gradient_test2 = compute_gradient(y_test,tx_test, w2_test)\n",
    "\n",
    "print(gradient_test)\n",
    "print(gradient_test2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What the values of the gradients tell us:\n",
    "\n",
    "### Magnitude of the gradients:\n",
    "\n",
    "- Far from the minimum (i.e., for w0=100, w1=20), the norm of the gradient will likely be larger. This indicates that the model parameters are far from the optimal solution, and the function is changing rapidly in that region. The larger gradient means the algorithm should take bigger steps to move closer to the optimal values.\n",
    "\n",
    "- Closer to the minimum (i.e., for w0=50, w1=10), the norm of the gradient is likely to be smaller. This means the model parameters are already somewhat close to the optimal solution, and the function is changing more slowly. The smaller gradient suggests the algorithm should take smaller steps as it approaches the minimum.\n",
    "\n",
    "### Bigger gradient near larger values of w0​ and w1​:\n",
    "\n",
    "- The gradient will be larger when the values of w0w0​ and w1w1​ are far from the minimum (i.e., when we are farther from the optimal solution).\n",
    "- The bigger gradient at w0=100 and w1=20 indicates that the model is far from the optimal weights. The gradient suggests a faster change is needed to adjust the parameters toward the minimum.\n",
    "\n",
    "### Smaller gradient near optimal values:\n",
    "- When w0=50 and w1=10, the gradient will be smaller because the model is closer to the minimum. This smaller gradient suggests that the step size should be smaller since the model is already approaching the solution.\n",
    "- The smaller gradient means we are in a flatter region of the loss function, so the weights should be updated more conservatively to avoid overshooting the minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"The Gradient Descent (GD) algorithm.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        initial_w: numpy array of shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        max_iters: a scalar denoting the total number of iterations of GD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "\n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of GD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of GD\n",
    "    \"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        gradient = compute_gradient(y,tx,w)\n",
    "        loss = compute_loss(y,tx,w)\n",
    "        w = w - gamma * gradient\n",
    "\n",
    "\n",
    "        # store w and loss\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        print(\n",
    "            \"GD iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}\".format(\n",
    "                bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your gradient descent function through gradient descent demo shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD iter. 0/49: loss=1062606.4462798787, w0=-892.6706077997898, w1=901.3479712434981\n",
      "GD iter. 1/49: loss=860714.1448053957, w0=-796.0741548196002, w1=812.561145362647\n",
      "GD iter. 2/49: loss=697181.3806110647, w0=-709.1373471374294, w1=732.6530020698812\n",
      "GD iter. 3/49: loss=564719.8416136571, w0=-630.8942202234758, w1=660.7356731063916\n",
      "GD iter. 4/49: loss=457425.9950257569, w0=-560.4754060009178, w1=596.0100770392513\n",
      "GD iter. 5/49: loss=370517.9792895593, w0=-497.09847320061544, w1=537.7570405788249\n",
      "GD iter. 6/49: loss=300122.48654323723, w0=-440.05923368034337, w1=485.3293077644413\n",
      "GD iter. 7/49: loss=243102.13741871712, w0=-388.7239181120987, w1=438.14434823149577\n",
      "GD iter. 8/49: loss=196915.65462785584, w0=-342.52213410067833, w1=395.67788465184515\n",
      "GD iter. 9/49: loss=159504.60356725854, w0=-300.94052849039974, w1=357.4580674301594\n",
      "GD iter. 10/49: loss=129201.65220817414, w0=-263.51708344114905, w1=323.0602319306422\n",
      "GD iter. 11/49: loss=104656.26160731615, w0=-229.83598289682348, w1=292.1021799810769\n",
      "GD iter. 12/49: loss=84774.49522062119, w0=-199.5229924069306, w1=264.2399332264682\n",
      "GD iter. 13/49: loss=68670.26444739822, w0=-172.2413009660271, w1=239.16391114732028\n",
      "GD iter. 14/49: loss=55625.83752108776, w0=-147.68777866921388, w1=216.5954912760871\n",
      "GD iter. 15/49: loss=45059.85171077603, w0=-125.58960860208197, w1=196.28391339197728\n",
      "GD iter. 16/49: loss=36501.4032044237, w0=-105.70125554166322, w1=178.00349329627844\n",
      "GD iter. 17/49: loss=29569.059914278198, w0=-87.80173778728633, w1=161.55111521014945\n",
      "GD iter. 18/49: loss=23953.861849260444, w0=-71.69217180834715, w1=146.74397493263334\n",
      "GD iter. 19/49: loss=19405.551416596038, w0=-57.193562427301906, w1=133.4175486828689\n",
      "GD iter. 20/49: loss=15721.419966137863, w0=-44.144813984361214, w1=121.42376505808089\n",
      "GD iter. 21/49: loss=12737.273491266713, w0=-32.400940385714485, w1=110.62935979577168\n",
      "GD iter. 22/49: loss=10320.114846621136, w0=-21.831454146932508, w1=100.9143950596934\n",
      "GD iter. 23/49: loss=8362.216344458187, w0=-12.318916532028721, w1=92.17092679722296\n",
      "GD iter. 24/49: loss=6776.318557706203, w0=-3.7576326786153516, w1=84.30180536099955\n",
      "GD iter. 25/49: loss=5491.741350437089, w0=3.9475227894567073, w1=77.21959606839849\n",
      "GD iter. 26/49: loss=4451.233812549132, w0=10.882162710721552, w1=70.84560770505753\n",
      "GD iter. 27/49: loss=3608.422706859872, w0=17.123338639859917, w1=65.10901817805068\n",
      "GD iter. 28/49: loss=2925.7457112515704, w0=22.740396976084458, w1=59.94608760374451\n",
      "GD iter. 29/49: loss=2372.777344808852, w0=27.79574947868653, w1=55.299450086868966\n",
      "GD iter. 30/49: loss=1924.8729679902428, w0=32.3455667310284, w1=51.11747632168097\n",
      "GD iter. 31/49: loss=1562.0704227671758, w0=36.4404022581361, w1=47.353699933011775\n",
      "GD iter. 32/49: loss=1268.200361136489, w0=40.12575423253302, w1=43.966301183209495\n",
      "GD iter. 33/49: loss=1030.165611215635, w0=43.442571009490244, w1=40.91764230838744\n",
      "GD iter. 34/49: loss=837.3574637797415, w0=46.42770610875173, w1=38.1738493210476\n",
      "GD iter. 35/49: loss=681.1828643566685, w0=49.11432769808709, w1=35.704435632441744\n",
      "GD iter. 36/49: loss=554.681438823978, w0=51.5322871284889, w1=33.48196331269647\n",
      "GD iter. 37/49: loss=452.2152841424998, w0=53.70845061585052, w1=31.481738224925728\n",
      "GD iter. 38/49: loss=369.2176988505025, w0=55.66699775447599, w1=29.681535645932055\n",
      "GD iter. 39/49: loss=301.9896547639847, w0=57.42969017923892, w1=28.061353324837754\n",
      "GD iter. 40/49: loss=247.5349390539046, w0=59.01611336152555, w1=26.60318923585288\n",
      "GD iter. 41/49: loss=203.42661932874077, w0=60.44389422558352, w1=25.290841555766495\n",
      "GD iter. 42/49: loss=167.6988803513573, w0=61.72889700323569, w1=24.10972864368875\n",
      "GD iter. 43/49: loss=138.75941177967704, w0=62.88539950312264, w1=23.04672702281878\n",
      "GD iter. 44/49: loss=115.31844223661591, w0=63.92625175302089, w1=22.090025564035805\n",
      "GD iter. 45/49: loss=96.33125690673643, w0=64.86301877792933, w1=21.22899425113113\n",
      "GD iter. 46/49: loss=80.9516367895341, w0=65.70610910034691, w1=20.45406606951692\n",
      "GD iter. 47/49: loss=68.49414449460018, w0=66.46489039052274, w1=19.75663070606413\n",
      "GD iter. 48/49: loss=58.40357573570374, w0=67.14779355168099, w1=19.12893887895662\n",
      "GD iter. 49/49: loss=50.23021504099761, w0=67.7624063967234, w1=18.564016234559862\n",
      "GD: execution time=0.018 seconds\n"
     ]
    }
   ],
   "source": [
    "# from gradient_descent import *\n",
    "from plots import gradient_descent_visualization\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([-1000, 1000])\n",
    "\n",
    "# Start gradient descent.\n",
    "start_time = datetime.datetime.now()\n",
    "gd_losses, gd_ws = gradient_descent(y, tx, w_initial, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "\n",
    "print(\"GD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf63b49fe7984c48ada90dee101a2149",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=51, min=1), Output()), _dom_classes=('widge…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time Visualization\n",
    "from ipywidgets import IntSlider, interact\n",
    "\n",
    "\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        gd_losses,\n",
    "        gd_ws,\n",
    "        grid_losses,\n",
    "        grid_w0,\n",
    "        grid_w1,\n",
    "        mean_x,\n",
    "        std_x,\n",
    "        height,\n",
    "        weight,\n",
    "        n_iter,\n",
    "    )\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(gd_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# 4. Stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_stoch_gradient(y, tx, w):\n",
    "    \"\"\"Compute a stochastic gradient at w from a data sample batch of size B, where B < N, and their corresponding labels.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(B, )\n",
    "        tx: numpy array of shape=(B,2)\n",
    "        w: numpy array of shape=(2, ). The vector of model parameters.\n",
    "\n",
    "    Returns:\n",
    "        A numpy array of shape (2, ) (same shape as w), containing the stochastic gradient of the loss at w.\n",
    "    \"\"\"\n",
    "\n",
    "    N = y.shape[0]\n",
    "    error = y - np.dot(tx,w)\n",
    "    gradient = -1/N*np.dot(tx.T,error)\n",
    "    return gradient\n",
    "    raise NotImplementedError\n",
    "\n",
    "\n",
    "def stochastic_gradient_descent(y, tx, initial_w, batch_size, max_iters, gamma):\n",
    "    \"\"\"The Stochastic Gradient Descent algorithm (SGD).\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        initial_w: numpy array of shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        batch_size: a scalar denoting the number of data points in a mini-batch used for computing the stochastic gradient\n",
    "        max_iters: a scalar denoting the total number of iterations of SGD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "\n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of SGD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of SGD\n",
    "    \"\"\"\n",
    "\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "\n",
    "    for n_iter in range(max_iters):\n",
    "        for minibatch_y, minibatch_tx in batch_iter(y, tx, batch_size):\n",
    "            sg = compute_stoch_gradient(minibatch_y, minibatch_tx,w)\n",
    "            loss = compute_loss(minibatch_y, minibatch_tx,w)\n",
    "    \n",
    "            w = w - gamma*sg\n",
    "    \n",
    "            # store w and loss\n",
    "            ws.append(w)\n",
    "            losses.append(loss)\n",
    "            \n",
    "\n",
    "        print(\n",
    "            \"SGD iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}\".format(\n",
    "                bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]\n",
    "            )\n",
    "        )\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD iter. 0/49: loss=2696.5521454147306, w0=7.343775793710932, w1=0.25833472932393964\n",
      "SGD iter. 1/49: loss=3357.1509970854045, w0=15.537860241756857, w1=11.123814285068107\n",
      "SGD iter. 2/49: loss=1877.906491629812, w0=21.66632904175872, w1=9.982539963636393\n",
      "SGD iter. 3/49: loss=1028.3667128056877, w0=26.20145133957096, w1=4.1935372648472935\n",
      "SGD iter. 4/49: loss=1510.9060967288879, w0=31.698552569726516, w1=7.162865896518815\n",
      "SGD iter. 5/49: loss=469.6848386187453, w0=34.76346639761384, w1=4.195819276412648\n",
      "SGD iter. 6/49: loss=403.9762776593771, w0=37.605917022196, w1=0.6606944037800639\n",
      "SGD iter. 7/49: loss=662.5911022689701, w0=41.246222235413626, w1=2.8472164436011256\n",
      "SGD iter. 8/49: loss=267.65950854149537, w0=43.55991844944647, w1=2.1987520787853247\n",
      "SGD iter. 9/49: loss=664.5460449003441, w0=47.20558997853502, w1=3.449534595781083\n",
      "SGD iter. 10/49: loss=581.1245325331492, w0=50.614767394080154, w1=3.1022551329416594\n",
      "SGD iter. 11/49: loss=543.8451405400884, w0=53.91278237445877, w1=2.7841444772041406\n",
      "SGD iter. 12/49: loss=377.4903109661044, w0=56.66047344503459, w1=5.614193371945401\n",
      "SGD iter. 13/49: loss=334.5154393238003, w0=59.24703656749575, w1=9.441618173379846\n",
      "SGD iter. 14/49: loss=36.899684479583485, w0=60.106102158467515, w1=8.916226315044625\n",
      "SGD iter. 15/49: loss=0.002058046716938811, w0=60.1125178372577, w1=8.920968270936688\n",
      "SGD iter. 16/49: loss=181.77791806473704, w0=62.01923185915368, w1=10.734091199096103\n",
      "SGD iter. 17/49: loss=146.4101714616639, w0=63.73043127162239, w1=12.174569177745411\n",
      "SGD iter. 18/49: loss=183.50315225275662, w0=65.64617213224398, w1=11.52178467682035\n",
      "SGD iter. 19/49: loss=258.3858368389454, w0=67.91943338796865, w1=11.654491700819548\n",
      "SGD iter. 20/49: loss=42.912621839686395, w0=68.84585253543578, w1=11.196256568925161\n",
      "SGD iter. 21/49: loss=177.64382302448502, w0=70.73076008237004, w1=11.778593411492437\n",
      "SGD iter. 22/49: loss=38.24162484753147, w0=71.60530710623476, w1=11.522249985714327\n",
      "SGD iter. 23/49: loss=21.64660182901254, w0=72.2632828174736, w1=12.3240149050831\n",
      "SGD iter. 24/49: loss=7.085008478703767, w0=72.63971364720739, w1=12.150262942306101\n",
      "SGD iter. 25/49: loss=22.451341146161838, w0=73.30980828449621, w1=12.458718386603099\n",
      "SGD iter. 26/49: loss=3.1725774192356835, w0=73.05791237665983, w1=12.400168395675093\n",
      "SGD iter. 27/49: loss=3.0299188367281595, w0=72.81174500121263, w1=12.67615353046565\n",
      "SGD iter. 28/49: loss=73.72129023359071, w0=74.02600436587564, w1=12.882747311332686\n",
      "SGD iter. 29/49: loss=14.224971726610494, w0=73.49261948182641, w1=11.77734903619394\n",
      "SGD iter. 30/49: loss=0.32885784915806116, w0=73.57371915496485, w1=11.84072650316717\n",
      "SGD iter. 31/49: loss=8.157539344600893, w0=73.97763843760931, w1=12.089120382102925\n",
      "SGD iter. 32/49: loss=22.342094078360546, w0=73.30917611139549, w1=12.25772903128415\n",
      "SGD iter. 33/49: loss=7.429074713481285, w0=73.69463881292396, w1=12.585775656131567\n",
      "SGD iter. 34/49: loss=15.2423097324341, w0=74.24676759740153, w1=11.58826825253617\n",
      "SGD iter. 35/49: loss=11.86753069239352, w0=73.75958117122786, w1=11.559450267987662\n",
      "SGD iter. 36/49: loss=3.050922884932758, w0=74.00660031562492, w1=11.566438190695825\n",
      "SGD iter. 37/49: loss=15.496673933706282, w0=74.56331701078874, w1=12.203792760877587\n",
      "SGD iter. 38/49: loss=17.173811961632616, w0=75.14938547029894, w1=11.940547915390365\n",
      "SGD iter. 39/49: loss=7.2580548692626055, w0=74.76838534249679, w1=12.252498278600305\n",
      "SGD iter. 40/49: loss=18.63322532659862, w0=74.15792280758414, w1=13.19243185270256\n",
      "SGD iter. 41/49: loss=0.045877625778935345, w0=74.12763167826253, w1=13.231794400148182\n",
      "SGD iter. 42/49: loss=3.3292281338416636, w0=73.86959183119075, w1=13.018609918418033\n",
      "SGD iter. 43/49: loss=12.652905904494437, w0=73.36654300842113, w1=12.937405695605733\n",
      "SGD iter. 44/49: loss=27.909336721008323, w0=72.61942405241736, w1=13.787118674265205\n",
      "SGD iter. 45/49: loss=0.4851087053107789, w0=72.71792366716898, w1=13.935477520215628\n",
      "SGD iter. 46/49: loss=4.047152630510586, w0=73.0024285926276, w1=13.705915903327034\n",
      "SGD iter. 47/49: loss=4.289228729099444, w0=73.29531863100718, w1=13.955178901175747\n",
      "SGD iter. 48/49: loss=14.351447687053541, w0=72.75956779923827, w1=13.608087976995911\n",
      "SGD iter. 49/49: loss=0.593942225877811, w0=72.86855791125562, w1=13.467439955366695\n",
      "SGD: execution time=0.002 seconds\n"
     ]
    }
   ],
   "source": [
    "# from stochastic_gradient_descent import *\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.1\n",
    "batch_size = 1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start SGD.\n",
    "start_time = datetime.datetime.now()\n",
    "sgd_losses, sgd_ws = stochastic_gradient_descent(\n",
    "    y, tx, w_initial, batch_size, max_iters, gamma\n",
    ")\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05609e0c8f16452987d948bdfdae1b50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=51, min=1), Output()), _dom_classes=('widge…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time Visualization\n",
    "from ipywidgets import IntSlider, interact\n",
    "\n",
    "\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        sgd_losses,\n",
    "        sgd_ws,\n",
    "        grid_losses,\n",
    "        grid_w0,\n",
    "        grid_w1,\n",
    "        mean_x,\n",
    "        std_x,\n",
    "        height,\n",
    "        weight,\n",
    "        n_iter,\n",
    "    )\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(sgd_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Effect of Outliers and MAE Cost Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from helpers import *\n",
    "\n",
    "height, weight, gender= load_data(sub_sample=True, add_outlier=True)\n",
    "\n",
    "x, mean_x, std_x = standardize(height)\n",
    "y, tx = build_model_data(x, weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((205,), (205, 2))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape, tx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD iter. 0/49: loss=3514.154380252628, w0=58.684506410530325, w1=50.97859838735797\n",
      "SGD iter. 1/49: loss=20.28249758801155, w0=63.142852251338475, w1=50.96377773384256\n",
      "SGD iter. 2/49: loss=62.65098081680604, w0=70.97853736500083, w1=50.9521795972925\n",
      "SGD iter. 3/49: loss=37.21557833887337, w0=77.01768190475941, w1=48.781151377183456\n",
      "SGD iter. 4/49: loss=58.01737661671691, w0=69.4773218944958, w1=46.49048427424348\n",
      "SGD iter. 5/49: loss=18.416196156759145, w0=73.72560063224641, w1=46.78027887311872\n",
      "SGD iter. 6/49: loss=97.03695318673749, w0=83.4773294083662, w1=40.47796396919519\n",
      "SGD iter. 7/49: loss=34.960498010045676, w0=89.33064368746012, w1=35.63049108265752\n",
      "SGD iter. 8/49: loss=6.013720444755969, w0=91.75828576071987, w1=33.935503294040224\n",
      "SGD iter. 9/49: loss=45.57372639473495, w0=85.07530350226368, w1=36.08708979639633\n",
      "SGD iter. 10/49: loss=23.10240593732426, w0=89.83349200405486, w1=30.703980807657285\n",
      "SGD iter. 11/49: loss=116.38883743136567, w0=79.15355470635791, w1=26.25286010623759\n",
      "SGD iter. 12/49: loss=14.492635267675924, w0=75.38489678047469, w1=28.341764384070174\n",
      "SGD iter. 13/49: loss=117.34414012849803, w0=64.66121936431152, w1=20.643164967244147\n",
      "SGD iter. 14/49: loss=87.28714101369401, w0=73.91007865578202, w1=6.518515937470447\n",
      "SGD iter. 15/49: loss=84.04368486644198, w0=64.83468273223613, w1=13.930678901605578\n",
      "SGD iter. 16/49: loss=69.1201060547643, w0=73.0649745820838, w1=2.369090644723931\n",
      "SGD iter. 17/49: loss=132.42034612956706, w0=61.6732258485936, w1=10.028105582984228\n",
      "SGD iter. 18/49: loss=7.919213721350664, w0=64.45905237709886, w1=7.091603180103094\n",
      "SGD iter. 19/49: loss=187.14238303193721, w0=78.00156083643897, w1=19.078111529041536\n",
      "SGD iter. 20/49: loss=29.44622245914067, w0=72.62966191613548, w1=14.968158381090038\n",
      "SGD iter. 21/49: loss=2.020391807781662, w0=74.03678094863629, w1=12.652686489937928\n",
      "SGD iter. 22/49: loss=6.62436749316245, w0=71.4888641243857, w1=11.319502989574017\n",
      "SGD iter. 23/49: loss=1.4613304817506894, w0=70.29215870026677, w1=12.558716133150789\n",
      "SGD iter. 24/49: loss=11.29129816891989, w0=66.96567824706379, w1=14.70854287865767\n",
      "SGD iter. 25/49: loss=104.93397411650707, w0=77.10645211468243, w1=17.99479120056831\n",
      "SGD iter. 26/49: loss=7175.8230394005905, w0=160.965301251864, w1=-370.98376142526615\n",
      "SGD iter. 27/49: loss=215801.70407258056, w0=620.8410127121768, w1=512.9366592078483\n",
      "SGD iter. 28/49: loss=218012.68702613344, w0=158.61549060677567, w1=406.5575526595308\n",
      "SGD iter. 29/49: loss=18656.498491942784, w0=293.8314975072308, w1=312.14928586997195\n",
      "SGD iter. 30/49: loss=689.7831301385652, w0=319.8312564971867, w1=289.3276009287457\n",
      "SGD iter. 31/49: loss=145569.22471854088, w0=-57.869471889945316, w1=-112.74797218025128\n",
      "SGD iter. 32/49: loss=70086.09112583069, w0=204.2075574417267, w1=390.03651061227697\n",
      "SGD iter. 33/49: loss=102507.37580884305, w0=-112.74232549351933, w1=120.06542429689978\n",
      "SGD iter. 34/49: loss=13132.507417466926, w0=0.7030678422948142, w1=132.50618477235028\n",
      "SGD iter. 35/49: loss=5059.211483527322, w0=71.11632833956745, w1=114.76940150552274\n",
      "SGD iter. 36/49: loss=22186.549568845105, w0=-76.3381343852623, w1=-204.3149268670981\n",
      "SGD iter. 37/49: loss=11804.53193063073, w0=-183.89482312203086, w1=-53.22354734680644\n",
      "SGD iter. 38/49: loss=19628.60287412938, w0=-45.20080444549674, w1=-168.0841994496218\n",
      "SGD iter. 39/49: loss=439.9259232102854, w0=-24.43719674485667, w1=-177.3490102909479\n",
      "SGD iter. 40/49: loss=153.61485073147506, w0=-12.167618361030863, w1=-181.8044544955495\n",
      "SGD iter. 41/49: loss=4239.422180976355, w0=-76.62406673787542, w1=-123.31116585416737\n",
      "SGD iter. 42/49: loss=1901.116552965422, w0=-33.46049156266417, w1=-147.74716059875863\n",
      "SGD iter. 43/49: loss=41.029791660384056, w0=-27.119419427198714, w1=-151.40632264548537\n",
      "SGD iter. 44/49: loss=257.1196524178206, w0=-42.99321101489018, w1=-139.97533060507178\n",
      "SGD iter. 45/49: loss=48315.41616126367, w0=174.60529044603072, w1=131.7909892018828\n",
      "SGD iter. 46/49: loss=2580.9543856918795, w0=124.31279301372933, w1=149.10012204611252\n",
      "SGD iter. 47/49: loss=843.2371835534394, w0=153.05948743034654, w1=127.79785502801931\n",
      "SGD iter. 48/49: loss=5469.003450300456, w0=79.85003871438879, w1=119.76948834662426\n",
      "SGD iter. 49/49: loss=10573.542657508635, w0=181.64430085306958, w1=-23.22706235069836\n",
      "GD: execution time=0.002 seconds\n"
     ]
    }
   ],
   "source": [
    "from plots import gradient_descent_visualization\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.7\n",
    "batch_size = 1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start SGD.\n",
    "start_time = datetime.datetime.now()\n",
    "sgd_losses, sgd_ws = stochastic_gradient_descent(\n",
    "    y, tx, w_initial, batch_size, max_iters, gamma\n",
    ")\n",
    "\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"GD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0c95b55997e4b5f909a03c826b62900",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=51, min=1), Output()), _dom_classes=('widge…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time Visualization\n",
    "from ipywidgets import IntSlider, interact\n",
    "\n",
    "\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        gd_losses,\n",
    "        gd_ws,\n",
    "        grid_losses,\n",
    "        grid_w0,\n",
    "        grid_w1,\n",
    "        mean_x,\n",
    "        std_x,\n",
    "        height,\n",
    "        weight,\n",
    "        n_iter,\n",
    "    )\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(gd_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# 6. Subgradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss_mae(y, tx, w):\n",
    "    \"\"\"\n",
    "    Compute the Mean Absolute Error (MAE).\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N,), target values\n",
    "        tx: numpy array of shape=(N, 2), input data\n",
    "        w: numpy array of shape=(2,), model parameters (w0, w1)\n",
    "\n",
    "    Returns:\n",
    "        mae: a scalar representing the mean absolute error\n",
    "    \"\"\"\n",
    "    # Compute the error (difference between predicted values and actual values)\n",
    "    error = y - np.dot(tx, w)\n",
    "    \n",
    "    # Compute the Mean Absolute Error (MAE)\n",
    "    mae = np.mean(np.abs(error))\n",
    "    \n",
    "    return mae\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_subgradient_mae(y, tx, w):\n",
    "    \"\"\"Compute a subgradient of the MAE at w.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        w: numpy array of shape=(2, ). The vector of model parameters.\n",
    "\n",
    "    Returns:\n",
    "        A numpy array of shape (2, ) (same shape as w), containing the subgradient of the MAE at w.\n",
    "    \"\"\"\n",
    "    # Compute the error (y - tx @ w)\n",
    "    error = y - np.dot(tx, w)\n",
    "    \n",
    "    # Compute the subgradient\n",
    "    subgrad = -np.dot(tx.T, np.sign(error)) / len(y)\n",
    "    return subgrad\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subgradient_descent(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"The SubGradient Descent (SubGD) algorithm.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        initial_w: numpy array of shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        max_iters: a scalar denoting the total number of iterations of GD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "\n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of SubGD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of SubGD\n",
    "    \"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        loss = compute_loss_mae(y,tx,w)\n",
    "        subgrad = compute_subgradient_mae(y,tx,w)\n",
    "        w = w - gamma * subgrad\n",
    "\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        print(\n",
    "            \"SubGD iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}\".format(\n",
    "                bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SubGD iter. 0/499: loss=75.02213752383784, w0=0.7, w1=1.3806841848679873e-15\n",
      "SubGD iter. 1/499: loss=74.32213752383784, w0=1.4, w1=2.7613683697359746e-15\n",
      "SubGD iter. 2/499: loss=73.62213752383784, w0=2.0999999999999996, w1=4.142052554603962e-15\n",
      "SubGD iter. 3/499: loss=72.92213752383783, w0=2.8, w1=5.522736739471949e-15\n",
      "SubGD iter. 4/499: loss=72.22213752383784, w0=3.5, w1=6.903420924339937e-15\n",
      "SubGD iter. 5/499: loss=71.52213752383784, w0=4.2, w1=8.284105109207923e-15\n",
      "SubGD iter. 6/499: loss=70.82213752383784, w0=4.9, w1=9.664789294075911e-15\n",
      "SubGD iter. 7/499: loss=70.12213752383785, w0=5.6000000000000005, w1=1.1045473478943898e-14\n",
      "SubGD iter. 8/499: loss=69.42213752383783, w0=6.300000000000001, w1=1.2426157663811886e-14\n",
      "SubGD iter. 9/499: loss=68.72213752383784, w0=7.000000000000001, w1=1.3806841848679873e-14\n",
      "SubGD iter. 10/499: loss=68.02213752383786, w0=7.700000000000001, w1=1.518752603354786e-14\n",
      "SubGD iter. 11/499: loss=67.32213752383784, w0=8.4, w1=1.6568210218415847e-14\n",
      "SubGD iter. 12/499: loss=66.62213752383785, w0=9.1, w1=1.7948894403283834e-14\n",
      "SubGD iter. 13/499: loss=65.92213752383783, w0=9.799999999999999, w1=1.9329578588151822e-14\n",
      "SubGD iter. 14/499: loss=65.22213752383784, w0=10.499999999999998, w1=2.071026277301981e-14\n",
      "SubGD iter. 15/499: loss=64.52213752383786, w0=11.199999999999998, w1=2.2090946957887797e-14\n",
      "SubGD iter. 16/499: loss=63.822137523837846, w0=11.899999999999997, w1=2.3471631142755784e-14\n",
      "SubGD iter. 17/499: loss=63.12213752383785, w0=12.599999999999996, w1=2.4852315327623772e-14\n",
      "SubGD iter. 18/499: loss=62.42213752383784, w0=13.299999999999995, w1=2.623299951249176e-14\n",
      "SubGD iter. 19/499: loss=61.72213752383785, w0=13.999999999999995, w1=2.7613683697359747e-14\n",
      "SubGD iter. 20/499: loss=61.022137523837856, w0=14.699999999999994, w1=2.899436788222773e-14\n",
      "SubGD iter. 21/499: loss=60.322137523837846, w0=15.399999999999993, w1=3.037505206709572e-14\n",
      "SubGD iter. 22/499: loss=59.62213752383786, w0=16.099999999999994, w1=3.1755736251963706e-14\n",
      "SubGD iter. 23/499: loss=58.922137523837854, w0=16.799999999999994, w1=3.3136420436831694e-14\n",
      "SubGD iter. 24/499: loss=58.22213752383785, w0=17.499999999999993, w1=3.451710462169968e-14\n",
      "SubGD iter. 25/499: loss=57.522137523837856, w0=18.199999999999992, w1=3.589778880656767e-14\n",
      "SubGD iter. 26/499: loss=56.82213752383786, w0=18.89999999999999, w1=3.7278472991435656e-14\n",
      "SubGD iter. 27/499: loss=56.12213752383786, w0=19.59999999999999, w1=3.8659157176303644e-14\n",
      "SubGD iter. 28/499: loss=55.422137523837854, w0=20.29999999999999, w1=4.003984136117163e-14\n",
      "SubGD iter. 29/499: loss=54.72213752383786, w0=20.99999999999999, w1=4.142052554603962e-14\n",
      "SubGD iter. 30/499: loss=54.022137523837856, w0=21.69999999999999, w1=4.2801209730907606e-14\n",
      "SubGD iter. 31/499: loss=53.32213752383786, w0=22.399999999999988, w1=4.4181893915775594e-14\n",
      "SubGD iter. 32/499: loss=52.62213752383786, w0=23.099999999999987, w1=4.556257810064358e-14\n",
      "SubGD iter. 33/499: loss=51.922137523837854, w0=23.799999999999986, w1=4.694326228551157e-14\n",
      "SubGD iter. 34/499: loss=51.22213752383786, w0=24.499999999999986, w1=4.8323946470379556e-14\n",
      "SubGD iter. 35/499: loss=50.522137523837856, w0=25.199999999999985, w1=4.9704630655247544e-14\n",
      "SubGD iter. 36/499: loss=49.82213752383786, w0=25.899999999999984, w1=5.108531484011553e-14\n",
      "SubGD iter. 37/499: loss=49.12213752383786, w0=26.599999999999984, w1=5.246599902498352e-14\n",
      "SubGD iter. 38/499: loss=48.42213752383787, w0=27.299999999999983, w1=5.3846683209851506e-14\n",
      "SubGD iter. 39/499: loss=47.72213752383786, w0=27.999999999999982, w1=5.5227367394719494e-14\n",
      "SubGD iter. 40/499: loss=47.02213752383787, w0=28.69999999999998, w1=5.660805157958748e-14\n",
      "SubGD iter. 41/499: loss=46.32213752383786, w0=29.39999999999998, w1=5.798873576445546e-14\n",
      "SubGD iter. 42/499: loss=45.62213752383786, w0=30.09999999999998, w1=5.936941994932344e-14\n",
      "SubGD iter. 43/499: loss=44.92213752383787, w0=30.79999999999998, w1=6.075010413419142e-14\n",
      "SubGD iter. 44/499: loss=44.222137523837866, w0=31.49999999999998, w1=6.21307883190594e-14\n",
      "SubGD iter. 45/499: loss=43.52213752383787, w0=32.19999999999998, w1=6.351147250392739e-14\n",
      "SubGD iter. 46/499: loss=42.82213752383787, w0=32.899999999999984, w1=6.489215668879537e-14\n",
      "SubGD iter. 47/499: loss=42.12213752383786, w0=33.59999999999999, w1=6.627284087366335e-14\n",
      "SubGD iter. 48/499: loss=41.422137523837854, w0=34.29999999999999, w1=6.765352505853133e-14\n",
      "SubGD iter. 49/499: loss=40.72213752383786, w0=34.99999999999999, w1=6.903420924339931e-14\n",
      "SubGD iter. 50/499: loss=40.022137523837856, w0=35.699999999999996, w1=7.041489342826729e-14\n",
      "SubGD iter. 51/499: loss=39.32213752383785, w0=36.4, w1=7.179557761313527e-14\n",
      "SubGD iter. 52/499: loss=38.62213752383785, w0=37.1, w1=7.317626179800326e-14\n",
      "SubGD iter. 53/499: loss=37.92213752383785, w0=37.800000000000004, w1=7.455694598287124e-14\n",
      "SubGD iter. 54/499: loss=37.22213752383784, w0=38.50000000000001, w1=7.593763016773922e-14\n",
      "SubGD iter. 55/499: loss=36.522137523837834, w0=39.20000000000001, w1=7.73183143526072e-14\n",
      "SubGD iter. 56/499: loss=35.82213752383784, w0=39.90000000000001, w1=7.869899853747518e-14\n",
      "SubGD iter. 57/499: loss=35.122137523837836, w0=40.600000000000016, w1=8.007968272234316e-14\n",
      "SubGD iter. 58/499: loss=34.42213752383783, w0=41.30000000000002, w1=8.146036690721114e-14\n",
      "SubGD iter. 59/499: loss=33.72213752383782, w0=42.00000000000002, w1=8.284105109207912e-14\n",
      "SubGD iter. 60/499: loss=33.02213752383783, w0=42.700000000000024, w1=8.42217352769471e-14\n",
      "SubGD iter. 61/499: loss=32.322137523837824, w0=43.40000000000003, w1=8.560241946181509e-14\n",
      "SubGD iter. 62/499: loss=31.62213752383782, w0=44.10000000000003, w1=8.698310364668307e-14\n",
      "SubGD iter. 63/499: loss=30.92213752383781, w0=44.80000000000003, w1=8.836378783155105e-14\n",
      "SubGD iter. 64/499: loss=30.222137523837812, w0=45.500000000000036, w1=8.974447201641903e-14\n",
      "SubGD iter. 65/499: loss=29.522137523837813, w0=46.20000000000004, w1=9.112515620128701e-14\n",
      "SubGD iter. 66/499: loss=28.822137523837803, w0=46.90000000000004, w1=9.250584038615499e-14\n",
      "SubGD iter. 67/499: loss=28.127521912413417, w0=47.59317073170736, w1=0.010429504301588326\n",
      "SubGD iter. 68/499: loss=27.444492376591334, w0=48.279512195121995, w1=0.030946823978452335\n",
      "SubGD iter. 69/499: loss=26.77094156972153, w0=48.96585365853663, w1=0.05146414365531635\n",
      "SubGD iter. 70/499: loss=26.10856365910505, w0=49.631707317073214, w1=0.09847423943588744\n",
      "SubGD iter. 71/499: loss=25.47725905104814, w0=50.290731707317114, w1=0.15646290526090736\n",
      "SubGD iter. 72/499: loss=24.852243473304743, w0=50.94292682926833, w1=0.22568941124273328\n",
      "SubGD iter. 73/499: loss=24.242382452952953, w0=51.58829268292687, w1=0.30310357050155323\n",
      "SubGD iter. 74/499: loss=23.64508794813849, w0=52.22000000000004, w1=0.3959832728587358\n",
      "SubGD iter. 75/499: loss=23.064916257493387, w0=52.84487804878053, w1=0.49756759237506976\n",
      "SubGD iter. 76/499: loss=22.497754146188534, w0=53.45609756097565, w1=0.6141004976225993\n",
      "SubGD iter. 77/499: loss=21.947745690837383, w0=54.0536585365854, w1=0.7446142087906341\n",
      "SubGD iter. 78/499: loss=21.413298621730906, w0=54.651219512195155, w1=0.8751279199586689\n",
      "SubGD iter. 79/499: loss=20.881892043782663, w0=55.23512195121955, w1=1.0188017246598564\n",
      "SubGD iter. 80/499: loss=20.36799934588207, w0=55.80536585365857, w1=1.1757985036032554\n",
      "SubGD iter. 81/499: loss=19.875207060007796, w0=56.34829268292686, w1=1.3523706085797649\n",
      "SubGD iter. 82/499: loss=19.41191755838288, w0=56.88439024390247, w1=1.5335342358145976\n",
      "SubGD iter. 83/499: loss=18.95637120600551, w0=57.40682926829272, w1=1.7274741826867106\n",
      "SubGD iter. 84/499: loss=18.519187588864522, w0=57.915609756097595, w1=1.931263665739125\n",
      "SubGD iter. 85/499: loss=18.095929938893647, w0=58.410731707317105, w1=2.14331638720961\n",
      "SubGD iter. 86/499: loss=17.68653451362574, w0=58.88536585365857, w1=2.367493925571536\n",
      "SubGD iter. 87/499: loss=17.29338738232081, w0=59.35317073170735, w1=2.5968440979373835\n",
      "SubGD iter. 88/499: loss=16.910329370202895, w0=59.7936585365854, w1=2.845106619036211\n",
      "SubGD iter. 89/499: loss=16.552544984306145, w0=60.2136585365854, w1=3.099383347738513\n",
      "SubGD iter. 90/499: loss=16.2094944449989, w0=60.62682926829272, w1=3.3570865484231294\n",
      "SubGD iter. 91/499: loss=15.870828346286116, w0=61.03317073170735, w1=3.6195557771269256\n",
      "SubGD iter. 92/499: loss=15.536537659277101, w0=61.439512195121985, w1=3.8820250058307217\n",
      "SubGD iter. 93/499: loss=15.20532490727048, w0=61.83219512195125, w1=4.151326832941741\n",
      "SubGD iter. 94/499: loss=14.883169580225578, w0=62.21121951219516, w1=4.428122107083478\n",
      "SubGD iter. 95/499: loss=14.571777285304503, w0=62.576585365853695, w1=4.707233742333243\n",
      "SubGD iter. 96/499: loss=14.26978369680246, w0=62.941951219512234, w1=4.986345377583008\n",
      "SubGD iter. 97/499: loss=13.968933024741299, w0=63.30048780487809, w1=5.2703748871203535\n",
      "SubGD iter. 98/499: loss=13.671602248618706, w0=63.645365853658575, w1=5.5589503139451555\n",
      "SubGD iter. 99/499: loss=13.382721326479059, w0=63.99024390243906, w1=5.8475257407699575\n",
      "SubGD iter. 100/499: loss=13.093840404339412, w0=64.33512195121955, w1=6.136101167594759\n",
      "SubGD iter. 101/499: loss=12.805008638488616, w0=64.67317073170736, w1=6.428199062504155\n",
      "SubGD iter. 102/499: loss=12.520912013170923, w0=65.00439024390248, w1=6.724237836622877\n",
      "SubGD iter. 103/499: loss=12.242874042217517, w0=65.31512195121955, w1=7.025314856467832\n",
      "SubGD iter. 104/499: loss=11.977571977360556, w0=65.61219512195126, w1=7.331051396051579\n",
      "SubGD iter. 105/499: loss=11.717961548210221, w0=65.90926829268297, w1=7.636787935635326\n",
      "SubGD iter. 106/499: loss=11.458351119059886, w0=66.20634146341467, w1=7.942524475219073\n",
      "SubGD iter. 107/499: loss=11.19874068990955, w0=66.50341463414638, w1=8.24826101480282\n",
      "SubGD iter. 108/499: loss=10.941286546868394, w0=66.79365853658541, w1=8.55492046282147\n",
      "SubGD iter. 109/499: loss=10.686598632636768, w0=67.08390243902444, w1=8.861579910840119\n",
      "SubGD iter. 110/499: loss=10.434747066462252, w0=67.36731707317078, w1=9.165557758274131\n",
      "SubGD iter. 111/499: loss=10.188894294255062, w0=67.64390243902444, w1=9.473394434694477\n",
      "SubGD iter. 112/499: loss=9.947299227499977, w0=67.90000000000005, w1=9.782778940421327\n",
      "SubGD iter. 113/499: loss=9.720236946011775, w0=68.14926829268298, w1=10.086222698393708\n",
      "SubGD iter. 114/499: loss=9.502085257951299, w0=68.38487804878054, w1=10.382361877715642\n",
      "SubGD iter. 115/499: loss=9.297848025467664, w0=68.61365853658542, w1=10.6800258525355\n",
      "SubGD iter. 116/499: loss=9.098400511342565, w0=68.82878048780493, w1=10.98265596522938\n",
      "SubGD iter. 117/499: loss=8.902802558139578, w0=69.03707317073176, w1=11.281064884238088\n",
      "SubGD iter. 118/499: loss=8.715010118400631, w0=69.25219512195127, w1=11.566888265015876\n",
      "SubGD iter. 119/499: loss=8.53219231997815, w0=69.46731707317079, w1=11.852711645793665\n",
      "SubGD iter. 120/499: loss=8.355356038930907, w0=69.6824390243903, w1=12.11802126154372\n",
      "SubGD iter. 121/499: loss=8.193848978914804, w0=69.89756097560982, w1=12.365141674706402\n",
      "SubGD iter. 122/499: loss=8.04049761820311, w0=70.11268292682934, w1=12.612262087869084\n",
      "SubGD iter. 123/499: loss=7.889524272645465, w0=70.32097560975616, w1=12.85295216343078\n",
      "SubGD iter. 124/499: loss=7.7450665070492, w0=70.52243902439031, w1=13.09006886182384\n",
      "SubGD iter. 125/499: loss=7.607135194072623, w0=70.71707317073177, w1=13.32379939200935\n",
      "SubGD iter. 126/499: loss=7.474974605982812, w0=70.91170731707322, w1=13.557529922194862\n",
      "SubGD iter. 127/499: loss=7.350377207341608, w0=71.08585365853664, w1=13.77481701983013\n",
      "SubGD iter. 128/499: loss=7.24058294606604, w0=71.25317073170737, w1=13.98526009939196\n",
      "SubGD iter. 129/499: loss=7.141024462238325, w0=71.41365853658542, w1=14.181387793756064\n",
      "SubGD iter. 130/499: loss=7.051326823318398, w0=71.56731707317078, w1=14.367310429971589\n",
      "SubGD iter. 131/499: loss=6.978764683600207, w0=71.67317073170737, w1=14.521455756299083\n",
      "SubGD iter. 132/499: loss=6.928813571237076, w0=71.77902439024396, w1=14.675601082626578\n",
      "SubGD iter. 133/499: loss=6.882651108006606, w0=71.85756097560981, w1=14.79902750110223\n",
      "SubGD iter. 134/499: loss=6.8526996526128245, w0=71.9224390243903, w1=14.897732527193842\n",
      "SubGD iter. 135/499: loss=6.832841812908921, w0=71.99414634146348, w1=14.989143068631245\n",
      "SubGD iter. 136/499: loss=6.813559203755046, w0=72.06585365853665, w1=15.080553610068648\n",
      "SubGD iter. 137/499: loss=6.79552401181772, w0=72.12390243902446, w1=15.162570008705144\n",
      "SubGD iter. 138/499: loss=6.781368308426309, w0=72.1682926829269, w1=15.23015714010145\n",
      "SubGD iter. 139/499: loss=6.772027574020451, w0=72.21268292682934, w1=15.297744271497757\n",
      "SubGD iter. 140/499: loss=6.763628301557192, w0=72.2502439024391, w1=15.35926083191652\n",
      "SubGD iter. 141/499: loss=6.756904617182943, w0=72.27414634146349, w1=15.417684803390415\n",
      "SubGD iter. 142/499: loss=6.751282024873237, w0=72.30487804878057, w1=15.468762651275583\n",
      "SubGD iter. 143/499: loss=6.7465882295202615, w0=72.32195121951227, w1=15.510410908426834\n",
      "SubGD iter. 144/499: loss=6.743757989423253, w0=72.3321951219513, w1=15.54464383192977\n",
      "SubGD iter. 145/499: loss=6.742178565583327, w0=72.33560975609764, w1=15.574418066202577\n",
      "SubGD iter. 146/499: loss=6.740895473079205, w0=72.33902439024398, w1=15.604192300475384\n",
      "SubGD iter. 147/499: loss=6.739612380575084, w0=72.34243902439032, w1=15.633966534748192\n",
      "SubGD iter. 148/499: loss=6.73852712783922, w0=72.33902439024398, w1=15.653193209808995\n",
      "SubGD iter. 149/499: loss=6.737982378181725, w0=72.33560975609764, w1=15.672419884869798\n",
      "SubGD iter. 150/499: loss=6.737437628524228, w0=72.3321951219513, w1=15.691646559930602\n",
      "SubGD iter. 151/499: loss=6.7368928788667315, w0=72.32878048780496, w1=15.710873234991405\n",
      "SubGD iter. 152/499: loss=6.736348129209237, w0=72.32536585365862, w1=15.730099910052209\n",
      "SubGD iter. 153/499: loss=6.735834753037406, w0=72.3151219512196, w1=15.74411717760865\n",
      "SubGD iter. 154/499: loss=6.735404151141794, w0=72.30487804878057, w1=15.758134445165092\n",
      "SubGD iter. 155/499: loss=6.734973549246182, w0=72.29463414634154, w1=15.772151712721534\n",
      "SubGD iter. 156/499: loss=6.734542947350567, w0=72.28439024390251, w1=15.786168980277976\n",
      "SubGD iter. 157/499: loss=6.734112345454954, w0=72.27414634146348, w1=15.800186247834418\n",
      "SubGD iter. 158/499: loss=6.733681743559341, w0=72.26390243902445, w1=15.81420351539086\n",
      "SubGD iter. 159/499: loss=6.733251141663727, w0=72.25365853658542, w1=15.828220782947302\n",
      "SubGD iter. 160/499: loss=6.732820539768115, w0=72.24341463414639, w1=15.842238050503743\n",
      "SubGD iter. 161/499: loss=6.732389937872503, w0=72.23317073170736, w1=15.856255318060185\n",
      "SubGD iter. 162/499: loss=6.731959335976889, w0=72.22292682926833, w1=15.870272585616627\n",
      "SubGD iter. 163/499: loss=6.731528734081275, w0=72.2126829268293, w1=15.884289853173069\n",
      "SubGD iter. 164/499: loss=6.731098132185663, w0=72.20243902439027, w1=15.89830712072951\n",
      "SubGD iter. 165/499: loss=6.730699347411477, w0=72.19902439024393, w1=15.913583800429242\n",
      "SubGD iter. 166/499: loss=6.730349295027189, w0=72.1956097560976, w1=15.928860480128973\n",
      "SubGD iter. 167/499: loss=6.729999242642901, w0=72.19219512195126, w1=15.944137159828704\n",
      "SubGD iter. 168/499: loss=6.729649190258613, w0=72.18878048780492, w1=15.959413839528436\n",
      "SubGD iter. 169/499: loss=6.7292991378743245, w0=72.18536585365858, w1=15.974690519228167\n",
      "SubGD iter. 170/499: loss=6.728949085490038, w0=72.18195121951224, w1=15.989967198927898\n",
      "SubGD iter. 171/499: loss=6.728599033105749, w0=72.1785365853659, w1=16.00524387862763\n",
      "SubGD iter. 172/499: loss=6.728248980721461, w0=72.17512195121957, w1=16.020520558327362\n",
      "SubGD iter. 173/499: loss=6.727898928337173, w0=72.17170731707323, w1=16.035797238027094\n",
      "SubGD iter. 174/499: loss=6.727548875952885, w0=72.16829268292689, w1=16.051073917726825\n",
      "SubGD iter. 175/499: loss=6.727198823568597, w0=72.16487804878055, w1=16.066350597426556\n",
      "SubGD iter. 176/499: loss=6.726848771184309, w0=72.16146341463421, w1=16.081627277126287\n",
      "SubGD iter. 177/499: loss=6.726498718800022, w0=72.15804878048787, w1=16.09690395682602\n",
      "SubGD iter. 178/499: loss=6.726177100417874, w0=72.16146341463421, w1=16.11135540517994\n",
      "SubGD iter. 179/499: loss=6.7258620945809025, w0=72.16487804878055, w1=16.12580685353386\n",
      "SubGD iter. 180/499: loss=6.72556225087091, w0=72.15463414634152, w1=16.1398241210903\n",
      "SubGD iter. 181/499: loss=6.725307673545225, w0=72.15804878048786, w1=16.15427556944422\n",
      "SubGD iter. 182/499: loss=6.724992667708255, w0=72.1614634146342, w1=16.16872701779814\n",
      "SubGD iter. 183/499: loss=6.724732611420821, w0=72.15804878048786, w1=16.18191905400877\n",
      "SubGD iter. 184/499: loss=6.7244673406412, w0=72.15463414634152, w1=16.1951110902194\n",
      "SubGD iter. 185/499: loss=6.724202069861577, w0=72.15121951219518, w1=16.208303126430028\n",
      "SubGD iter. 186/499: loss=6.724095061638527, w0=72.15463414634152, w1=16.214296522471866\n",
      "SubGD iter. 187/499: loss=6.724027089463572, w0=72.15804878048786, w1=16.220289918513703\n",
      "SubGD iter. 188/499: loss=6.723990055435357, w0=72.16829268292689, w1=16.21757869739639\n",
      "SubGD iter. 189/499: loss=6.723932360492694, w0=72.17170731707323, w1=16.22357209343823\n",
      "SubGD iter. 190/499: loss=6.7238643883177405, w0=72.17512195121957, w1=16.229565489480066\n",
      "SubGD iter. 191/499: loss=6.723796416142786, w0=72.1785365853659, w1=16.235558885521904\n",
      "SubGD iter. 192/499: loss=6.723757359710732, w0=72.18195121951224, w1=16.233672895750402\n",
      "SubGD iter. 193/499: loss=6.723771458409538, w0=72.1785365853659, w1=16.24049152313805\n",
      "SubGD iter. 194/499: loss=6.723770649573719, w0=72.18195121951224, w1=16.23860553336655\n",
      "SubGD iter. 195/499: loss=6.723748911454045, w0=72.18536585365858, w1=16.236719543595047\n",
      "SubGD iter. 196/499: loss=6.723758438080852, w0=72.18195121951224, w1=16.243538170982696\n",
      "SubGD iter. 197/499: loss=6.72376220131703, w0=72.18536585365858, w1=16.241652181211194\n",
      "SubGD iter. 198/499: loss=6.723740463197358, w0=72.18878048780492, w1=16.239766191439692\n",
      "SubGD iter. 199/499: loss=6.723745417752164, w0=72.18536585365858, w1=16.24658481882734\n",
      "SubGD iter. 200/499: loss=6.723753753060344, w0=72.18878048780492, w1=16.24469882905584\n",
      "SubGD iter. 201/499: loss=6.723732014940671, w0=72.19219512195126, w1=16.242812839284337\n",
      "SubGD iter. 202/499: loss=6.723732397423477, w0=72.18878048780492, w1=16.249631466671985\n",
      "SubGD iter. 203/499: loss=6.723779100794489, w0=72.18536585365858, w1=16.240673717277303\n",
      "SubGD iter. 204/499: loss=6.723737826950257, w0=72.18878048780492, w1=16.2387877275058\n",
      "SubGD iter. 205/499: loss=6.723754948867846, w0=72.18536585365858, w1=16.24560635489345\n",
      "SubGD iter. 206/499: loss=6.723751116813243, w0=72.18878048780492, w1=16.24372036512195\n",
      "SubGD iter. 207/499: loss=6.723729378693568, w0=72.19219512195126, w1=16.241834375350447\n",
      "SubGD iter. 208/499: loss=6.72374192853916, w0=72.18878048780492, w1=16.248653002738095\n",
      "SubGD iter. 209/499: loss=6.723766579602042, w0=72.18536585365858, w1=16.239695253343413\n",
      "SubGD iter. 210/499: loss=6.723735190703155, w0=72.18878048780492, w1=16.23780926357191\n",
      "SubGD iter. 211/499: loss=6.723764479983528, w0=72.18536585365858, w1=16.24462789095956\n",
      "SubGD iter. 212/499: loss=6.723748480566142, w0=72.18878048780492, w1=16.242741901188058\n",
      "SubGD iter. 213/499: loss=6.723726742446467, w0=72.19219512195126, w1=16.240855911416556\n",
      "SubGD iter. 214/499: loss=6.7237514596548404, w0=72.18878048780492, w1=16.247674538804205\n",
      "SubGD iter. 215/499: loss=6.723754058409597, w0=72.18536585365858, w1=16.238716789409523\n",
      "SubGD iter. 216/499: loss=6.72373898311655, w0=72.18195121951224, w1=16.24553541679717\n",
      "SubGD iter. 217/499: loss=6.723767582438714, w0=72.18536585365858, w1=16.24364942702567\n",
      "SubGD iter. 218/499: loss=6.7237458443190405, w0=72.18878048780492, w1=16.241763437254168\n",
      "SubGD iter. 219/499: loss=6.723725962787864, w0=72.18536585365858, w1=16.248582064641816\n",
      "SubGD iter. 220/499: loss=6.723759134182027, w0=72.18878048780492, w1=16.246696074870314\n",
      "SubGD iter. 221/499: loss=6.723741537217152, w0=72.18536585365858, w1=16.237738325475632\n",
      "SubGD iter. 222/499: loss=6.7237485142322315, w0=72.18195121951224, w1=16.24455695286328\n",
      "SubGD iter. 223/499: loss=6.723764946191611, w0=72.18536585365858, w1=16.24267096309178\n",
      "SubGD iter. 224/499: loss=6.723743208071939, w0=72.18878048780492, w1=16.240784973320277\n",
      "SubGD iter. 225/499: loss=6.723735493903544, w0=72.18536585365858, w1=16.247603600707926\n",
      "SubGD iter. 226/499: loss=6.723756497934925, w0=72.18878048780492, w1=16.245717610936424\n",
      "SubGD iter. 227/499: loss=6.723734759815251, w0=72.19219512195126, w1=16.243831621164922\n",
      "SubGD iter. 228/499: loss=6.723730990050589, w0=72.18195121951223, w1=16.243578488929394\n",
      "SubGD iter. 229/499: loss=6.723762309944511, w0=72.18536585365857, w1=16.241692499157892\n",
      "SubGD iter. 230/499: loss=6.723740571824836, w0=72.1887804878049, w1=16.23980650938639\n",
      "SubGD iter. 231/499: loss=6.723745025019228, w0=72.18536585365857, w1=16.24662513677404\n",
      "SubGD iter. 232/499: loss=6.723753861687824, w0=72.1887804878049, w1=16.244739147002537\n",
      "SubGD iter. 233/499: loss=6.7237321235681495, w0=72.19219512195124, w1=16.242853157231036\n",
      "SubGD iter. 234/499: loss=6.7237320046905396, w0=72.1887804878049, w1=16.249671784618684\n",
      "SubGD iter. 235/499: loss=6.723779616734578, w0=72.18536585365857, w1=16.240714035224002\n",
      "SubGD iter. 236/499: loss=6.723737935577735, w0=72.1887804878049, w1=16.2388280454525\n",
      "SubGD iter. 237/499: loss=6.723754556134909, w0=72.18536585365857, w1=16.24564667284015\n",
      "SubGD iter. 238/499: loss=6.723751225440721, w0=72.1887804878049, w1=16.243760683068647\n",
      "SubGD iter. 239/499: loss=6.723729487321049, w0=72.19219512195124, w1=16.241874693297145\n",
      "SubGD iter. 240/499: loss=6.723741535806223, w0=72.1887804878049, w1=16.248693320684794\n",
      "SubGD iter. 241/499: loss=6.723767095542131, w0=72.18536585365857, w1=16.23973557129011\n",
      "SubGD iter. 242/499: loss=6.723735299330634, w0=72.1887804878049, w1=16.23784958151861\n",
      "SubGD iter. 243/499: loss=6.723764087250593, w0=72.18536585365857, w1=16.24466820890626\n",
      "SubGD iter. 244/499: loss=6.723748589193621, w0=72.1887804878049, w1=16.242782219134757\n",
      "SubGD iter. 245/499: loss=6.7237268510739465, w0=72.19219512195124, w1=16.240896229363255\n",
      "SubGD iter. 246/499: loss=6.7237510669219045, w0=72.1887804878049, w1=16.247714856750903\n",
      "SubGD iter. 247/499: loss=6.7237545743496865, w0=72.18536585365857, w1=16.23875710735622\n",
      "SubGD iter. 248/499: loss=6.723738590383614, w0=72.18195121951223, w1=16.24557573474387\n",
      "SubGD iter. 249/499: loss=6.723767691066192, w0=72.18536585365857, w1=16.243689744972368\n",
      "SubGD iter. 250/499: loss=6.723745952946519, w0=72.1887804878049, w1=16.241803755200866\n",
      "SubGD iter. 251/499: loss=6.723725570054927, w0=72.18536585365857, w1=16.248622382588515\n",
      "SubGD iter. 252/499: loss=6.723759242809506, w0=72.1887804878049, w1=16.246736392817013\n",
      "SubGD iter. 253/499: loss=6.723742053157242, w0=72.18536585365857, w1=16.23777864342233\n",
      "SubGD iter. 254/499: loss=6.723748121499295, w0=72.18195121951223, w1=16.24459727080998\n",
      "SubGD iter. 255/499: loss=6.723765054819091, w0=72.18536585365857, w1=16.242711281038478\n",
      "SubGD iter. 256/499: loss=6.723743316699417, w0=72.1887804878049, w1=16.240825291266976\n",
      "SubGD iter. 257/499: loss=6.723735101170609, w0=72.18536585365857, w1=16.247643918654624\n",
      "SubGD iter. 258/499: loss=6.723756606562404, w0=72.1887804878049, w1=16.245757928883123\n",
      "SubGD iter. 259/499: loss=6.72373486844273, w0=72.19219512195124, w1=16.24387193911162\n",
      "SubGD iter. 260/499: loss=6.723731004630262, w0=72.18195121951221, w1=16.243618806876093\n",
      "SubGD iter. 261/499: loss=6.723762418571988, w0=72.18536585365855, w1=16.24173281710459\n",
      "SubGD iter. 262/499: loss=6.723740680452314, w0=72.18878048780489, w1=16.23984682733309\n",
      "SubGD iter. 263/499: loss=6.723744632286293, w0=72.18536585365855, w1=16.246665454720738\n",
      "SubGD iter. 264/499: loss=6.723753970315301, w0=72.18878048780489, w1=16.244779464949236\n",
      "SubGD iter. 265/499: loss=6.723732232195628, w0=72.19219512195123, w1=16.242893475177734\n",
      "SubGD iter. 266/499: loss=6.7237316119576045, w0=72.18878048780489, w1=16.249712102565383\n",
      "SubGD iter. 267/499: loss=6.723780132674666, w0=72.18536585365855, w1=16.2407543531707\n",
      "SubGD iter. 268/499: loss=6.723738044205214, w0=72.18878048780489, w1=16.2388683633992\n",
      "SubGD iter. 269/499: loss=6.723754163401973, w0=72.18536585365855, w1=16.245686990786847\n",
      "SubGD iter. 270/499: loss=6.7237513340682, w0=72.18878048780489, w1=16.243801001015346\n",
      "SubGD iter. 271/499: loss=6.723729595948527, w0=72.19219512195123, w1=16.241915011243844\n",
      "SubGD iter. 272/499: loss=6.723741143073286, w0=72.18878048780489, w1=16.248733638631492\n",
      "SubGD iter. 273/499: loss=6.723767611482221, w0=72.18536585365855, w1=16.23977588923681\n",
      "SubGD iter. 274/499: loss=6.723735407958112, w0=72.18878048780489, w1=16.23788989946531\n",
      "SubGD iter. 275/499: loss=6.723763694517656, w0=72.18536585365855, w1=16.244708526852957\n",
      "SubGD iter. 276/499: loss=6.723748697821098, w0=72.18878048780489, w1=16.242822537081455\n",
      "SubGD iter. 277/499: loss=6.723726959701425, w0=72.19219512195123, w1=16.240936547309953\n",
      "SubGD iter. 278/499: loss=6.7237506741889685, w0=72.18878048780489, w1=16.247755174697602\n",
      "SubGD iter. 279/499: loss=6.723755090289776, w0=72.18536585365855, w1=16.23879742530292\n",
      "SubGD iter. 280/499: loss=6.723738197650676, w0=72.18195121951221, w1=16.24561605269057\n",
      "SubGD iter. 281/499: loss=6.72376779969367, w0=72.18536585365855, w1=16.243730062919067\n",
      "SubGD iter. 282/499: loss=6.723746061573998, w0=72.18878048780489, w1=16.241844073147565\n",
      "SubGD iter. 283/499: loss=6.72372517732199, w0=72.18536585365855, w1=16.248662700535213\n",
      "SubGD iter. 284/499: loss=6.723759351436984, w0=72.18878048780489, w1=16.24677671076371\n",
      "SubGD iter. 285/499: loss=6.723742569097331, w0=72.18536585365855, w1=16.23781896136903\n",
      "SubGD iter. 286/499: loss=6.723747728766359, w0=72.18195121951221, w1=16.244637588756678\n",
      "SubGD iter. 287/499: loss=6.723765163446569, w0=72.18536585365855, w1=16.242751598985176\n",
      "SubGD iter. 288/499: loss=6.723743425326895, w0=72.18878048780489, w1=16.240865609213675\n",
      "SubGD iter. 289/499: loss=6.723734708437673, w0=72.18536585365855, w1=16.247684236601323\n",
      "SubGD iter. 290/499: loss=6.723756715189882, w0=72.18878048780489, w1=16.24579824682982\n",
      "SubGD iter. 291/499: loss=6.723734977070209, w0=72.19219512195123, w1=16.24391225705832\n",
      "SubGD iter. 292/499: loss=6.723731019209936, w0=72.1819512195122, w1=16.24365912482279\n",
      "SubGD iter. 293/499: loss=6.723762527199468, w0=72.18536585365854, w1=16.24177313505129\n",
      "SubGD iter. 294/499: loss=6.723740789079793, w0=72.18878048780488, w1=16.239887145279788\n",
      "SubGD iter. 295/499: loss=6.723744239553355, w0=72.18536585365854, w1=16.246705772667436\n",
      "SubGD iter. 296/499: loss=6.723754078942781, w0=72.18878048780488, w1=16.244819782895934\n",
      "SubGD iter. 297/499: loss=6.723732340823107, w0=72.19219512195122, w1=16.242933793124433\n",
      "SubGD iter. 298/499: loss=6.7237312192246685, w0=72.18878048780488, w1=16.24975242051208\n",
      "SubGD iter. 299/499: loss=6.723780648614757, w0=72.18536585365854, w1=16.2407946711174\n",
      "SubGD iter. 300/499: loss=6.723738152832693, w0=72.18878048780488, w1=16.238908681345897\n",
      "SubGD iter. 301/499: loss=6.723753770669036, w0=72.18536585365854, w1=16.245727308733546\n",
      "SubGD iter. 302/499: loss=6.723751442695679, w0=72.18878048780488, w1=16.243841318962044\n",
      "SubGD iter. 303/499: loss=6.723729704576005, w0=72.19219512195122, w1=16.241955329190542\n",
      "SubGD iter. 304/499: loss=6.723740750340349, w0=72.18878048780488, w1=16.24877395657819\n",
      "SubGD iter. 305/499: loss=6.723768127422312, w0=72.18536585365854, w1=16.23981620718351\n",
      "SubGD iter. 306/499: loss=6.723735516585591, w0=72.18878048780488, w1=16.237930217412007\n",
      "SubGD iter. 307/499: loss=6.723763301784719, w0=72.18536585365854, w1=16.244748844799656\n",
      "SubGD iter. 308/499: loss=6.723748806448577, w0=72.18878048780488, w1=16.242862855028154\n",
      "SubGD iter. 309/499: loss=6.7237270683289045, w0=72.19219512195122, w1=16.240976865256652\n",
      "SubGD iter. 310/499: loss=6.723750281456032, w0=72.18878048780488, w1=16.2477954926443\n",
      "SubGD iter. 311/499: loss=6.723755606229865, w0=72.18536585365854, w1=16.23883774324962\n",
      "SubGD iter. 312/499: loss=6.72373780491774, w0=72.1819512195122, w1=16.245656370637267\n",
      "SubGD iter. 313/499: loss=6.72376790832115, w0=72.18536585365854, w1=16.243770380865765\n",
      "SubGD iter. 314/499: loss=6.723746170201476, w0=72.18878048780488, w1=16.241884391094263\n",
      "SubGD iter. 315/499: loss=6.723724784589055, w0=72.18536585365854, w1=16.248703018481912\n",
      "SubGD iter. 316/499: loss=6.7237594600644615, w0=72.18878048780488, w1=16.24681702871041\n",
      "SubGD iter. 317/499: loss=6.7237430850374205, w0=72.18536585365854, w1=16.237859279315728\n",
      "SubGD iter. 318/499: loss=6.723747336033423, w0=72.1819512195122, w1=16.244677906703377\n",
      "SubGD iter. 319/499: loss=6.723765272074048, w0=72.18536585365854, w1=16.242791916931875\n",
      "SubGD iter. 320/499: loss=6.723743533954375, w0=72.18878048780488, w1=16.240905927160373\n",
      "SubGD iter. 321/499: loss=6.723734315704736, w0=72.18536585365854, w1=16.24772455454802\n",
      "SubGD iter. 322/499: loss=6.72375682381736, w0=72.18878048780488, w1=16.24583856477652\n",
      "SubGD iter. 323/499: loss=6.7237350856976885, w0=72.19219512195122, w1=16.243952575005018\n",
      "SubGD iter. 324/499: loss=6.72373103378961, w0=72.18195121951219, w1=16.24369944276949\n",
      "SubGD iter. 325/499: loss=6.723762635826946, w0=72.18536585365852, w1=16.241813452997988\n",
      "SubGD iter. 326/499: loss=6.723740897707273, w0=72.18878048780486, w1=16.239927463226486\n",
      "SubGD iter. 327/499: loss=6.723743846820418, w0=72.18536585365852, w1=16.246746090614135\n",
      "SubGD iter. 328/499: loss=6.723754187570259, w0=72.18878048780486, w1=16.244860100842633\n",
      "SubGD iter. 329/499: loss=6.723732449450585, w0=72.1921951219512, w1=16.24297411107113\n",
      "SubGD iter. 330/499: loss=6.723730826491732, w0=72.18878048780486, w1=16.24979273845878\n",
      "SubGD iter. 331/499: loss=6.723781164554846, w0=72.18536585365852, w1=16.240834989064098\n",
      "SubGD iter. 332/499: loss=6.72373826146017, w0=72.18878048780486, w1=16.238948999292596\n",
      "SubGD iter. 333/499: loss=6.7237533779361005, w0=72.18536585365852, w1=16.245767626680244\n",
      "SubGD iter. 334/499: loss=6.723751551323158, w0=72.18878048780486, w1=16.243881636908743\n",
      "SubGD iter. 335/499: loss=6.723729813203484, w0=72.1921951219512, w1=16.24199564713724\n",
      "SubGD iter. 336/499: loss=6.723740357607414, w0=72.18878048780486, w1=16.24881427452489\n",
      "SubGD iter. 337/499: loss=6.7237686433623995, w0=72.18536585365852, w1=16.239856525130207\n",
      "SubGD iter. 338/499: loss=6.7237356252130684, w0=72.18878048780486, w1=16.237970535358706\n",
      "SubGD iter. 339/499: loss=6.723762909051782, w0=72.18536585365852, w1=16.244789162746354\n",
      "SubGD iter. 340/499: loss=6.723748915076055, w0=72.18878048780486, w1=16.242903172974852\n",
      "SubGD iter. 341/499: loss=6.723727176956382, w0=72.1921951219512, w1=16.24101718320335\n",
      "SubGD iter. 342/499: loss=6.723749888723094, w0=72.18878048780486, w1=16.247835810591\n",
      "SubGD iter. 343/499: loss=6.723756122169957, w0=72.18536585365852, w1=16.238878061196317\n",
      "SubGD iter. 344/499: loss=6.723737412184804, w0=72.18195121951219, w1=16.245696688583966\n",
      "SubGD iter. 345/499: loss=6.723768016948628, w0=72.18536585365852, w1=16.243810698812464\n",
      "SubGD iter. 346/499: loss=6.723746278828954, w0=72.18878048780486, w1=16.241924709040962\n",
      "SubGD iter. 347/499: loss=6.723724540709282, w0=72.1921951219512, w1=16.24003871926946\n",
      "SubGD iter. 348/499: loss=6.723759419838777, w0=72.18878048780486, w1=16.24685734665711\n",
      "SubGD iter. 349/499: loss=6.72374360097751, w0=72.18536585365852, w1=16.237899597262427\n",
      "SubGD iter. 350/499: loss=6.723746943300486, w0=72.18195121951219, w1=16.244718224650075\n",
      "SubGD iter. 351/499: loss=6.723765380701526, w0=72.18536585365852, w1=16.242832234878573\n",
      "SubGD iter. 352/499: loss=6.723743642581853, w0=72.18878048780486, w1=16.24094624510707\n",
      "SubGD iter. 353/499: loss=6.723733922971799, w0=72.18536585365852, w1=16.24776487249472\n",
      "SubGD iter. 354/499: loss=6.723756932444839, w0=72.18878048780486, w1=16.24587888272322\n",
      "SubGD iter. 355/499: loss=6.723735194325166, w0=72.1921951219512, w1=16.243992892951717\n",
      "SubGD iter. 356/499: loss=6.723731048369283, w0=72.18195121951217, w1=16.24373976071619\n",
      "SubGD iter. 357/499: loss=6.723762744454425, w0=72.18536585365851, w1=16.241853770944687\n",
      "SubGD iter. 358/499: loss=6.723741006334752, w0=72.18878048780485, w1=16.239967781173185\n",
      "SubGD iter. 359/499: loss=6.723743454087481, w0=72.18536585365851, w1=16.246786408560833\n",
      "SubGD iter. 360/499: loss=6.723754296197739, w0=72.18878048780485, w1=16.24490041878933\n",
      "SubGD iter. 361/499: loss=6.723732558078066, w0=72.19219512195119, w1=16.24301442901783\n",
      "SubGD iter. 362/499: loss=6.723730694539623, w0=72.18195121951216, w1=16.2427612967823\n",
      "SubGD iter. 363/499: loss=6.7237601082073235, w0=72.1853658536585, w1=16.2408753070108\n",
      "SubGD iter. 364/499: loss=6.72373837008765, w0=72.18878048780483, w1=16.238989317239298\n",
      "SubGD iter. 365/499: loss=6.723752985203164, w0=72.1853658536585, w1=16.245807944626947\n",
      "SubGD iter. 366/499: loss=6.723751659950636, w0=72.18878048780483, w1=16.243921954855445\n",
      "SubGD iter. 367/499: loss=6.723729921830962, w0=72.19219512195117, w1=16.242035965083943\n",
      "SubGD iter. 368/499: loss=6.723739964874476, w0=72.18878048780483, w1=16.24885459247159\n",
      "SubGD iter. 369/499: loss=6.72376915930249, w0=72.1853658536585, w1=16.23989684307691\n",
      "SubGD iter. 370/499: loss=6.723735733840549, w0=72.18878048780483, w1=16.238010853305408\n",
      "SubGD iter. 371/499: loss=6.723762516318844, w0=72.1853658536585, w1=16.244829480693056\n",
      "SubGD iter. 372/499: loss=6.723749023703535, w0=72.18878048780483, w1=16.242943490921554\n",
      "SubGD iter. 373/499: loss=6.723727285583861, w0=72.19219512195117, w1=16.241057501150053\n",
      "SubGD iter. 374/499: loss=6.723749495990157, w0=72.18878048780483, w1=16.2478761285377\n",
      "SubGD iter. 375/499: loss=6.723756638110044, w0=72.1853658536585, w1=16.23891837914302\n",
      "SubGD iter. 376/499: loss=6.7237370194518675, w0=72.18195121951216, w1=16.245737006530668\n",
      "SubGD iter. 377/499: loss=6.7237681255761075, w0=72.1853658536585, w1=16.243851016759166\n",
      "SubGD iter. 378/499: loss=6.723746387456434, w0=72.18878048780483, w1=16.241965026987664\n",
      "SubGD iter. 379/499: loss=6.72372464933676, w0=72.19219512195117, w1=16.240079037216162\n",
      "SubGD iter. 380/499: loss=6.7237590271058405, w0=72.18878048780483, w1=16.24689766460381\n",
      "SubGD iter. 381/499: loss=6.723744116917599, w0=72.1853658536585, w1=16.23793991520913\n",
      "SubGD iter. 382/499: loss=6.723746550567548, w0=72.18195121951216, w1=16.244758542596777\n",
      "SubGD iter. 383/499: loss=6.723765489329004, w0=72.1853658536585, w1=16.242872552825276\n",
      "SubGD iter. 384/499: loss=6.723743751209332, w0=72.18878048780483, w1=16.240986563053774\n",
      "SubGD iter. 385/499: loss=6.723733530238862, w0=72.1853658536585, w1=16.247805190441422\n",
      "SubGD iter. 386/499: loss=6.723757041072319, w0=72.18878048780483, w1=16.24591920066992\n",
      "SubGD iter. 387/499: loss=6.723735302952646, w0=72.19219512195117, w1=16.24403321089842\n",
      "SubGD iter. 388/499: loss=6.723731062948958, w0=72.18195121951214, w1=16.24378007866289\n",
      "SubGD iter. 389/499: loss=6.723762853081904, w0=72.18536585365848, w1=16.24189408889139\n",
      "SubGD iter. 390/499: loss=6.72374111496223, w0=72.18878048780482, w1=16.240008099119887\n",
      "SubGD iter. 391/499: loss=6.7237430613545435, w0=72.18536585365848, w1=16.246826726507535\n",
      "SubGD iter. 392/499: loss=6.723754404825217, w0=72.18878048780482, w1=16.244940736736034\n",
      "SubGD iter. 393/499: loss=6.723732666705544, w0=72.19219512195116, w1=16.243054746964532\n",
      "SubGD iter. 394/499: loss=6.723730709119296, w0=72.18195121951213, w1=16.242801614729004\n",
      "SubGD iter. 395/499: loss=6.723760216834802, w0=72.18536585365847, w1=16.240915624957502\n",
      "SubGD iter. 396/499: loss=6.723738478715129, w0=72.1887804878048, w1=16.239029635186\n",
      "SubGD iter. 397/499: loss=6.723752592470227, w0=72.18536585365847, w1=16.24584826257365\n",
      "SubGD iter. 398/499: loss=6.723751768578116, w0=72.1887804878048, w1=16.243962272802147\n",
      "SubGD iter. 399/499: loss=6.723730030458443, w0=72.19219512195114, w1=16.242076283030645\n",
      "SubGD iter. 400/499: loss=6.72373957214154, w0=72.1887804878048, w1=16.248894910418294\n",
      "SubGD iter. 401/499: loss=6.72376967524258, w0=72.18536585365847, w1=16.23993716102361\n",
      "SubGD iter. 402/499: loss=6.7237358424680265, w0=72.1887804878048, w1=16.23805117125211\n",
      "SubGD iter. 403/499: loss=6.723762123585909, w0=72.18536585365847, w1=16.24486979863976\n",
      "SubGD iter. 404/499: loss=6.723749132331014, w0=72.1887804878048, w1=16.242983808868257\n",
      "SubGD iter. 405/499: loss=6.72372739421134, w0=72.19219512195114, w1=16.241097819096755\n",
      "SubGD iter. 406/499: loss=6.72374910325722, w0=72.1887804878048, w1=16.247916446484403\n",
      "SubGD iter. 407/499: loss=6.7237571540501335, w0=72.18536585365847, w1=16.23895869708972\n",
      "SubGD iter. 408/499: loss=6.72373662671893, w0=72.18195121951213, w1=16.24577732447737\n",
      "SubGD iter. 409/499: loss=6.723768234203586, w0=72.18536585365847, w1=16.243891334705868\n",
      "SubGD iter. 410/499: loss=6.723746496083912, w0=72.1887804878048, w1=16.242005344934366\n",
      "SubGD iter. 411/499: loss=6.723724757964238, w0=72.19219512195114, w1=16.240119355162864\n",
      "SubGD iter. 412/499: loss=6.723758634372904, w0=72.1887804878048, w1=16.246937982550513\n",
      "SubGD iter. 413/499: loss=6.723744632857688, w0=72.18536585365847, w1=16.23798023315583\n",
      "SubGD iter. 414/499: loss=6.723746157834613, w0=72.18195121951213, w1=16.24479886054348\n",
      "SubGD iter. 415/499: loss=6.723765597956485, w0=72.18536585365847, w1=16.242912870771978\n",
      "SubGD iter. 416/499: loss=6.723743859836811, w0=72.1887804878048, w1=16.241026881000476\n",
      "SubGD iter. 417/499: loss=6.723733137505926, w0=72.18536585365847, w1=16.247845508388124\n",
      "SubGD iter. 418/499: loss=6.723757149699797, w0=72.1887804878048, w1=16.245959518616623\n",
      "SubGD iter. 419/499: loss=6.723735411580125, w0=72.19219512195114, w1=16.24407352884512\n",
      "SubGD iter. 420/499: loss=6.723731077528632, w0=72.18195121951211, w1=16.243820396609593\n",
      "SubGD iter. 421/499: loss=6.723762961709384, w0=72.18536585365845, w1=16.24193440683809\n",
      "SubGD iter. 422/499: loss=6.723741223589709, w0=72.18878048780479, w1=16.24004841706659\n",
      "SubGD iter. 423/499: loss=6.7237426686216075, w0=72.18536585365845, w1=16.246867044454238\n",
      "SubGD iter. 424/499: loss=6.723754513452695, w0=72.18878048780479, w1=16.244981054682736\n",
      "SubGD iter. 425/499: loss=6.723732775333023, w0=72.19219512195113, w1=16.243095064911234\n",
      "SubGD iter. 426/499: loss=6.723730723698969, w0=72.1819512195121, w1=16.242841932675706\n",
      "SubGD iter. 427/499: loss=6.723760325462282, w0=72.18536585365844, w1=16.240955942904204\n",
      "SubGD iter. 428/499: loss=6.723738587342607, w0=72.18878048780478, w1=16.239069953132702\n",
      "SubGD iter. 429/499: loss=6.723752199737289, w0=72.18536585365844, w1=16.24588858052035\n",
      "SubGD iter. 430/499: loss=6.723751877205594, w0=72.18878048780478, w1=16.24400259074885\n",
      "SubGD iter. 431/499: loss=6.723730139085921, w0=72.19219512195112, w1=16.242116600977347\n",
      "SubGD iter. 432/499: loss=6.723739179408603, w0=72.18878048780478, w1=16.248935228364996\n",
      "SubGD iter. 433/499: loss=6.723770191182668, w0=72.18536585365844, w1=16.239977478970314\n",
      "SubGD iter. 434/499: loss=6.723735951095506, w0=72.18878048780478, w1=16.238091489198812\n",
      "SubGD iter. 435/499: loss=6.7237617308529725, w0=72.18536585365844, w1=16.24491011658646\n",
      "SubGD iter. 436/499: loss=6.723749240958493, w0=72.18878048780478, w1=16.24302412681496\n",
      "SubGD iter. 437/499: loss=6.72372750283882, w0=72.19219512195112, w1=16.241138137043457\n",
      "SubGD iter. 438/499: loss=6.723748710524286, w0=72.18878048780478, w1=16.247956764431105\n",
      "SubGD iter. 439/499: loss=6.723757669990223, w0=72.18536585365844, w1=16.238999015036423\n",
      "SubGD iter. 440/499: loss=6.723736233985993, w0=72.1819512195121, w1=16.245817642424072\n",
      "SubGD iter. 441/499: loss=6.723768342831065, w0=72.18536585365844, w1=16.24393165265257\n",
      "SubGD iter. 442/499: loss=6.7237466047113905, w0=72.18878048780478, w1=16.24204566288107\n",
      "SubGD iter. 443/499: loss=6.723724866591718, w0=72.19219512195112, w1=16.240159673109567\n",
      "SubGD iter. 444/499: loss=6.723758241639968, w0=72.18878048780478, w1=16.246978300497215\n",
      "SubGD iter. 445/499: loss=6.723745148797779, w0=72.18536585365844, w1=16.238020551102533\n",
      "SubGD iter. 446/499: loss=6.723745765101675, w0=72.1819512195121, w1=16.24483917849018\n",
      "SubGD iter. 447/499: loss=6.723765706583963, w0=72.18536585365844, w1=16.24295318871868\n",
      "SubGD iter. 448/499: loss=6.72374396846429, w0=72.18878048780478, w1=16.241067198947178\n",
      "SubGD iter. 449/499: loss=6.723732744772989, w0=72.18536585365844, w1=16.247885826334826\n",
      "SubGD iter. 450/499: loss=6.723757258327276, w0=72.18878048780478, w1=16.245999836563325\n",
      "SubGD iter. 451/499: loss=6.723735520207602, w0=72.19219512195112, w1=16.244113846791823\n",
      "SubGD iter. 452/499: loss=6.723731092108305, w0=72.18195121951209, w1=16.243860714556295\n",
      "SubGD iter. 453/499: loss=6.7237630703368625, w0=72.18536585365842, w1=16.241974724784793\n",
      "SubGD iter. 454/499: loss=6.723741332217189, w0=72.18878048780476, w1=16.24008873501329\n",
      "SubGD iter. 455/499: loss=6.723742275888672, w0=72.18536585365842, w1=16.24690736240094\n",
      "SubGD iter. 456/499: loss=6.723754622080174, w0=72.18878048780476, w1=16.245021372629438\n",
      "SubGD iter. 457/499: loss=6.723732883960501, w0=72.1921951219511, w1=16.243135382857936\n",
      "SubGD iter. 458/499: loss=6.723730738278645, w0=72.18195121951207, w1=16.242882250622408\n",
      "SubGD iter. 459/499: loss=6.72376043408976, w0=72.18536585365841, w1=16.240996260850906\n",
      "SubGD iter. 460/499: loss=6.723738695970086, w0=72.18878048780475, w1=16.239110271079404\n",
      "SubGD iter. 461/499: loss=6.723751807004353, w0=72.18536585365841, w1=16.245928898467053\n",
      "SubGD iter. 462/499: loss=6.723751985833075, w0=72.18878048780475, w1=16.24404290869555\n",
      "SubGD iter. 463/499: loss=6.723730247713399, w0=72.19219512195109, w1=16.24215691892405\n",
      "SubGD iter. 464/499: loss=6.723738786675665, w0=72.18878048780475, w1=16.248975546311698\n",
      "SubGD iter. 465/499: loss=6.723770707122757, w0=72.18536585365841, w1=16.240017796917016\n",
      "SubGD iter. 466/499: loss=6.7237360597229845, w0=72.18878048780475, w1=16.238131807145514\n",
      "SubGD iter. 467/499: loss=6.723761338120035, w0=72.18536585365841, w1=16.244950434533163\n",
      "SubGD iter. 468/499: loss=6.7237493495859715, w0=72.18878048780475, w1=16.24306444476166\n",
      "SubGD iter. 469/499: loss=6.723727611466297, w0=72.19219512195109, w1=16.24117845499016\n",
      "SubGD iter. 470/499: loss=6.723748317791348, w0=72.18878048780475, w1=16.247997082377807\n",
      "SubGD iter. 471/499: loss=6.723758185930312, w0=72.18536585365841, w1=16.239039332983126\n",
      "SubGD iter. 472/499: loss=6.723735841253059, w0=72.18195121951207, w1=16.245857960370774\n",
      "SubGD iter. 473/499: loss=6.723768451458544, w0=72.18536585365841, w1=16.243971970599272\n",
      "SubGD iter. 474/499: loss=6.72374671333887, w0=72.18878048780475, w1=16.24208598082777\n",
      "SubGD iter. 475/499: loss=6.723724975219196, w0=72.19219512195109, w1=16.24019999105627\n",
      "SubGD iter. 476/499: loss=6.72375784890703, w0=72.18878048780475, w1=16.247018618443917\n",
      "SubGD iter. 477/499: loss=6.7237456647378675, w0=72.18536585365841, w1=16.238060869049235\n",
      "SubGD iter. 478/499: loss=6.7237453723687395, w0=72.18195121951207, w1=16.244879496436884\n",
      "SubGD iter. 479/499: loss=6.723765815211442, w0=72.18536585365841, w1=16.242993506665382\n",
      "SubGD iter. 480/499: loss=6.723744077091768, w0=72.18878048780475, w1=16.24110751689388\n",
      "SubGD iter. 481/499: loss=6.723732352040053, w0=72.18536585365841, w1=16.24792614428153\n",
      "SubGD iter. 482/499: loss=6.7237573669547555, w0=72.18878048780475, w1=16.246040154510027\n",
      "SubGD iter. 483/499: loss=6.7237356288350805, w0=72.19219512195109, w1=16.244154164738525\n",
      "SubGD iter. 484/499: loss=6.72373110668798, w0=72.18195121951206, w1=16.243901032502997\n",
      "SubGD iter. 485/499: loss=6.723763178964339, w0=72.1853658536584, w1=16.242015042731495\n",
      "SubGD iter. 486/499: loss=6.723741440844668, w0=72.18878048780473, w1=16.240129052959993\n",
      "SubGD iter. 487/499: loss=6.723741883155735, w0=72.1853658536584, w1=16.246947680347642\n",
      "SubGD iter. 488/499: loss=6.723754730707655, w0=72.18878048780473, w1=16.24506169057614\n",
      "SubGD iter. 489/499: loss=6.723732992587979, w0=72.19219512195107, w1=16.24317570080464\n",
      "SubGD iter. 490/499: loss=6.723730752858318, w0=72.18195121951204, w1=16.24292256856911\n",
      "SubGD iter. 491/499: loss=6.7237605427172396, w0=72.18536585365838, w1=16.24103657879761\n",
      "SubGD iter. 492/499: loss=6.7237388045975655, w0=72.18878048780472, w1=16.239150589026107\n",
      "SubGD iter. 493/499: loss=6.723751414271415, w0=72.18536585365838, w1=16.245969216413755\n",
      "SubGD iter. 494/499: loss=6.723752094460552, w0=72.18878048780472, w1=16.244083226642253\n",
      "SubGD iter. 495/499: loss=6.723730356340878, w0=72.19219512195106, w1=16.24219723687075\n",
      "SubGD iter. 496/499: loss=6.72373839394273, w0=72.18878048780472, w1=16.2490158642584\n",
      "SubGD iter. 497/499: loss=6.723771223062846, w0=72.18536585365838, w1=16.240058114863718\n",
      "SubGD iter. 498/499: loss=6.723736168350463, w0=72.18878048780472, w1=16.238172125092216\n",
      "SubGD iter. 499/499: loss=6.723760945387099, w0=72.18536585365838, w1=16.244990752479865\n",
      "SubGD: execution time=0.010 seconds\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = 500\n",
    "gamma = 0.7\n",
    "batch_size = 1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start SubSGD.\n",
    "start_time = datetime.datetime.now()\n",
    "subgd_losses, subgd_ws = subgradient_descent(y, tx, w_initial, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SubGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "546fbb4275394efe9c69a202a87d851f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=501, min=1), Output()), _dom_classes=('widg…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ipywidgets import IntSlider, interact\n",
    "\n",
    "\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        subgd_losses,\n",
    "        subgd_ws,\n",
    "        grid_losses,\n",
    "        grid_w0,\n",
    "        grid_w1,\n",
    "        mean_x,\n",
    "        std_x,\n",
    "        height,\n",
    "        weight,\n",
    "        n_iter,\n",
    "    )\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(subgd_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Subgradient Descent\n",
    "\n",
    "**NB** for the computation of the subgradient you can reuse the `compute_subgradient` method that you implemented above, just making sure that you pass in a minibatch as opposed to the full data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_subgradient_descent(y, tx, initial_w, batch_size, max_iters, gamma):\n",
    "    \"\"\"Compute a stochastic subgradient at w from a data sample batch of size B, where B < N, and their corresponding labels.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(B, )\n",
    "        tx: numpy array of shape=(B,2)\n",
    "        initial_w: numpy array of shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        batch_size: a scalar denoting the number of data points in a mini-batch used for computing the stochastic subgradient\n",
    "        max_iters: a scalar denoting the total number of iterations of SubSGD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "\n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of SubSGD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of SubSGD\n",
    "    \"\"\"\n",
    "\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "\n",
    "    for n_iter in range(max_iters):\n",
    "       for minibatch_y, minibatch_tx in batch_iter(y, tx, batch_size):\n",
    "            subgradient = compute_subgradient_mae(minibatch_y, minibatch_tx,w)\n",
    "            loss = compute_loss(minibatch_y, minibatch_tx,w)\n",
    "    \n",
    "            w = w - gamma*subgradient\n",
    "           \n",
    "    print(\n",
    "         \"SubSGD iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]\n",
    "        )\n",
    "    )\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SubSGD iter. 499/499: loss=1.1654809906225532, w0=72.80000000000014, w1=16.596190855465327\n",
      "SubSGD: execution time=0.009 seconds\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = 500\n",
    "gamma = 0.7\n",
    "batch_size = 1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start SubSGD.\n",
    "start_time = datetime.datetime.now()\n",
    "subsgd_losses, subsgd_ws = stochastic_subgradient_descent(\n",
    "    y, tx, w_initial, batch_size, max_iters, gamma\n",
    ")\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SubSGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71f29c12f5034de7893074b9f5657256",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=1, min=1), Output()), _dom_classes=('widget…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ipywidgets import IntSlider, interact\n",
    "\n",
    "\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        subsgd_losses,\n",
    "        subsgd_ws,\n",
    "        grid_losses,\n",
    "        grid_w0,\n",
    "        grid_w1,\n",
    "        mean_x,\n",
    "        std_x,\n",
    "        height,\n",
    "        weight,\n",
    "        n_iter,\n",
    "    )\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(subsgd_ws)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
