{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from helpers import *\n",
    "\n",
    "height, weight, gender = load_data(sub_sample=False, add_outlier=False)\n",
    "x, mean_x, std_x = standardize(height)\n",
    "y, tx = build_model_data(x, weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000,), (10000, 2))"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape, tx.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NB: throughout this laboratory the data has the following format: \n",
    "  * there are **N = 10000** data entries\n",
    "  * **y** represents the column vector containing weight information -- that which we wish to predict/the output (see also the first page of $\\texttt{exercise02.pdf}$). Its **shape** is **(N,)**.\n",
    "  * **tx** represents the matrix $\\tilde{X}$ formed by laterally concatenating a column vector of 1s to the column vector of height information -- the input data (see also the first page of $\\texttt{exercise02.pdf}$). Its **shape** is **(N,2)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Computing the Cost Function\n",
    "Fill in the `compute_loss` function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(y, tx, w):\n",
    "    \"\"\"Calculate the loss using either MSE or MAE.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        w: numpy array of shape=(2,). The vector of model parameters.\n",
    "\n",
    "    Returns:\n",
    "        the value of the loss (a scalar), corresponding to the input parameters w.\n",
    "    \"\"\"\n",
    "    e = y  - np.dot(tx, w)\n",
    "    loss = np.dot(e.T, e) / (2 * len(y))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2694.483365887078"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test compute_loss\n",
    "w = np.array([1, 2])\n",
    "compute_loss(y, tx, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in the function `grid_search()` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "from costs import *\n",
    "\n",
    "\n",
    "def grid_search(y, tx, grid_w0, grid_w1):\n",
    "    \"\"\"Algorithm for grid search.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        grid_w0: numpy array of shape=(num_grid_pts_w0, ). A 1D array containing num_grid_pts_w0 values of parameter w0 to be tested in the grid search.\n",
    "        grid_w1: numpy array of shape=(num_grid_pts_w1, ). A 1D array containing num_grid_pts_w1 values of parameter w1 to be tested in the grid search.\n",
    "\n",
    "    Returns:\n",
    "        losses: numpy array of shape=(num_grid_pts_w0, num_grid_pts_w1). A 2D array containing the loss value for each combination of w0 and w1\n",
    "    \"\"\"\n",
    "\n",
    "    losses = np.zeros((len(grid_w0), len(grid_w1)))\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO: compute loss for each combination of w0 and w1.\n",
    "    # ***************************************************\n",
    "    for i in range(len(grid_w0)):\n",
    "        for j in range(len(grid_w1)):\n",
    "            w = np.array([grid_w0[i], grid_w1[j]])\n",
    "            losses[i, j] = compute_loss(y, tx, w)\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us play with the grid search demo now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid Search: loss*=4.443009009145579, w0*=72.72727272727272, w1*=13.636363636363626, execution time=0.919 seconds\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA14AAAITCAYAAAAXac30AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuNSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/xnp5ZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAC+7klEQVR4nOzde1zUZf7//8cAAyqeK0XSyupTVptGh0VLyso8dNzUWlsrMzezrDZoS9nEUFCxg3Qyrc9uaWVrm1rfti03S00ppc207dOn+nSwVTO0Xx4QDzDA/P64fDPvGQYYYIY58LzfbnMbmHnPe64ZyObJ67pel8PtdrsRERERERGRkIkL9wBERERERERinYKXiIiIiIhIiCl4iYiIiIiIhJiCl4iIiIiISIgpeImIiIiIiISYgpeIiIiIiEiIKXiJiIiIiIiEmIKXiIiIiIhIiCl4iYiIiIiIhJiCl4iIiIiISIhFVfBau3YtV111FampqTgcDt544w2v+2+55RYcDofXZdiwYV7H7N69mzFjxtCxY0c6d+7M+PHjKSsra8FXISLS+syePZvzzjuPDh060K1bN37zm9/w9ddfex1z+PBhJk2axFFHHUX79u0ZOXIkO3fu9Dpm69atXHHFFbRr145u3bpx//33U1lZ2ZIvRUREpEmiKngdOHCAfv36MW/evDqPGTZsGD/99FPN5a9//avX/WPGjOGLL75g5cqVvPXWW6xdu5YJEyaEeugiIq3aBx98wKRJk9iwYQMrV67E5XIxZMgQDhw4UHNMZmYmf//733nttdf44IMP2LFjByNGjKi5v6qqiiuuuIKKigo++ugjFi1axMKFC5k2bVo4XpKIiEijONxutzvcg2gKh8PB66+/zm9+85ua22655Rb27t1bqxJm+fLLLzn99NP517/+xbnnngvAihUruPzyy9m+fTupqaktMHIREfn555/p1q0bH3zwARdeeCH79u3jmGOO4ZVXXmHUqFEAfPXVV5x22mmsX7+e/v37884773DllVeyY8cOunfvDsCCBQuYPHkyP//8M4mJieF8SSIiIvVKCPcAgm3NmjV069aNLl26cMkll5Cfn89RRx0FwPr16+ncuXNN6AIYPHgwcXFxFBcXc+211/o9Z3l5OeXl5TXfV1dXs3v3bo466igcDkdoX5CItDput5v9+/eTmppKXFzzJiYcPnyYioqKII3Mm9vtrvVvYFJSEklJSQ0+dt++fQB07doVgI0bN+JyuRg8eHDNMX369OG4446rCV7r16/nzDPPrAldAEOHDuWOO+7giy++IC0tLRgvKypVV1ezY8cOOnTooP8viYi0sED/vx1TwWvYsGGMGDGC3r1789133/GnP/2J4cOHs379euLj4ykpKaFbt25ej0lISKBr166UlJTUed7Zs2czffr0UA9fRMTLtm3b6NmzZ5Mff/jwYXq2bcsvQRyTXfv27WutkX3ooYfIzc2t93HV1dXce++9XHDBBfzqV78CoKSkhMTERDp37ux1bPfu3Wv+fS4pKfEKXdb91n2t2Y4dO+jVq1e4hyEi0qo19P/tmApeo0ePrvn6zDPPpG/fvpx00kmsWbOGSy+9tMnnzc7OJisrq+b7ffv2cdxxx7EcSG7kufqPavIwgu/ecA+ged4+85JwD0Ei0OWfrwr3EJrncSh1Qa//Bx06dGjWqSoqKvgFmvRvVUMOACPKyti2bRsdO3asuT2QatekSZP4n//5H4qKioI8qtbL+l3x/XkEyuVy8e677zJkyBCcTmewh9cq6D0MDr2Pzaf3sPka+x6WlpbSq1evBv+/HVPBy9eJJ57I0Ucfzbfffsull15KSkoKu3bt8jqmsrKS3bt3k5KSUud56po6k0zjPsxccEMjDg61yeEeQPO82W8I7cI9CIlIay4YwtWfvRvuYTSd7d/3YE0Za+y/VY3RsWPHRn3Qv+uuu2oaG9n/KpiSkkJFRQV79+71qnrt3Lmz5t/nlJQUPv74Y6/zWV0P6/s3vDWwflca+/OwuFwu2rVrR8eOHfVBrYn0HgaH3sfm03vYfE19Dxv6/3ZUdTVsrO3bt/PLL7/Qo0cPAAYMGMDevXvZuHFjzTGrVq2iurqa9PT0cA1TGuHNfkN4s9+QcA9DIlxU/45E+R9F6uJ2u7nrrrt4/fXXWbVqFb179/a6/5xzzsHpdPL+++/X3Pb111+zdetWBgwYAJh/wz///HOvP6CtXLmSjh07cvrpp7fMCxEREWmiqKp4lZWV8e2339Z8v2XLFjZv3kzXrl3p2rUr06dPZ+TIkaSkpPDdd9/xwAMPcPLJJzN06FAATjvtNIYNG8Ztt93GggULcLlc3HXXXYwePbp1dTSM0g92Uf1hWlrcm/2ivPIVYyZNmsQrr7zC//t//48OHTrUrMnq1KkTbdu2pVOnTowfP56srCy6du1Kx44dufvuuxkwYAD9+/cHYMiQIZx++uncdNNNPPzww5SUlDB16lQmTZoU0BRHERGRcIqqitcnn3xCWlpaTeeqrKws0tLSmDZtGvHx8fz73//m6quv5pRTTmH8+PGcc845rFu3zut/yIsXL6ZPnz5ceumlXH755QwcOJDnnnsu5GOPmGmGCl3SikTt78294R5A8M2fP599+/YxaNAgevToUXN59dVXa44pLCzkyiuvZOTIkVx44YWkpKSwfPnymvvj4+N56623iI+PZ8CAAdx4443cfPPNzJgxIxwvSUREpFGiquI1aNAg6tt27J///GeD5+jatSuvvPJKMIclIRa1H54lIqjyFRkC2TKyTZs2zJs3j3nz5tV5zPHHH8/bb78dzKGJiIi0iKiqeEkzRWG1S6FLgkG/RyIiIhJuCl4tIGKmGUYZfViWYNLvk4iIiISTgldrEWXVLn1IllDQ75WIiIiEi4KXRBx9OJZQ0u+XiIiIhIOCV4hFxDTDKKp26UOxtAT9nomIiEhLU/CSiKEPw9KS9PsmIiIiLUnBK9ZFSbVLH4IlHPR7JyIiIi1FwSuEImKaYRTQh18JJ/3+iYiISEtQ8IplUVLtEgk3hS8REREJNQUvCSt94JVIod9FERERCSUFLwkbfdCVSKPfSREREQkVBa8QCfv6rgifZqgPuCIiIiLSmih4SYtT6JJIpt9PERERCQUFr1gUwdUufaiVaKDfUxERkdiTkwPt25vrcFDwCoGwTzMUkWZT+BIREYkthYVw4IC5DgcFr1ijapeIiIiISC2ZmZCcDFlZ4Xl+BS9pEQpdEo30eysiIhI78vKgrAxmzAjP8yeE52mlNdGH18ZZwO0hf46JPBvy54gVb/YbwtWfvRvuYYiIiEiUU/CKJRE8zVD8a4mQFejzKozVTeFLREREmktTDYNMjTW8qdrlbQG3e10iSSSPTSSSrV27lquuuorU1FQcDgdvvPFGzX0ul4vJkydz5plnkpycTGpqKjfffDM7duzwOsfu3bsZM2YMHTt2pHPnzowfP56ysrIWfiUiIhJKCl6xIgKrXQpdRrSGmWgdd6jo91nqcuDAAfr168e8efNq3Xfw4EE+/fRTcnJy+PTTT1m+fDlff/01V199tddxY8aM4YsvvmDlypW89dZbrF27lgkTJrTUSxARkRagqYYiIRBrYcX+elrzlERNORR/hg8fzvDhw/3e16lTJ1auXOl129NPP82vf/1rtm7dynHHHceXX37JihUr+Ne//sW5554LwFNPPcXll1/Oo48+Smpqashfg4iIhJ6Cl4REa6wOxFrYqotCmEjz7Nu3D4fDQefOnQFYv349nTt3rgldAIMHDyYuLo7i4mKuvfbaMI1URESCScEriMK2vivCphm2ttDVWgKXP9Zrb00BTFUvaY7Dhw8zefJkbrjhBjp27AhASUkJ3bp18zouISGBrl27UlJS4vc85eXllJeX13xfWloKmDVlLper0eOyHtOUx4qh9zA49D42n97DJvjmG/iv/6r5trHvYaDHKXiJNFFrDly+VAUTaZjL5eL666/H7XYzf/78Zp1r9uzZTJ8+vdbt7777Lu3atWvyeX2nRUrj6T0MDr2Pzaf3MDBdvvqKgVOn8p/LLuPz3/8ed3x8zX2BvocHDx4M6DgFLwmq1lDtUuCqX2uogqnqJY1lha7//Oc/rFq1qqbaBZCSksKuXbu8jq+srGT37t2kpKT4PV92djZZWVk135eWltKrVy+GDBnide7GjG/lypVcdtllOJ3ORj9e9B4Gi97H5tN72Ag7d5Jw5504Kis5vm1bel55JTgcjX4PrVkHDVHwinYRNM0w1kOXAlfjxHoAU/iSQFmh65tvvmH16tUcddRRXvcPGDCAvXv3snHjRs455xwAVq1aRXV1Nenp6X7PmZSURFJSUq3bnU5nsz5oNffxovcwWPQ+Np/ewwZUVsKNN8KOHXDaacQtXEhcYqLXIYG+h4G+zwpeIg1Q4GqeBdwes+FLBKCsrIxvv/225vstW7awefNmunbtSo8ePRg1ahSffvopb731FlVVVTXrtrp27UpiYiKnnXYaw4YN47bbbmPBggW4XC7uuusuRo8erY6GIiKhMmUKfPABtG8Py5dDhw4hf0oFryBp7Rsnx2K1S4EreGK1+qWqlwB88sknXHzxxTXfW1MAx44dS25uLm+++SYAZ511ltfjVq9ezaBBgwBYvHgxd911F5deeilxcXGMHDmSJ598skXGLyLS6rz2Gjz2mPl64ULo06dFnlbBK5pF0DTDWKLAFTqxGsCkdRs0aBBut7vO++u7z9K1a1deeeWVYA5LRET8+fJLGDfOfH3//TByZIs9dVyLPZPErFiqdil0tYxYep9j6fdfREQkppWWwrXXwoEDcPHFMGtWiz69Kl4ixFYQiBaqfomIiEiLcbtNpevrr+HYY2HJEkho2Sikile0ipBphrHw136FrvCKhfc/Fv47EBERiWmPPmqaaDidsHQp+Gxc3xIUvKRVi4UP/bFAPwcREREJmVWrTBdDgCefhP79wzIMTTUMgtba0TCa/8qvD/qRJ9qnHqrDoYiISATavh1Gj4bqahg7Fm4P32dAVbyiUYRMM4xWCl2RTT8fERERCYrychg1Cn7+Gc46C+bPB4cjbMNR8JImidZqlz7UR4do/TlF638XIiIiMSkzE4qLoUsXWLYM2rYN63AUvKTViNYP862Vfl4iIiKtR04OtG9vroPy2EWLPBWuxYvhxBODNtamUvCSRovGv+rrQ3x0isafWzT+9yEiIhJuhYVme63CwiA8dtMmmDjRfJ2bC8OHB2uYzaLgFW20vqvRovHDu3gs4Hb9DEVERGJcZiYkJ0NWVuOrX/bHsns3jBwJhw/DFVfA1KkhHXdjKHg1U2vraBhNf83XB/bYEk0/y2j670RERCQS5OVBWRnMmNH46lfNY3Or4cYbYcsWdnc+kZ6rXyLnociJO5EzEpEgiqYP6RI4/VxFRERin1cFqzFmzIB33oE2bbj88HJ+PNilSVMXQ0XBSwIWLX/F14fz2BYtP99o+e9FREQk0tirX3b1TkH8xz9g+nTz9XPPcdkf+zUtvIWQglc00fquBkXLh3JpHv2cRUREWp86pyB+/72ZYggwaRLcdFOd4S2cFLwkINHw13t9GG9d9PMWERGJXf6qW36nIB48CCNGwN690L8/zJ3b0kMNmIKXiEiIRMMfLERERCKRv+pWrSqW223axn/2GXTrBkuXQmJiWMYbCAUvaVA0fHhU9aN10s9dREQksjRnI2S7gBpszJ8PL70E8fHw6qtw7LHNe9IQU/CKFlrfVSd9+G7dIv3nHw1/uGgJa9eu5aqrriI1NRWHw8Ebb7zhdb/D4fB7eeSRR2qOOeGEE2rdX1BQ0MKvREREoO6A5a9S1ZQw1uAarfXr4d57zdcFBTBoUCNGHx4KXs3Qf1S4RyCR/qFbWoZ+DyLfgQMH6NevH/PmzfN7/08//eR1ef7553E4HIwcOdLruBkzZngdd/fdd7fE8EVExIdvwLLCVVpa7UpVY/flstQZ2HbuhFGjwOWC666D++5r1mtpKQnhHoBEtkj+a70+bIvdAm5nIs+GexhSh+HDhzN8+PA6709JSfH6/v/9v//HxRdfzIknnuh1e4cOHWodKyIiLS8z0wQpK2BZ4WrTJlOpqu/YQNkDW17ekRsrK2H0aNixA047Df7yF3A4mv16WoIqXhKVFLrEn0j9vYjkP2A0V2lpqdelvLy82efcuXMn//jHPxg/fnyt+woKCjjqqKNIS0vjkUceobKystnPJyIijec7FbC+NVlNbe3u95zZ2bBmjSmFLV8OHTo09SW0OFW8pE6R+mExUj9ci0Sq/qOgozO45yx1AUuhV69eXrc/9NBD5ObmNuvcixYtokOHDowYMcLr9nvuuYezzz6brl278tFHH5Gdnc1PP/3E3AhuHSwi0lrk5dmqUgHKyTHVrMxM/4+tdc7XXoNHHzVfL1wIffo0dbhhoeAVDdRYQyRgmnLYsrZt20bHjh1rvk9KSmr2OZ9//nnGjBlDmzZtvG7Psv3Js2/fviQmJnL77bcze/bsoDyviIi0LL9TCevy5Zcwbpz5+v77wWcNcDTQVEOJKqp2SSAi8fckUivIzdWxY0evS3MD0Lp16/j666/5/e9/3+Cx6enpVFZW8sMPPzTrOUVEJDysqYRpaQ10PSwthWuvNSnt4oth1qwWHWewKHiJX5H4ITESP0xL5NLvS3T6y1/+wjnnnEO/fv0aPHbz5s3ExcXRrVu3FhiZiIgEm7X2a9Omeroeut1w663w9ddmn64lSyAhsEl7wdpTLFgUvEREJOTKysrYvHkzmzdvBmDLli1s3ryZrVu31hxTWlrKa6+95rfatX79eh5//HE+++wzvv/+exYvXkxmZiY33ngjXbp0aamXISIiIVDvZsmPPgrLloHTCUuXQiP+2NbUNvahouAlUUHVC2mKSPu9icRKckv55JNPSEtLIy0tDTDrtdLS0pg2bVrNMUuWLMHtdnPDDTfUenxSUhJLlizhoosu4owzzmDmzJlkZmby3HPPtdhrEBGR0Kiz6+GqVTBlivn6ySehf/9GnbfeQBcGaq4R6cLQWCPSPhxG2odniS5qthEZBg0ahNvtrveYCRMmMGHCBL/3nX322WzYsCEUQxMRkUi0bZvZr6u6GsaOhdsb/3mwKZ0WQ0kVL4loCl0iIiIisc9rPVZ5OVx3Hfz8M5x1FsyfHzWbJNdHwUtEYp4CvIiISGTzWo+VmQnFxdCli1nf1bZtuIcXFApe4iWSphnqw7LEokj6b0xERKQuLd0R0FqPlZ26CObPpxoHLF4MJ54Y1nEFk4KXiLQKCvIiIiKBC1ZHwECDUl4elK3bRNY3EwGY7XwIhg8P2bjCQcFLIpI+JEso6PdKREQkMPaOgPbwFEiQsh/jLyj5Pcfu3TByJG05zDuO4UyvziEjo/ZxkdapsDHU1TCStXBHw0iZAqUPx/V7Z+2Ieu8ffuHyFhqJNNWb/YZw9WfvhnsYIiIidbJ3BGzf3js8WV/X1THQHrYyM821PSjZ78/Lw3QuvPFG2LIFevdmws6XcR2Mo6jIc7z1XJHWqbAxFLxEIlBD4aqpj1UoU3t5ERGRxrKHJ7fbO0hZVa3MTE8g8j0ePNe+9wOsGjSDS9a9wyHa8Pyly7klpSuFhZCWBps2RWd1yx8FL4korbXa1Zyg1dznURgTERGR+liBau5cE5rKyjz31apeUXe1zG/V6h//4JJ10wG4nWdZ/tezKCuL3qpWfaJqjdfatWu56qqrSE1NxeFw8MYbb3jd73a7mTZtGj169KBt27YMHjyYb775xuuY3bt3M2bMGDp27Ejnzp0ZP348ZfbfHpEW8M7aEV6XSBlLa9FaA76IiEhT1dXUoqE1V/Xe/913ZoohMN9xJ0ucN8dMdcufqApeBw4coF+/fsybN8/v/Q8//DBPPvkkCxYsoLi4mOTkZIYOHcrhw4drjhkzZgxffPEFK1eu5K233mLt2rVMmDChpV5CxIqE9V2x/mE4GgJONIwxVkTCf3MiIiJ18W2AUVeAysszFTC323/TDev+GTO8zztjykEYORL27oX+/bnjcCEVFZ7jYlFUTTUcPnw4w/20lQRT7Xr88ceZOnUq11xzDQAvvvgi3bt354033mD06NF8+eWXrFixgn/961+ce+65ADz11FNcfvnlPProo6SmprbYa5HWI1pDjH3csTgdUWu9REREarPWbJWXQ2WlZ4pgQ00t/E05rPs4Nyc/NhEqP4Nu3eC11yAxMfgvJsJEVcWrPlu2bKGkpITBgwfX3NapUyfS09NZv349AOvXr6dz5841oQtg8ODBxMXFUVxcXOe5y8vLKS0t9bpIcMVitSuWKkex9FpERESkblaAcjgCb9uek2OCmtPZ8PGZmXBv4nx+V/kSxMfDq69Cz54154nWzZEDETPBq6SkBIDu3bt73d69e/ea+0pKSujWrZvX/QkJCXTt2rXmGH9mz55Np06dai69evUK8ugllsRySIm11xaLgV9ERKQ5rCmFU6Z4TxGsT2GhqY4lJjZ8fK/t65lTca/5pqAABg3yOk+0bo4ciJgJXqGUnZ3Nvn37ai7btm0L/ZO24B5e4V5rEisffmMtlNSnNb1WERGR1sR3TVYgGmqwYVWyCjJ3csXCUSTiYqljFNx3n9dxaWne17EmZoJXSkoKADt37vS6fefOnTX3paSksGvXLq/7Kysr2b17d80x/iQlJdGxY0evi4ilNYeQWHjd4Qz+4f6jh4iISFPZpwU2FNYKC+HwgUouePK3HMsOvqQPE+KfN/MZbTZt8r6ONTETvHr37k1KSgrvv/9+zW2lpaUUFxczYMAAAAYMGMDevXvZuHFjzTGrVq2iurqa9PT0Fh+zRH+1KxaCR3O15uApIiLSWjVmWmBmJjzqzCaj+gPKE9szpu3r3JXdwe9xga4ri0ZRFbzKysrYvHkzmzdvBkxDjc2bN7N161YcDgf33nsv+fn5vPnmm3z++efcfPPNpKam8pvf/AaA0047jWHDhnHbbbfx8ccf8+GHH3LXXXcxevRodTSURlHYqC2a349o/wOAiIhIS2tMSMrrt5R7XY8CkPTKQj492IcZM+pupuF2h2DAESCqgtcnn3xCWloaaUcmfmZlZZGWlsa0adMAeOCBB7j77ruZMGEC5513HmVlZaxYsYI2bdrUnGPx4sX06dOHSy+9lMsvv5yBAwfy3HPPheX1RIJwTnWK1g+70RwwQk2BVEREJHrUFXwC6S7ou39XRkYdj/nySxg3znx9//1m764jfKtmDVXRor3rYVQFr0GDBuF2u2tdFi5cCIDD4WDGjBmUlJRw+PBh3nvvPU455RSvc3Tt2pVXXnmF/fv3s2/fPp5//nnat28fhlcj0UahInB6n0RERCKfFXTmzPEONP4CUF2hp6DAHFtU5Cc0lZbCtddCWRlr4wbxkHMWGRlmaVdGRu2qWUNVtGjvehhVwUtiR7RVuxQkGi/a3rNw/U6qwYaIiISLFXTcbu9A06WLuT5woP4wBp7+GHFxPqHJ7YZbb4Wvv+ZHx7FcV72Ex55IoKjI3F1UVLspR0NNOqJ9DZiCl0gDoi1ARBK9dyIiIpGvf3/vQLN9u+c+K2jV1ep98mTz2Acf9AlNjz0Gy5ZRgZMZZy5lj7M75eXeQc0S6BTCprS6jyQKXiL1UHBovmiaohltlVgREZGG5OSYjY2dztrBZs4cU8UqLvYONAMHmmuHwxPG/LV6z8kxwSwz0ycMrV5tEhnwB55g8Xf9cbvNJsvgCWqWaJ9CGCgFr1YsXFOcouXDbbSEhWih91NERCR4/FWJcnLAt1F3YSG4XCb0FBZ6P87qHmjvIpiTY8LV1KlQXe0JVJmZkJAAFRUNTD/cvh1++1uormbTmTfzUruJZGV5Kl0JCbWrVpmZJhiWl0dv44xAKHhFosnhHoAoJISG3lcREZHg8Bd6rNvsrKmBVvXK/rgpU0z1KTu79jl8G27k5ZlzuFzmPvCsuUpLM8deckE5Hx8/Cn7+Gc46i7TiBZQdcDBjhmdK4pQpnueyQiCYqpwVDmOVgpe0qGiodikchFakv7/R8DsqIiKtW06OqQ45nd6NJqwgZLdhg7mOjzdVJnuDCt+W8Dk5nuqTy+UJaFZAsqYKulymK6H1+E2bzLGjPsrk19XF7KYLcy9YBm3b1ozD3/ose8jz93qs1xrNLeTtFLxEbCI9FMQKvc/e1NlQREQao7DQhKDERO8gk5cHO3Z4H1td7X1dXwAqLDT3JyZ67uvSBfLzzf3x8Z7bi4q8w9ptiYu4k/lU4+BGXmbawhMbDE32ror+Xo/v2KKdgpfIEQoDLUvvt4iISNME0lY9P9+EHmv9Vlw9n/r97adlsXc4zM72Ps+BA+Z5epRs4rm4iQC8cNxDvMPlHDrk2ePLCk2+QcwKgf37m+/T0mof05QW8pFaJVPwkhYTyVO4FALCI1Lf90j+XRUREQmkrfozz5jQk5DgWcdVVyDxt59WQoL3MQMHmvvPP9/79i7sZvhfRsLhw6yIv5w7fjQnr642a8Ks0JST46mc+Vav7B0TfStcTWkhH6lVMgWvVkpTmzwi9cO/iIiISFPdeaenmYUVXKwKVEFB7eN9285PmeIdvvy1k3dQzcvcSG/3Fr53nMgNVS9DXBwJCeY8U6aYitXcuZ6GHFC7e6G9qhWMTZIjdaNlBS9p1RS6wk8/AxEREY9gTZObOtXTOMPpNKGqqsrc53DUfh572/n8fHOby2XOY+9cmJZmAllcHDzEDC7nHcrj2nB93DIOOLswZYp5XEWFCXtW9cntNudxOmt3L7RXtYKxSXKkbrSs4CUtIhKnbukDf+SIxJ9FJP7OiohI7PEXgOpbF9VYViMOl8uz3is93ft5evWq3YbePtUvM9M00zhwwFS8kpJgWPU/eIjpANxW/Swbq87y2xzDqj5lZ5swZLWVj7RqVEtQ8BKRiBCJ4UtERCTUfIOW7zQ5f+uVAg1jGRm1AxWYFvMHD5qv09K8G2gMHOhd4crJ8X7urCyYccv3vMyNAMzjTlb3vJmEBDh0yDNV0WKvPuXkeFrH2zdtbi0UvKRV0of8yKSfi4iItDa+Qct3mpy/9UqBNo8oKvJ87XB4rh0OT/DZtAk6dPAct2mTCV1WhSs/3zO90OmEj947yOD5I+jCXorj+pNJIdu3m3NWV/vfBNkKigUFnumMkdb4oiUoeEnIRdqULX24l0ikhjciIq1TQ+uR/N3vG8bqqoANHGiuHQ5Po4x27cx0P2ud1sGDsH+/5zEHDngHNjDfV1eDy+Xm5vUT6Vv9Gbvoxtq7XsOF2fTLWkuWkOBdLQNPUHQ4PMdoqqFIjFPoinyR9DOKtD8aSGRau3YtV111FampqTgcDt544w2v+91uN9OmTaNHjx60bduWwYMH880333gds3v3bsaMGUPHjh3p3Lkz48ePp6ysrAVfhYhEE98wVlcFbN06zwbFLpcJPlbgSUoywSvQKX/V1TCRBdzMS1QSz1s3vcr9T/Ssab6RnW0aakyZ4qmW+U6fnDLFHONyRV7ji5ag4CUiESeSwpdIQw4cOEC/fv2YN2+e3/sffvhhnnzySRYsWEBxcTHJyckMHTqUw4cP1xwzZswYvvjiC1auXMlbb73F2rVrmTBhQku9BBEJg2Bu8puW5n1tZ98M2e2G1as9+2lZ0w4bkpwMA1jPE/wBgGmJBdz64iDAEwLdbs90QktFhXl9kdplsKUpeLVCrXVKkz7Mi0goDB8+nPz8fK699tpa97ndbh5//HGmTp3KNddcQ9++fXnxxRfZsWNHTWXsyy+/ZMWKFfz5z38mPT2dgQMH8tRTT7FkyRJ27NjRwq9GRFpKoOu0/AU039v87bFlycvzDlf2aYTp6f4rXvbje/WCsu928hqjSMTFUkaS8MB9tR4za5Z5PZWVnnO4XK1zLVddFLwkpCJlqpZCV/TRz0xiwZYtWygpKWHw4ME1t3Xq1In09HTWr18PwPr16+ncuTPnnntuzTGDBw8mLi6O4uLiFh+ziLQMa/qd73ooX/4CmnVbQYFnby3rXKmp5pj8fM95H3zQ/7k//LD2bQMHwgUXeL7f90sljB7NsezgK/pwKy9QMMdRa7zV1f6fw18VrrVKaPgQEZHweGftCIZfuDysY1jA7Uzk2bCOQaJXSUkJAN27d/e6vXv37jX3lZSU0K1bN6/7ExIS6Nq1a80xvsrLyykvL6/5vrS0FACXy4XL5Wr0OK3HNOWxYug9DI7W9D5Om2YuRx1lQssTT5jv7fLzzTqsjh1NUHO5zG3x8aYToVVV2rjRHL9xI7Rta967+fNdVFfDggWwYwd89BEc+XtPvT791FTB2rY13z939BRYs4byxPbcmPAqle42JOBiwQIz3vx8eOYZOPlk+PFH6NkT9uzxrOX66itzHU0a+3sY6HEKXhLzVDkRkVgze/Zspk+fXuv2d999l3bt2jX5vCtXrmzOsAS9h8HSmt7Hl17yfP322973nX127fvPPhtefLHh8/73f3vew7ffhnvuMZfG6PHRR/z64bkAfHbvneScvwXYUms8f/5z/efxfV3RItDfw4PWpmgNUPCKNJPDPYDYotAV/SKh6iXSVCkpKQDs3LmTHj161Ny+c+dOzjrrrJpjdu3a5fW4yspKdu/eXfN4X9nZ2WTZejGXlpbSq1cvhgwZQseOHRs9TpfLxcqVK7nssstwOp2NfrzoPQyWWH0f8/PhkUfM18nJpgIFZlqgtcHxAw/UnhLo73FWhWnSJHN8fj489phnqt+JJ7qYOXMlt956GXFxzprnGjbMu+I1YIDn++Rk6NzZVKwsp1Z/yboKs0ny3IQspj6VD0+Z+xISzFquhATPnmBZWZ7xW2O8807zvfX11KlNfw9bUmN/D61ZBw1R8JKQiZT1XSIi4dK7d29SUlJ4//33a4JWaWkpxcXF3HHHHQAMGDCAvXv3snHjRs455xwAVq1aRXV1Nenp6X7Pm5SURFJSUq3bnU5nsz6sNvfxovcwWGLlfczJMeuxyss9TSf++EezlxXAxInm/qwsyM2t/fjp02HVKtMQ45xzzOOmTzch69FH4f33TUONzEyYM8dM6du2zTz28GEnBw86SUoygai4GA4d8px71SrP13/8o2nCYWnPfl7ht7SnjNUM4oHKOVRVmtiQkGCmR27fbqZBVleb4Jaba15vQYHntc6Y4TnmscfM2KNJoL+Hgf6uqrmGxCxVu2KHfpYSycrKyti8eTObN28GTEONzZs3s3XrVhwOB/feey/5+fm8+eabfP7559x8882kpqbym9/8BoDTTjuNYcOGcdttt/Hxxx/z4YcfctdddzF69GhSrVXyIhKV7BsHJyebYGK1VLdCWWam923t20NGBiQmmqBl9dgpLvbcZ7WDt++XZXUntJYbWd+73eb4uppcOJ3mGKuToQM3ixzjOI2v2M6x/JZXqbLVaiorTeiyzm3fyLmw0BO6LNXV5jnq2zA5mK31I5mCl4hIA1S9lfp88sknpKWlkXbkU01WVhZpaWlMO7JK/oEHHuDuu+9mwoQJnHfeeZSVlbFixQratGlTc47FixfTp08fLr30Ui6//HIGDhzIc889F5bXIyLBY9842L6PVU6OJzwVFpow5XDAzJmeQOVymRBjhRu323OfnRVq+vevfyybNpnzWDp08EwZzM/3hK/qRx5jhHsZFTgZxVJ+xrv5j724k5Dg/boyM81tTqdpQ29xuerfqDnQ1vrRTsFLYpIqJLFHP1OJVIMGDcLtdte6LFy4EACHw8GMGTMoKSnh8OHDvPfee5xyyile5+jatSuvvPIK+/fvZ9++fTz//PO0b98+DK9GRILJ38bBVuiypKV5wpQVsnr2NN87HJCd7b0Jsj8zZvjfw8suLQ3sPSD274ekJO9AdJF7NVX3m4YDf+AJiqmd5hITPV/7zobOyzMhq6ICtm71XtNVX6iyAmp9VbFYoOAlIhIhWuvm5iIirYk9gOTkeAemjAwTQqypfAkJMHu2CWrWFD6n0+y1ZamsNGHo4EHvjY99FRXVrjpZjT0AjmU7r/Jb4qlmETezgIm1KlcdOniHwIbCXl6eCV8NhSp/ATUWKXhJSIRzapYqI7FLP9votXbtWq666ipSU1NxOBy88cYbXvffcsstOBwOr8uwYcO8jtm9ezdjxoyhY8eOdO7cmfHjx1NWVtaCr0JEpPms6o613sv+/dq1pkmGxe2uvWbK7YZ16zzhy+32TOWrbzpffRIpZymj6MbPbOIsJrIAcNC9u6dZB5gqmT1MpaWZIJiYWPf6rNYSqgKh4NXK6C/qIhIOBw4coF+/fsybN6/OY4YNG8ZPP/1Uc/nrX//qdf+YMWP44osvWLlyJW+99RZr165lwoQJoR66iEhQ5OSYgFJQ4N1QwzeYWOEpIcGsDfNVXW3O5bvWKxBOp+kyaOdwwBNxmfSnmD10ZiTLOIzZPdmqvFkyMrzHvGmTCYYul3ldraFBRnMoeElMUUUk9ulnHJ2GDx9Ofn4+1157bZ3HJCUlkZKSUnPp0qVLzX1ffvklK1as4M9//jPp6ekMHDiQp556iiVLlrDD2qRGRCSCFRZ6Gmb4rnfKyTGhKD7eU+GqqoI1a2qfx+pS2BgDB3qafNiDl8MB4+IWMbF6PtU4GMNitnBizX2+LrrI+3t7Mw2Hw0xdnDNHAawuCl4iIgFQZ8PQW7NmDd26dePUU0/ljjvu4Jdffqm5b/369XTu3Jlzzz235rbBgwcTFxdHsdVrWUQkgmVmmoCSkFB7vZO195W1CTKYgOWvqlXfdMK4OHN+X0VFZg3Y6tXegaqfexNPV00EYLbzIf4Zd3m9z2PvwBgXZ4KhtYFyerp398XGdChUO3mRKKNKiEjLKy0t9bqUl5c36TzDhg3jxRdf5P3332fOnDl88MEHDB8+nKqqKgBKSkro1s27pXFCQgJdu3alpKSk2a9DRCRY6goReXmm25/LVXvfLnvgAk846tChcc9dXe2pmPlWrKwg53KZ+7qwm2WMpC2H+QeXUzE5h/PPN8f6Tke0+HZgtLe937TJTD+cMqXxHQpbSzt5P5lYRCSyvbN2BMMvXB7uYUSPe4FgdyYvA5ZCL3u7K+Chhx4iNze30acbPXp0zddnnnkmffv25aSTTmLNmjVceumlzRysiEjLsYeIvDxzW0aGCSkDB5rGGBar0uVweDYyzs42nQwrK00zi6byV7GyqlO4q3mZGzmRLXxPb6akvsSWwriadvO+QdDy4Yf+b7dX8fLyPK87UJmZ5v1SO3mRRtKUrBDLbeAi0oK2bdvGvn37ai7Z2dlBOe+JJ57I0UcfzbfffgtASkoKu3bt8jqmsrKS3bt3k5KSEpTnFBEJBn97UllVoqIi057d4TDXR4r6gJmqV1lpNlG2go/DYc5VX5v4xrDC2DRmcDnvcIg2jGA5X+3qyoED3mHNX9XNfr+111hGhncVzy7QKYS+DUZideqhgpfEhJicZphbx6Upj4tBMfkzj0IdO3b0uiQlJQXlvNu3b+eXX36hR48eAAwYMIC9e/eycePGmmNWrVpFdXU16b47eIqIhFl5ualmWcHBav3eq5enU+D27Z71WAkJnnBWXW2m+iUnwwUXmNsuuCB44ety/kEu0wG4nWf5jLP8VriskGWvuvXs6Wl9v22bOWbt2rqDUlOnEMbq1EMFL5FIkktowlIuMR/EJLKVlZWxefNmNm/eDMCWLVvYvHkzW7dupaysjPvvv58NGzbwww8/8P7773PNNddw8sknM3ToUABOO+00hg0bxm233cbHH3/Mhx9+yF133cXo0aNJTU0N4ysTEfFWWOhpsV5YaMLIhg1mKuFPP3mO69ULJk/2dBu0wllcnJlumJlpwtiBA2b9VLt2zR9bb77nZW4EYD538BI3AybsDRxoAmB9Ae/HH814Vq/2hK2MDNNl0V9Q8lf9C0RTHxfpFLwk6kV95SOXlg9ELf180up98sknpKWlkZaWBkBWVhZpaWlMmzaN+Ph4/v3vf3P11VdzyimnMH78eM455xzWrVvnVUFbvHgxffr04dJLL+Xyyy9n4MCBPPfcc+F6SSIiftlbrGdleQcxa+pgTg5s3eqZYud2m3A1daqZfrh6tXfL+IqKuhteBKotB1nOCLqwl/X05w887nX/pk0mgNXVNdFagwYmEFphy9550TcoNXXz5FjddFnNNUTCJTfcA8B7DLl1HBPB1GQjegwaNAh3PT2Q//nPfzZ4jq5du/LKK68Ec1giIkGVk2OC1pQpngYTbreZdmiFmrQ0mDvXfJ2XZx5jhayCAs80OzuXy1yazs187uAsPmMXx3Adr+EiseZeh8O0m7f/M52c7OnCCGbz5/rG4HTGXlAKNlW8RFpaLpEZcnKJzHFFEDWOERGR+lihyb6JcF4e9O/vafVuTR/Mzzf3FxR4Hl9dXTt0WZqzxmsiCxjLi1Q74hjNq/xIT6/73W7v0JWRYSp3brfneY9MWKhh7UlmSU+PzYYYwaTgJVEt6qYZ5oZ7AAHIDfcAREREQisYXfN8z5GTY5pqWFPy7Gue/G2EDCZ8WftugXcb9+Rk72Pj45s2znQ28AR/AOAB9xxWc3GDj9myxTM2K5BZUyGtqZIVFSZQglkftmlTbDbECCYFLwkqVQTqkEt0BZpcoma8URe+RUQk7JrSNc83aM2Z46luWeesrDRT8qxAYlWJrMYZlkCqV9aeWhZ7QAvUMexiKaNIxMVSRvIY99V5rH1MVudFuy5daq+9+ugjz3WsNsQIJgUvkVDLDfcAmiE33AMQEREJvqaEBN+wZlWCrGv7OTdtMrcVFZmmGEVF3vth9exZf7OMmo2Obcc3Vry7klf5LT35ka84lXG8ANSd+C64wFTr6uIvjFkVuurq2G2IEUwKXhK1Ir7SkUtsBJfccA9AREQkuJoSEnzDmlXVqqrynrLodptj7d+D935Y27bhd+8s38dY/IWehkyvnMrFrGE/7bmW1ymjQ73HFxVB9+6e73v18p7umJFRu+pnVfIyMvyfM1Y3Qm4qdTUUCYXccA8gyHJ9rkVERFqZvDxPp0LwVLXcbrMeyun07N1VVmbus7eEb0k9PvqIayrnAjCOF/iK02od06GDdxgE74Bn7TnmdJoujTNmmBBlVf3y8mDduvrHYa8S2t+71koVL5Fgyw33AEIoN9wD8C/iq58iIhJzMjO9p+a53d4Vsby85u+91RSnVn/J2U8+CcAj/JFljKp1TEJC3d0TLVYXxsRET2WwsVM0te7Lm4KXSDDlhnsALSA33AMQERGJHA6HCWDZ2Z7pizk5pgthfdMJ6zpXc7RnP3+tuJ6Ew4f5IO4ispnt97jKytpjy8kxXQstcXGe0JSTY17jrFmmc2M92zJ60bovbwpeEpUissKRG+4BtKDccA9AREQkvAoLzdRCt9u7KmTdZw82gQQqh8NUoprSSMNw8wLj6OP+mkNHHcXYxJepCnBVkdXMIy/PhC+rkpeZaV6X1bHRqoKpZXzTKHiJBENuuAcQBrnhHoCIiEjw5OSYABUXZ659G0JYjSIyMsx1ly6e+yoqTDMKh8Oz+bAlLg4efLDh53e7TZBrSiMNgPt4jFEsowIn/7r/fnY5ujf8INtz5+eb8a9ZY16/PWBlZppQGBdnrjV1sGkUvESaKzfcAwij3HAPQEREJDjsFSyrSYZdQYFZF1VUZK7tAckemIqKzGMHDvRspmzt9RUqg1jNHCYDcL9zLnv69GnwMU5n7f3FwIzfWpuVlmZCJpjXWFVlrjV1sGkUvESaIzfcAxBLRE4/FRGRsAu0pbnVLMNas+Vb1bGmC1prn+qbEnjgAHz4oakaWUEuVI5lO6/yW+KpZhE389/xE/weZ1XkLC6XCVm+4ctqCFJWZjo3+ttoWm3im0bBS6KOPmBHmNxwD0BERKRuvhsf1yUvz0wZrK42175VncmTTeB68EFTCbJXvKw1UfZ9r9zuwJtQNFUi5SxlFN34mc304w7m17mgbNs2s0myr6Ii7++rq2tPMayo8A5Zgb6n4k3BS6SpcsM9gAiSG+4BiIiI+NfUlua+a77A06HPHlZ69vRUtNLSGt5UOJgKyaQ/xeyhMyNYziHa1Xu8b8jy5XDUbonvcJjXZ58uqTbxTaPgJdIUueEeQATKDfcAWsYCbg/3EEREpBGa2tK8vjVfVlHJ4fCufBUVwWefma8b2ly4uW5mEXcyn2ocjGExWzix2edMSPB0MrRUVnpfg9rEN5WClwSNPpCKiIhILMjJMftVxcV51nxZjSZycjxTCP1NJdy/P/Tj68dmFjARgOk8xDtc3qTzJCeb9vHJyea1ulxmry6n09PZMT7eHGtdS9MpeIk0Vm64BxDBcsP79Fr/JyIiTWFvFpGTY1qrV1aaMGKt+aqr0URD/HUObI7O7GE5I2jLYf7B5eTR9A4XaWmmepWZ6dl3zNqry6ryTZligll2dpBeQCum4CXSGLnhHkAUyA33AERERBrH3izCHqyqqsy1VQGzuh36C1MOh/9Ohw2tq2oMB9W8zI2cyBa+pzc38RLuRn6ct/fe2LTJXNtfc5ztdFYw07TC4FDwkqiiioaIiIg0h79W6Glpnmv75scJCea6oMDTGn7GDLN+y7oPTBCrroYTTgjx2MnjCt7mEG0YwXL20LXR57CP22qOYTXLyMmBP/3Jc/+GDc0csHhR8BIJVG64BxBFcsM9ABEREf/8tUK3Kj/WtdNpAsqUKeZ7+zQ8MB0L7c0mPvzQhLlgVrd8DedtHmI6ABNZwGecVe/xCQme6laHDuY6I8PTFj8nx1PFsle18vI87fHt1THt3dV8Cl4igcgN9wBEREQkGOzVLYu9PbrVzRBg9mzTVMIevDIyagcst9uEuVDpzfcsZgxxuHmGO3iRsQ0+prra0/xj/37TRGPtWs/9q1fXHaSscGYFT9DeXcGg4CUioZEb7gGIiIjUZlW1ioo8ocNe8bFCmMNhqlpW6LKEsqrlT1sOspwRdGEv6+nPvTwe0ON8x23tw2UFqKIic23fn8uqakHtdV3au6v5FLxEGpIb7gGIiIhIsNjXcBUWejZKdjrN11aXP/tUQjDruJKTg9+lsH5u5nMHZ/EZuziG63gNF4lNO5Pbu0mIxf467VUt36mFarLRfApeIhI6ueEegIiIiLe8PM/eVfaphZWVpo18Roa5tu/RNXAgDBoEBw+2bMVrIgsYy4tUEcdveZUf8dM20YdVsfKVnW1ea2WlCZpWk434eE/ISkvzfl/qCmHSNApeIvXJDfcAREREJNh8pxbaK0D+glVRkels6G/D5FBJZwNP8AcAplDAGi4O6HFlZf5vnzHDs66tSxfP7ZWVMHOmCVmbNtWecukbwqTpFLxEJKZoywEREamL70bJVnVo8mT/x9q7+ll7erWEY9jFUkaRiIuljORR/hjwY+0h0tKrl7m21rdt3+49xdDtrr1+y9+6N63vap6Ehg8RiQz6QB2lclHlUEREWlROjqnOWOu5rK99Kzf1VXFWr/aucLVUtSueSpYwmp78yJf0YRwvAI4GH2exOjLabd1qrq33IC3N7NFlha+MDLjoIpg717zOvDzvx+fl1b5NGk8VL5G65IZ7ACIiItIU9oBl/9peufH3tVUZgpZZy+Xwk6dm8ScuYTX7ac8IllNGh6A8lz2MrlsHSUnm9uRk02Ze0wlDL+aCV25uLg6Hw+vSp0+fmvsPHz7MpEmTOOqoo2jfvj0jR45k586dYRyxSCuQG+4BiIhIa2Lfqysz0zSSqKiANWvM7atXe0KI223Wb1VUwNix/qfqhYpvFW0Ey3iARwAYxwt8xWmNPmecz6d76/UUFJhgVVBgvvedPqjphKEXc8EL4IwzzuCnn36quRTZ/mSRmZnJ3//+d1577TU++OADduzYwYgRmsImPnLDPQARERFpKmst06ZNZopcUpKZgmftXWVdFxaafawqK839hYVmvVdCGBbjnMpXLOQWAB7hjyxjVJPO4xu8rE2Qreqade3bHl7t4kMvJoNXQkICKSkpNZejjz4agH379vGXv/yFuXPncskll3DOOefwwgsv8NFHH7Fhw4Ywj1pEREREgsFfNceu55Gu7Glp3lWnrl3NdVKSaSHvbypgKLRnP69zLR0oYzWDyGZ2k89lrdtKTjbTC60gNXmyuW3KlNp7l0nLiMng9c0335CamsqJJ57ImDFj2HpkReHGjRtxuVwMHjy45tg+ffpw3HHHsX79+jrPV15eTmlpqddFRBopN9wDEBGRWGV1KMzI8HQq9K3mWHt35eRASYm5fcMGT0UIYNs2T2v1Dz9sqYYabp7nVk7jK7ZzLKNZQlUQ+t/t2OFdvbI2hp4711T5rL3LrDVd2qsr9GIueKWnp7Nw4UJWrFjB/Pnz2bJlCxkZGezfv5+SkhISExPp3Lmz12O6d+9OifVfoB+zZ8+mU6dONZde9pWXEntywz0AERERCVROjtnw2HcKoS9rKp3b7akKVVbCrFmeYxwOT9hyu6FDcPpa1Os+HuM6llKBk1EsZRfdm3wuhwMGDDBf5+eb9yYuztzesaPnfXK7TbUrIcFTFVRzjdCLueA1fPhwrrvuOvr27cvQoUN5++232bt3L3/729+afM7s7Gz27dtXc9m2bVsQRywiIiIiTWUPCgMH1t8gwgppdtXVnq99K1z793umJYbCRaxhDmYTsT/wBMX0b9b53G4oLjZfP/KIea3Wa9q/3/vYyZNN1cuqilkbSZeXq+oVKjEXvHx17tyZU045hW+//ZaUlBQqKirYu3ev1zE7d+4kJSWlznMkJSXRsWNHr4uIiIiIhJ+1nisnBwYNMrfVNUXQ6ugHJqQlJNRuRuFr+/agDLOWY9nO37ieeKpZxM0sYGJQzmsPknZWd0OHw3uKoSUvz6z78nefBEfMB6+ysjK+++47evTowTnnnIPT6eT999+vuf/rr79m69atDLDqsiISOrnhHoCIiMQaezc+a7rcnDnea76sayuUOJ1mLyuXC/70p5YfcyLlvMZ1dONnNtOPO5hPfZsk+2tx73R6Aqe9CYjvR9q4OBNEExPN9wkJdVcF1VI+tGIueP3xj3/kgw8+4IcffuCjjz7i2muvJT4+nhtuuIFOnToxfvx4srKyWL16NRs3bmTcuHEMGDCA/v2bV9qVGJEb7gGIiIhIU1nBwe32XvNlXcfFeTr7gQlkvlMPW8JcshjABvbQmREs5xDt6j0+MdF7yqPTaV6DFTgffNATwlasMMdYLfGtip61n5n1tb+28WopH1ph2KUgtLZv384NN9zAL7/8wjHHHMPAgQPZsGEDxxxzDACFhYXExcUxcuRIysvLGTp0KM8880yYRy0iIiIizZWXZ67nzPFUgezTDrOzvUOFbavXFnMTLzIJ89lzDIvZwokNPubAAXOxTJ5cu2Oh9dpdLs/tVkCzWE1FCgs9x0vLibngtWTJknrvb9OmDfPmzWPevHktNCIRERERaSmFhd7hw5KRAatXm0AWFxeeKYZ9+YxnuR2AXB7iHS5v0nmstWozZ5pg2bMn7NljKlnTppn7KitNpcwKaPZ1W5pKGB4xN9VQRERERCJXc/aL8t34NyPDBKmMDM8x1nRDO7cb1q71VLiqq01osTgc/tdRBVNn9rCcEbTlMG8znBlMa/K5rAYYVjVv+/bareB912rZm5BoKmF4KHiJSMvKDfcAREQknJq6X5TVCt6+8a8VpIqKPIEOzDqlgQPN1w6HJ+RZt4H3FMTqajN9L1QcVPMyN3IS37OFE7iRl3E382P4wYO1b/OtZLndtd8Xha7wUfASseSGewAiIiKxr6md8+xBLSEB0tI8jSPi4sz0O3ugs7eWz883VbF162Dq1NrnzskxjwvVhsk55HEFb3OINoxgOXvo2uxz+rbM961kWd0dtTFy5FDwEolmq4vDPQIREZFGaWrnPPtUOZcLNm3ytIevrjaVLXug8w0aRUUmfBUWele+wASzAwdqbzIcDMN5m4eYDsBEFrCZtKCc1+HwTJGsa/qg260W8ZEk5ppriMSsukKWv9svTg/tWERERMLEqvRkZpoQlZZmQlhWlnf4yMys3Srempq4YUPLjLU337OYMcTh5hnu4EXGBu3c8fG1m4jk5MCCBfDnP5uwdccd5j1RB8PIoOAlEsmaWtGyP04hTEREYoB9ypzVPt1foLCmDWZmmmmF/vbpstqqh1JbDrKMkXRhLxtIJ5PgzvWrrjZrtzIzzfeFhVBR4dmra8eO0DcMkcbRVEORSBWsaYSrizUlUUREol6gU+ZmzjQBbeZME8wSwlJmcDOfO0hjM7s4hlEspYKkoJzZ2gQ6Ls4TRK1Q6nZ7wpa/wNmcjpLSfApeIpEmVEFJ4UtERKKYtTbMChdxcWa6nbXGyWJNRXS7TchISWn5sU5kAWN5kSri+C2v8iM9g3bu888378OUKZ4gaoXS7GzTbh/gkUdqByw12ggvBS+RSBLqcKTwJWGydu1arrrqKlJTU3E4HLzxxhs197lcLiZPnsyZZ55JcnIyqamp3HzzzezYscPrHCeccAIOh8PrUmDtIioirUZhoZkq6Hab6XZWa3lLT1vGOXDA7HHVktLZwBP8AYApFLCGi5t1Pt/pgps2mWt7kxL713fe6TnWN2Cp0UZ4KXiJRIqWCkUKXxIGBw4coF+/fsybN6/WfQcPHuTTTz8lJyeHTz/9lOXLl/P1119z9dVX1zp2xowZ/PTTTzWXu+++uyWGLyIRJDPTTB90OEzVKyHBEyRyclo+aNkdwy6WMopEXCxlJI/yx6Ccd+BAE8B8X2v79qZTo336oNUu31/AampHSQkONdcQgfDv4dXSYWh1cXibbuQS/vdcWtTw4cMZPny43/s6derEypUrvW57+umn+fWvf83WrVs57rjjam7v0KEDKeGYNyQiESUpyQQw3+YavhUep9NUxlqimUY8lSxhND35kS/pwzheABx+j3U4au/DBWYfMaulvcNhgpbLZboxJiSY6YVWaCooMK/L6tRoNR2xqLlG5FHFSyTcwlWBUuVLIti+fftwOBx07tzZ6/aCggKOOuoo0tLSeOSRR6hsiU9TIhJRrHVK+fmeKk9GhgkqBw54H1tZGVjostaMNccs/sQlrKaMZEayjDLq3o35wQdr35aR4b2PWHw8TJ7s+b6y0vs1Oxyea00fjA4KXiLhFO7wE+7nl6hXWlrqdSkvL2/2OQ8fPszkyZO54YYb6NixY83t99xzD0uWLGH16tXcfvvtzJo1iwceeKDZzxdqVVVV5OTk0Lt3b9q2bctJJ51EXl4ebtufu91uN9OmTaNHjx60bduWwYMH880334Rx1CKRy2qfDp4gYlV9LFYo8VdV8sfl8mzG3BQjWMYDPALAOF7gS06v93h/HQfXrvWMG8zXeXmeqYMWq6o3ebKpgiUkmPekrumD6mQYOTTVUKS1C/e0Qwm5t8+8hHYdg/vP/cHSSmAVvXr18rr9oYceIjc3t8nndblcXH/99bjdbubPn+91X5btz7l9+/YlMTGR22+/ndmzZ5OUFJw2zaEwZ84c5s+fz6JFizjjjDP45JNPGDduHJ06deKee+4B4OGHH+bJJ59k0aJF9O7dm5ycHIYOHcr//u//0qZNmzC/ApHwsO/HlZdnKkJFRWa9k11+vmmoYV/b5S9wDRxYO6AFw6l8xUJuAeAR/shSrmv0Oax/Si+4wDPG9CP/a7amD86ZY16X9U9hXl7tvc3AvB9nn22up0/3f4yEhypeIuGiapPEgG3btrFv376aS3Z2dpPPZYWu//znP6xcudKr2uVPeno6lZWV/PDDD01+zpbw0Ucfcc0113DFFVdwwgknMGrUKIYMGcLHH38MmGrX448/ztSpU7nmmmvo27cvL774Ijt27PDq/ijS2liBoaDAVGysQFJU5N25EODHHz1f17WuKRShqz37eZ1r6UAZqxlENrObdJ5t28xUxw0bPLcVFXkaZ4DZHNnl8q5s+etS+Mwz3tfqZBg5VPESCYdIC12qekkTdezYscGAFAgrdH3zzTesXr2ao446qsHHbN68mbi4OLp169bs5w+l888/n+eee47/+7//45RTTuGzzz6jqKiIuXPnArBlyxZKSkoYPHhwzWM6depEeno669evZ/To0bXOWV5e7jWts7S0FDDvo8vlavQYrcc05bFi6D0MDvv7eN99JjxYgaNtW89xhw+b248+2txnN2CAuV6/PsSDdbtZVHELp1V/xY8cy7g2L5HocAPN+x2wB8eNG831E0/AggXQty/8+9+mZfzUqSastW1rpiVa78Ndd5kv7r7bhcsF06aZC9R+r8S/xv73HOhxCl4iYih8SQiVlZXx7bff1ny/ZcsWNm/eTNeuXenRowejRo3i008/5a233qKqqoqSkhIAunbtSmJiIuvXr6e4uJiLL76YDh06sH79ejIzM7nxxhvp0qVLuF5WQKZMmUJpaSl9+vQhPj6eqqoqZs6cyZgxYwBqXmv37t29Hte9e/ea+3zNnj2b6dOn17r93XffpV27dk0eq293SWk8vYfBsXLlSs4+G/7857qPefttePHFuu8/MpM3ZE564w1+tXA51QkJfJd/N0/02RjaJ/Tx9tt4vUdvv22uzzrLXPfrt7LmNmmaQP97PnjwYEDHKXiJtLRIq3aJtIBPPvmEiy/2bCJqrdcaO3Ysubm5vPnmmwCcZX1iOGL16tUMGjSIpKQklixZQm5uLuXl5fTu3ZvMzEyvdV+R6m9/+xuLFy/mlVde4YwzzmDz5s3ce++9pKamMnbs2CadMzs72+u1l5aW0qtXL4YMGdKkCqTL5WLlypVcdtllONV/ukla63uYn2+qUlYFprms93Hz5suYM8e8j1bziGeegUmTTEfAYcNMRat9e7MvVUu7sGoN/6gwqS/LMZfnpk9s8rmOPRZ27qzdfdF63VOnet7ngwc969esvcysdV9Wp0T77+KcOU6eeaZ2pUzq19j/nq1ZBw1R8BIRkZAbNGiQVxc/X/XdB3D22Wezwb74IYrcf//9TJkypWbK4Jlnnsl//vMfZs+ezdixY2v2Jdu5cyc9evSoedzOnTtrBVFLUlKS34YiTqezWR/6m/t4aX3v4WOPmXVYjz1mGjk0ldVI4777TBXn6aedHDpk3ken05zbfv5Vq8z1oUPNGHwTHct2XmQM8VSziJt5wnUXuPzv11UfKzR9/33dHRVnz/a89unTPQ1GHA5zqa4274+/nkZOp5PHHnNy4IDn/Wruz6m1CfS/50D/m1dzDZGWFOnVrkgfn0gUOnjwIHE+GwTFx8dTfeSTVu/evUlJSeH999+vub+0tJTi4mIGWItVRCJUsBo3WI00rIYQd95pAoW1aXBODiQmmjVNiYnNH3dTJVLOa1xHN35mM/24g/nUtUlyQ6y/N9lDl+/nd7fbux38pk3m9nbtPPuO1fd3K+vnM3CgGmxEAgUvERGRELrqqquYOXMm//jHP/jhhx94/fXXmTt3Ltdeey0ADoeDe++9l/z8fN58800+//xzbr75ZlJTU/nNb34T3sGLNCAvz0z1q2sPqUBlZprQYfWMmTrVu4tfYaH52u32bhDhcDR/4+PGmEsWA9jAHjozkmUcoulrKn05nd4bJgNkZ3u3g7cH3SlTzNf1NZO1fj7r1gXn5yTNo6mGIiIiIfTUU0+Rk5PDnXfeya5du0hNTeX2229nmtVmDHjggQc4cOAAEyZMYO/evQwcOJAVK1ZoDy9pNaw9qXzXOVm6dDHhwy4hAVJSvPfv8mVN5wuGm3iRSZiS3BgW8z0nBefER0yZ4gmZBw6YUDVjhhl/YaEJWzNmeO/FpX25oosqXiItJVqm8UXLOEWiRIcOHXj88cf5z3/+w6FDh/juu+/Iz88n0TZfyuFwMGPGDEpKSjh8+DDvvfcep5xyShhHLRIa9mlzvqxqjj/+wlVSUv2hC4IXuvqxmWe5HYBcHuIdLm/yuZKTTUXPcWSGosNh3g+rGuU7fTNYVUUJPwUvEREREWkR9mlzvvLyYMeO2rfn5HhCisXhgLS0uqcZWs0ngqEze1jOCNpymH9wOTOY1vCD6pCQYMZdWOgJhW63d6hS0IpdCl4iIiIi0iKa0ozDHlLAPP7BB013v7q6Abrdwal2OajmZW7kRLbwPb25iZdwN+Hjsz0EbtrkPW0yWAFRIp+Cl4iIiIi0iECrOfn5nimJaWnmtg4dzLVVMbILVQf/qeRzBW9ziDaMZBl76NqoxzscZlphwpGuCpWVntA1cKAZd3y8/6mX9U3LlOik4CXSEqJt3VS0jVdERKKWv4DxyCMmoMyZYypbAPv3m+viYk8Ys6SkBL9yNIx3yCUXgNt5ls2k1f8AP9xuKCjw3zRk0yZzf2Wl/6mX9U3LlOik4CUiIiIiYZGTY6pbBw7AzJmQmup9v711vKWyEnz3U9+2LXiNNAB68z2v8DvicDOfibzEzU0+V3W1/7EdOGDCYl1TL4O1R5pEDgUvEREREQmaxkyRKyjwfO12e6bhJSTUP32wrrbzwdCGQyxjJF3YywbSuZfHm3U+f+vQnE4TqqZMqXvqpZpsxB4FLxEREREJmsZMkatremBSkql2+XYtdDg866VCw8187iCNzeziGEaxlAqSAnpkYzZybtPGvEerV3vfrnVdsU3BS0RERESCpjFT5NLTzbXVOMNy552e7oX2vb3cbhNaQtVMYwLPcQuLqCKO3/IqP9Iz4MfW1WHRH2u9WlERZGSYQJmRoXVdsU7BS0RERESCpjFT5DZtMtdWELEUFZlzrF7t3XrdOtbf2q/m+jXFPMXdAEyhgDVcHPTn8A2YGRme5iFFRVrXFesUvEREREQkqBqaMmfd79ud0LJ+vbm2QkmoHcMuljKKRFwsZSSP8seQPM/hw571azk5sHYt9DxSVOvVS+u6Yp2Cl4iIiIgElX3KnL8QVlBg7vftTti+vbm2wsjAgaEfazyVLGE0vdjOl/RhHC8Awd/VODnZ0z4+MdETrvbsMde7dwf9KSXCKHiJiIiISFDZp8zZQ5i1nqmqyhzncHhCVocOptoDsH27CWqDBoV+rDN5kEtYzX7aM4LllNGh3uN7Br7sy0tZmeli6DuVUNMLWw8FL5EodRZf8w5/oB//F+6hiIiI1MjJMSErM9NUdezBwpo66HZ72qlbe3D5NqeYM8fs8RVKI1jGZB4GYBwv8BWnNfiY7dv9317fBs4Oh3lfCgqgosJ7X6+8PPMezZ1rgqm6GsYuBS+RKHU97zOMYq7n/XAPRUREpIZV4ZozxzN10Fq3ZE0dzMgwt7ndnqCRmelpFe90BndDZH9O5SsWcgsAj/BHljGqWeerb7wJCeZ9qaw0jUF8uxZa71lRkbnOz1f4ikUKXiJR6lrWeF2LiIhEAqvCZW2IbF/nNWgQTJ1q1nY5nTBzpjlm5kxz3HnnmXOce27j2rM3Vnv28zrX0oEyVjOIbGaH7skwlT0rWDqdpqmIvbJlvWf2NW1qKR97FLxEQm11cdBPeQI76MNWAE7jPxzPjqA/h4iISGPZpxna1zNZFZ38fFMJc7lM9ceqElkhzepmuH59KIOXm+e5ldP4iu0cy2iWUEXTd2XOyfGs+7LvL+Z0esLU3LmwZo3ZGHryZNNG317ZsroZrltngmlycu1wZn8+TUeMTgpeIlHoSoqoOtJxqRoHV/JhmEckIiLi3UjD3ho9M9NzjNttQklCQtMbVTRHFnO5jqVU4GQUS9lF9yafKyPDvD5rnVpioue+9HTz+q2QZZ9GaG+j71vZst4363F1TUtURSz6KHiJRKFrWFvztdvnexERkWALtMpib6SRk2OCiFUFsio52dmmwYTLBT/+GPqx213EGuYwGYA/8ATF9G/W+ayKnb99yTZtMrdXVJiQaZ9GuGmT5/2oq5thZqZ578rLvd93dUGMXgpeIlGmAwe4iE3EY/61j8fNID6lPQfCPDIREYlVjamylJeb7n32KYX2x7ndnrbyoW6gYXcs2/kb15NAFYu4mQVMbPY5i4o80yutqtbAgd5TLF0uM8XQPo0wK6vhzZLz8kxw9X3/tMly9FLwEokyQyjGSZXXbU6qGELw15KJiIhA4FUWe+e+ykrPlELfdV5WW/mW4qSC17iObvzMZvpxB/MJ1ibJc+bUrnTZp1ja3zerdfzs2SZUNaaCKNFPwUskylzFOlzEe93mIp6raOH/i4mISKsRaJXFvpYrPt4zpdB3nVdLKySTAWxgD50ZyTIO0S5o566sNGHLkpXlqeitWePZo8sKWfW1lfel6lZsaXoLFxEJqlR20Z3d9R7jAK6myG/F6xrWcTZf0dCsjZ10ZQfdmjdYERERP/LyzHVhYe0qTV6emYJYWWlCSbt2nmDmKyEheFMRb+JFJvEMAGNYzPec1Oxz2sfmdptwZb3mGTM8mykXFZnW+ZWVns2gMzPN++Bw1F/JsneItN5XiW4KXiIR4q/kcCGfNXhcdR1TIzpRxsYjG0HW5wPOYhALGjs8ERGRgOTlmYvVcMKqdBUWQv/+pjrkG1B8VVUFJ3T1YzPPcjsAuTzEO1ze/JP6YQWjuXPNuAcO9EyntL/GwkJTwfIXpHyDlm+HSIl+mmooEiH+zDUcIrHOYGWJq6OmVdftlmocHCKRv3B1k8coIiISKHtwmDPHfF1c7D11zt7pzy4Yoasze1jGSNpymLcZzgymNf+kR/iOz95gwz59MC7O7GdmvU77WjBfvo/X+q7Yo+AlEiFe4nLOYRHf0IuqIP+nWUUc/8dxnMMiXgrRX/tERETs7MHBPi0vJ8c03XA4Qtdkw0E1L3MjJ/E9WziBG3kZd5D+32q1hk+wzRsrKDDdHJ1O83qt11VdbULmhg3m+w0bPOu/MjK8z+uvEYfWd8UWBS+RCPIlvTmbRbzIcACqm3k+6/GLuJyzWcSX9G7mGUVEJNYEukdXY4+1B4cpUzx7eFnrvBoS14xPqVPJ5wre5hBtGMFy9tC16Sez6dnTrElbt85cT51qAlhlpadhhjXVEKBXL/N+VR1Zmm0Pm76hU0Er9il4iUSYg7TlVnIYSw7lJNbqYBgoF/GUk8jNTGM8UzlEmyCPVEREYkFj9uhqzLF29lBR17ouu2OPNdWiphjGO+SSC8BEFrCZeub3NdL27Z7wmZFh3gffcRYWwqBBJmiWlJj3KyHBfG+fduhb8ZLYp+AlEqFe5ArOYRHfc2yjpx5WEcd39ORsTS0UEZEGNGYtUVPXHVnT6+LiID294eN//LFx57f05nsWM4Y43MxnIi8ytmknqkNcnKnYWZslHzjgCV4Oh/fGyQcOeCp76eme4LlunamKrV0b1KFJFFDwEolg1tTD5VzUqMct5yLOZhFfaWqhiIg0oDFT3Oo6tr4piBkZnml1brdZ52St8QqmNhxiGSPpyh6K+TX38nizz+k73fH8872nSSYkmOmEABdcUHvj5Pgjk1aKigKbnimxTcFLJMIdpC0/cXTAUw5dxLODYzS1UERE6tSYtVqBsE9BzMmBxEQTrnJyaq9lqqryrIUKHjfzuYM0NrOLYxjFUipIavZZfacR+r4WhwO2bTNf2xtnrFljQtiUKZ5jGzs9U2KPgpdIhHNQzW95r9amyXVxUsVoVuJodmsOERGJVYGs1aovnPneZ1V40tLMRsEul6kMzZlT+7Fud/MaZ/hzO89yC4uoIo7RLGE7vYJ2bn+VuQ4dzLV98+fKSu/GGYmJ5uupU9UWXgwFL5EIdz7/pjt7at1e7XNt1509DODzkI5LRESiV31rtayKVX5+3eHMt8JlbfxbXOx9nD2YWDIyvCtJTmfzXsuvKeZJ7gEgm9ms5pLmndDG4fBuG2/Zv9//8fZA6XKZ4KluhWJR8BKJcNfzfq1phlbHwrmM9tv50EU81/N+Sw5TRESiSH1hoLDQOzD5C2f24GYPYXVNH4yLM8fn5NQ+xl84C9Qx7GIpo0jExTJG8Aj3N/1kfrjd9Y/Pel09e5rvzz/fVLjsjxexKHiJRDB/0wytjoXnsIj7uNdv50NNNxQRkabKzDRVqIQEE5Tqq9S43d4hbMoU76qPVc2qrjbHzZgRvE2T46lkCaPpxXa+4lTG8QIQ5I4dDXjwQRNg9xyZmFJUZNZ3WdMLs7MDW08X7DV3EpkUvEQimH2aYV2bIde16bKmG4qISFMlJpoQVVfosle58vJMqCoogJkzPdMIk5O9q0UzZ5prqzrUXDN5kEtYTRnJjGA5++kYnBPXIyHBs+arVy+YO9eEpcxMzzFFRd4VxUDW0zV1fzSJLgpeIhHset7HDVQ2sBmy76bLlcThPvJ4ERGRxvBdv+WvEuO7RsyanmifWlde7mlCAZ77Skq8z1XXOqr6jGAZk3kYgHG8wJec3rgTNEFCgmmgYb2ObdvM+1RQYIJWXRsjB7L3WVP3R5PoouAlEqGsaYYO4NsjUwsb2gzZ2nT5O3riAE03FBGRRrOHAGuz4IICz/32ZhpWRaxLF8/9CQmekGJvQmHtd1Xl06TX7fbeG6shp/IVC7kFgEe5j6VcF/iDfdiDoaWujosOh2cTaHtQtF5PXRsjB9JcQw04WgcFL5EI1ZZyvuNYnudKr6mFDbGmHr7AFXzHsbSlPMQjFRGRWGIPAda0OntLdasiNnOmJ4hs3+59jpSU2ufdts0c25yGE+3Zz3JG0IEy1nARUyho+EH1sIKh/fX57t1lsSp61dWQlORZv+Z2myqX1mhJQxS8RCLUQdoykOf8Ti0M5LG3ksNAnuMgbUM0QhERiXWTJ3uqO1a4SEszFTErQPnuy1VZ6R3E7KGmeV3+3DzPrZzOl/xIKr/lVapo5BzFOvjbqwtMRczetbBXL081cPJkz3FFRVqjJQ1T8BKJYO5m/ifa3MeLiEjrlpdnqjsulydcFBVBRUXt9Vv26pi9gYbbHZyGGlnM5TqWUoGTUSxlF92bf9Ij6qpy7d9vplRaQXLbNs8Uy7w8T/fCgQO1RksaFtRPZcW+u+aJiIiISERpbHtza82X1TwCTBCzr99yuz3VrHbtPO3VLb5TERvrItYwB1NiyqSQDQxo3gkD1KuX2QTZLj/f00DDmpa5bp3/NVpqEy92QQ1e113X9MWNIjHr4vRwj0BERKRGY9ub28PF1Kl1T8uzHDzo3WyjuY5lO3/jehKo4kVu4hnuDN7Jj8jI8N9oY/duT6C0v+6iosBCldrEi12jJ8Zef/31fm93u93s3r272QMSERERkdDJzDRBoKH25vZj7J0M27UzYcLh8L9my+2uu8IVF2c2HX7hhcCqYE4qeI3r6MbPbKYfE1lAKDZJXrfO/+3W2jWr5X337mbcGRm1w6k/gbzX0no0uuL13nvvMXbsWCZNmlTrkpycHIoxhsS8efM44YQTaNOmDenp6Xz88cfhHpKISMxau3YtV111FampqTgcDt544w2v+91uN9OmTaNHjx60bduWwYMH880333gds3v3bsaMGUPHjh3p3Lkz48ePp6ysrAVfhUhsaEp7c3vISEsztzVU+fKnutq0pg906mEhmQxgA3vozEiWcYh2jX/SZti/37N3l8tlplC63XDRRWafMqez/lClNvFi1+jgNWjQIDp06MBFF13kdRk0aBB9+/YNxRiD7tVXXyUrK4uHHnqITz/9lH79+jF06FB27doV7qGJiMSkAwcO0K9fP+bNm+f3/ocffpgnn3ySBQsWUFxcTHJyMkOHDuXw4cM1x4wZM4YvvviClStX8tZbb7F27VomTJjQUi9BpFWz1nmlpZlpdlC7IUWgQczlCuy4G3mJSTxz5OuX+Z6TAhxt83To4Kl0WXuPWcrLTfVvzhzP3mMKVRKogIPXt99+C8Dy5cu58MIL/R6zcuXK4IwqxObOncttt93GuHHjOP3001mwYAHt2rXj+eefD/fQRERi0vDhw8nPz+faa6+tdZ/b7ebxxx9n6tSpXHPNNfTt25cXX3yRHTt21FTGvvzyS1asWMGf//xn0tPTGThwIE899RRLlixhx44djR7P2LFjWeu7y6mINGjDhrrva16reG99qzfzHOYPK9OZxttcEbyTN2D/fk+oPP54T+dCp9OErcJCT+hqzMbPIgEHrzPOOIOrrrqK999/P5TjCbmKigo2btzI4MGDa26Li4tj8ODBrF+/3u9jysvLKS0t9bqIiAi1/m0sL2/8ht1btmyhpKTE69/lTp06kZ6eXvPv8vr16+ncuTPnnntuzTGDBw8mLi6uSR119+3bx+DBg/mv//ovZs2axY8//tjoc4i0JtZUw8pKs9bJakThryFFcznLynilYjRtOczbDGc6DwX/SXzE1fGJuKjIM11w8mRPy3hrb7OE4GwjJq1EwL8u3377Lc8++yxjxozh6KOP5g9/+AM33XQTbdo0bmPXcPv//r//j6qqKrp39977oXv37nz11Vd+HzN79mymT5/eEsMTEQm6vzAOZ5DXRbg4CKyil888nIceeojc3NxGnaukpATA77/L1n0lJSV069bN6/6EhAS6du1ac0xjvPHGG/z888+89NJLLFq0iIceeojBgwczfvx4rrnmGpxOZ6PPKRLLMjNNG3Uw+3odOmS+3r/ftJnfsCE41R+Hu5qzCwtJcX/P9/TmRl5ukT0p69rHy2obD54GGnPnQno6bNqkphnSOAH/Jvfq1Yv8/Hy2bdvGn/70JxYtWkTPnj3Jzs5m27ZtoRxj2GVnZ7Nv376aS6y/XhGRQG3bts3r38fs7OxwDylgxxxzDFlZWXz22WcUFxdz8sknc9NNN5GamkpmZmat5h4irU1Ghlm3ZYUPp9NUeLKyardWr6pq/Pn9VcumVM4iZeNGDtGGkSxjD12bNvhmsKpYycmmiYa9ZfycOabyV1ysphnSeAEHr4qKCnbt2sX333/PiSeeyJ/+9CfGjRvH008/zcknnxzKMQbV0UcfTXx8PDt37vS6fefOnaSkpPh9TFJSEh07dvS6iIgItf5tTEpKavQ5rH976/t3OSUlpVYDpMrKSnbv3l3nv92B+umnn1i5ciUrV64kPj6eyy+/nM8//5zTTz+dQm2+I61UTo6niUZRkZlq6HKZylBeHvhOeGrK+i77BswAw3iHBytNWeke59NsJq0JIw+cbzMQh8O87ilTPFMKfffhsl5nMNezSesRcPBq06YNJ598MsOHD2fixIkUFBTw1VdfcfXVVzN+/PhQjjGoEhMTOeecc7zWqlVXV/P+++8zYEDL7IIuIiIevXv3JiUlxevf5dLSUoqLi2v+XR4wYAB79+5l48aNNcesWrWK6upq0tMbv0m5y+Vi2bJlXHnllRx//PG89tpr3HvvvezYsYNFixbx3nvv8be//Y0Z+nO2xIhANvu18/2bw8GDZh2UNSXPNzQ11wlsYTFjiMPNlmHDWJxwc3CfwMfAgZ7wlJBggtaDD5oKlr0FvNXN0ZpSOGWKOd4KaSKNEfAar+uvv56VK1dy9dVXc88993DiiSeGclwhlZWVxdixYzn33HP59a9/zeOPP86BAwcYN25cuIcmEhkubvwHWZH6lJWV1XTHBdNQY/PmzXTt2pXjjjuOe++9l/z8fP7rv/6L3r17k5OTQ2pqKr/5zW8AOO200xg2bBi33XYbCxYswOVycddddzF69GhSU1MbPZ4ePXpQXV3NDTfcwMcff8xZZ51V65iLL76Yzp07N/EVi0SWQDb7Bc9GyWlpZg1TeblnH6v6qjy+reYbow2HWM4IurKHjx2/Zuf48fBB48/TGJs2eb6urjbvjbWGzf7+5OXV/j7Q91LEV8AVryVLlvDZZ5/VbDj8m9/8hjVr1oRwaKHz29/+lkcffZRp06Zx1llnsXnzZlasWFFrYbdI0CjISCv3ySefkJaWRtqRnVezsrJIS0tj2rRpADzwwAPcfffdTJgwgfPOO4+ysjJWrFjh1cBp8eLF9OnTh0svvZTLL7+cgQMH8txzzzVpPIWFhezYsYN58+b5DV0AnTt3ZsuWLU06v0ik8a3cWHwrYVaoKCoyj+nfP7DzZ2XV32q+bm7mcwdpbGYXxzAmaQnVLdDcJi3NvB8DB3o31pg1y/N1XVXCut5LkYY0qk1Mz549KSgo4D//+Q9Dhw5l4sSJnHXWWSxcuDBEwwudu+66i//85z+Ul5dTXFzcpKkqIiISmEGDBuF2u2tdrP9/OBwOZsyYQUlJCYcPH+a9997jlFNO8TpH165deeWVV9i/fz/79u3j+eefp3379k0aTzR25RVpSF1BwapiZWaaqpW/oGVNLezSxfO4wkLvylB9Cgqa1tVwAs9xC4uoIo7RLOFHR8/Gn6QB/jZ23rTJTCf0fX3V1XW/Nxb7VESRxgg4eD399NPMnj2bP/3pT0yePJni4mL69OnD999/H1VrvERERERiUV1BwX677zG+1Zvt2z2Py8oy99tNnWq6G/qGGZer8eP9NcU8xd0ATKGA1VzS+JMEwN8UyQMHzJo1q/LV05b3rPfmSIG+5trS2PVyIpaAg9fixYtZu3YtW7ZsobKykh49ejBgwAAeeeQRXnnllVCOUURakqZFiohEpbqmwNlv9z3Gt3ozcKC5djg8gcUKWRkZ5viKCmjXzK0Bj2EXSxlFIi6WMYJH+WPzTtgEbren8rVtmwmV9vfGqoZt2uQdtuoKuCINCbi5xvr160M5DpHYd3E6rC4O9yhERCRG+TaCqOv2uhpCZGSYtV1W6LK3UHc6zRoupxNSUkzwaKp4KlnCaHqxna84lXG8APiZD9gCDhwwr3vdOs/7ZIUsq8FIVpbZNNkKW5mZ5lprvKSxAg5eItIKqNolItLqWIHLYlW6rCl2RUXeUwnt0xGbYiYPcgmrKSOZESxnPy23P6q9kmcpKjJhywqkVkXLqoaBJ4hmZXlazos0VqOaa4hIMynYiIhIiAW6Bsk6rq4W8Js2Bd5cI1AjWMZkHgZgHC/wJacH9wka4HabfbicTu91Xfn5nvfL35RNNdSQYFDwEhFDoVBEJCYEsgYpJ8eEDfuUQd+GGWlptRtLNMepfMULmD1TH+U+lnJd8E5ejw4dPGvXevWCpCRIT4edO71fs/V+KWRJqCh4ibQ0BRwREQmhQPaZ8hfKLrjA+/uioqZtiOxPe/aznBF0ZD+rGcQUCoJz4gDs32/WcLndsHu3Z58yl8vcZoWvYIZMEX8UvEREYVBEJIYEUrGxQoZ9ul1xsacyFFxu/sJ4TudLfiSV0SyhqgXbDGRkmOucHDh40Hzds6eZbpiQYC5gmoeoTbyEkoKXSDhEUtCJpLGIxKgff/yRG2+8kaOOOoq2bdty5pln8sknn9Tc73a7mTZtGj169KBt27YMHjyYb775Jowjllhnrd3avt1T8amsrLvC1bMZ+xpnMZfreY0KnIxiKbvo3vSTNZLDAZ9+6mkDbzXW2LPHtMV3uWDyZFMhdDjUJl5CS8FLJFwUeERahT179nDBBRfgdDp55513+N///V8ee+wxunTpUnPMww8/zJNPPsmCBQsoLi4mOTmZoUOHcvjw4TCOXGKF1UQjI8NT0bFvjGyFEX8bDTud5vidO5v23BexhjlMBuBeHmcDA5p2oiZyu73bwFtVLn+NM9KP/G9ZUw4lVBS8RMIp3OEr3M8v0grMmTOHXr168cILL/DrX/+a3r17M2TIEE466STAVLsef/xxpk6dyjXXXEPfvn158cUX2bFjB2+88UZ4By9Ry9+Gv0VF3hUdp9P/Y+0NJ1wuKCjwbicfqFR+5FV+SwJVvMhNzOeOxp+kmXr2rL3eraoKZs70hEqLfcNkkVDQPl4i4RaujZUVukRaxJtvvsnQoUO57rrr+OCDDzj22GO58847ue222wDYsmULJSUlDB48uOYxnTp1Ij09nfXr1zN69Oha5ywvL6e8vLzm+9LSUgBcLheuJnxCth7TlMeKEWnv4YIFUF1tru+7D555Bvr2hX//GyZNgnnzPOubHA5z3b27Z4+uuDjzeEtCIz8xOt0VLKsYRffqXfzbcSaZSU/R1lHZ4OPatnV5XTdG+/amchUXB/HxJiyWlkLbtuY1zp/vvaYL4LHH4KOPYMUKz/s0aVLTgmakiLTfxWjU2Pcw0OMUvEQiQUuGLwUukRb1/fffM3/+fLKysvjTn/7Ev/71L+655x4SExMZO3YsJSUlAHTv7r3upXv37jX3+Zo9ezbTp0+vdfu7775Lu3btmjzWlStXNvmxYkTKe/jnPzfu+2A787nnOPHtDVQkJ7Pz0Tt5vseaRj3++eeD/z7W95rffhvOPttzzNtvB/3pW1yk/C5Gs0Dfw4NW15YGKHiJRIqWCF8KXSItrrq6mnPPPZdZs2YBkJaWxv/8z/+wYMECxo4d26RzZmdnk2WbO1VaWkqvXr0YMmQIHTt2bPT5XC4XK1eu5LLLLsNZ1/wzqVdLvYepqWa6YHIy7NjhfV9+vqnYlJebRhnWMfn5ZnqhwwH33guPPOL9uPbt4fBhU+GyV7ma6obKl7nGZZLLaNdiVmRdHvBj27Z18fzzK7n11ss4dCiw93HAAFPJ69sX1q83tyUne+9RZpeQYNZ7PfaYeb3nnw/vvFP/extN9N9z8zX2PbRmHTREwUskkljBKNgBLNICV264ByDScnr06MHpp5/uddtpp53GsmXLAEhJSQFg586d9OjRo+aYnTt3ctZZZ/k9Z1JSEklJSbVudzqdzfqg1dzHS+jfw4kTTYi6447aa7Qee8wEB6cTEhOhTx/o0sXTvc865vBh70Yahw4Fb3z92MzT3AnAdKbxesU1TTrPoUPOOoOX0+k9FXDVKhMqzz/fvNaiIhM+U1M9Uyd9H2+9V8nJ8N575vb63ttopP+emy/Q9zDQ91nNNUQi0cXpwQlLwTqPiDTZBRdcwNdff+112//93/9x/PHHA9C7d29SUlJ4//33a+4vLS2luLiYAQNatgOcRL669ujKyTFhw+mEKVPMMZs2mXDhdpsqj9NpOvbFx4dmbJ3ZwzJG0pbDvMMwZjAtJM/jbzmN221C04YN5vvqatMy3u2GqVNNwBo40FxPmeJ/k+lA9j8TaQ4FLxGI3AqMFZwaE56a8hgRCZnMzEw2bNjArFmz+Pbbb3nllVd47rnnmDRpEgAOh4N7772X/Px83nzzTT7//HNuvvlmUlNT+c1vfhPewUvUmDPHTC8EExxyckz1x+Ewl/79Pft0VVaaEJacbI6bOrX2+Rq7b5eDal7iJk7ie7ZwAmNYTDWhSXh1FRcOHPC8BwBdu5pplGAC1bp1JnDNneu5ze3WpsnSchS8RKKFPVDVdxGRiHLeeefx+uuv89e//pVf/epX5OXl8fjjjzNmzJiaYx544AHuvvtuJkyYwHnnnUdZWRkrVqygTZs2YRy5RCJ7m3g73724CgtNZcjtNtdFRd7TC62qWF3VnZISUyEK1FTyuZJ/cIg2jGA5e+ga+IMbKdBmfdu21d4Q2Wqtb93m+71IKCl4iYiIhNiVV17J559/zuHDh/nyyy9rWslbHA4HM2bMoKSkhMOHD/Pee+9xyimnhGm0EsnqCgpTppgKVna2+d6aSldXeJo927Oh8syZte+vqvJM22vIMN4h98jUkYksYDPh24E4Ls4zpdC6Tkvz3jjaPsXQ35RDkVBRcw0RERGRKJGZaUKXb1DIyzMX+/fgHdAcDmjXzjMlr6io7udxu72n7dXlBLawmDHE4WY+E3mRpnXq9Me3iYbF2uDZXsGztG1rphTatW/vCatlZd7vk8XfuUSCTRUvERERkSjRmAYQc+bUbqlurfsKhjYcYhkj6coeivk19/J4cE58hL/QlZMDDz7oCUrWWjWLv8pVfVUtTTWUlqTgJSIiIhIBrPVb1hTA5jZ8sFdxnE7T2dBa99V8bp7hTs5mEz9zNKNYSgW1tzgINrfbBEqLvUNhTo53s4yMDBMy16zxNNXwfU/T0ryvRUJJwUtEWlZuuAcgIhKZrOpLUVHzqzD+mm/UN3XQXxWsvsrYBJ5jHAupIo7RLGE7vZo20EbKz/dUwhISPG3kMzNNFdBewbKmUhYVeW4vKPAOtZs2eV+LhJKCl4glN9wDEBGR1szeEKOxDR98q2X5+d5Bq6qq/kqXv/vcbv/h69cU8xR3A5DNbFZxaeADDaL+/T3TKfPzazfPsMbucHhudzi8Q62aa0hLUnMNERERkQjg2yCjMezVMn+sENXYaYa+xx/DLpYyikRcLOdaHuH+pg04CIqKvIOhb/MMqxqWleV5Hd27w/btnqmFzXnPRRpLFS8RERGRKGcFCd+Nj3vZZgC63Y3fGNkunkqWMJpebOcrTuUWFgJB6tTRRA6HmXLocJjGIfb1cfZGJFYw3b7dPE5TCyUcFLxEpOXkhnsAIiKxyQoSe/Z4V4HGjoWpUz3fW8GjKWbyIJewmjKSGcFy9tOx6SfzkZxsguH9fgpo9tfjO/XR7YakJE/jkLrWx/mbxlnXZtQioaLgJSIiIhLl7GuVHnzQc3thoan8TJ3q3XYdvKthDRnBMibzMAC38jxfcnqTxllXw44DB0znxccfr31ffLy5D8w+ZPZNod1u81i3u/71cVb1a9262lUwtZKXlqLgJWKXG+4BiIiINJ59Wl1eniecpKWZis6cOVBe7v2YbdsCO/epfMVCbgHgUe7jNa5v8jjrW2NWWel/767KSk+wysryP00wO7t2sGqIGmtIS1PwEhEREYkCgUyNs4758EPz/YYNpqLjcpkAE9fIT37t2c9yRtCBMtZwEVMoaPoLaCKn03QwBFi92qzlSkjwVLes/bucTkhMDHzqYGM2oxYJBgUvEWkZueEegIhIdAtkapzVXt2qLDkcnsYbDof3NMSGufkL4zmdL/mRVH7Lq1SFoSF2YqKpclldG10us65r0KAjozzSvdCqmNk3WBaJJApeIr5ywz0AERGR2uxT4+qqftkDV3IyTJnimZoXHw9z5wb+fFnM5XpeowIno1jKLroH54XUI+FIrktO9qxBS0szr93qXuh0mvfAHkStcAmNb5kv0lIUvERERESigH1qnO/GwZYpU0xomTrVc6wVSiorzWPs6movfxFrmMNkADIpZAMDQvCKPOyBCmDHDti923y9aROsWeNZ65WYaF6XPYja1305HN5t5UUihYKXiIRebrgHICISXRpaz2Wv6tinHublmUAye7ZnvZO13ssff+3lj2U7r/JbEqjiJW7kGe5s2osIQFycCYnt2kF6Osyb57nPHqzsG0PbNz+2wqV1bENt5UXCScFLxJ/ccA9ARERas4bWc02ZYkKGvUpkf6y13qmwsHFT75xU8BrX0Z1dfEZfbudZQrlJcnW1qdpZ67esilx+vnewsreQ99fV0DrWqvjV1VZeJJwUvEQktHLDPQARkejT0HquvDwTMhITvYNVRoYnvFihrK69s/yZSxYD2MAeOjOC5RyiXXBeEJ69uOpi31fsmWe871u3zrMXWX1hyt9+XSKRQsFLREREJMLYqz1W9ct3PZe1zmvWLE8ws0/JmzzZPP6CCwJ7zht5ibuYd+Trl/mek4L4irz36LJXsCzbtsGxx5qvDx0yr8ceOtX+XaKdgpdIXXLDPYAYkNvyTzn8wuUt/6QiIiGSk+O98bF96qFV6aqu9kxLtAeamTNNtauuNV4OB3ToYL7ux2aeYwIA05nG21wRxFdR26ZNnrHaK10//miuq6vN6wmkhb5ItFDwEhEREYlQBQVmvZbVHj4tzVMBmjLF+9isLO8peVYwq2uNl9sN+/dDZ/awjJG05TDvMIzpPBTaFwV06WLC19SpsHWrZ8z2aZFZWd5TLkWinYKXSH1ywz2AKJYb7gGIiEQ/K4i43SaEWBsJFxaaqXdWYMnJMce0b2+OLyvzrKlyOutuG++gmpe4iZP4ni2cwBgW4w7Bx8M4n1Nu3+5dybKmEf7xj+b7Bx4wUwo1vVBiiYKXRA1NIRMRkdZm8mTP17NmmbASF+epANmDSUGBub+gwAQxa02Vy+WZwufrQWZyJf/gEG0YwXL20LXZY7amL9rFxXm6DfpuhGw3deqRcT3Y7GGIRBwFL5GG5IZ7AFEoN9wDEBGJDg3t12WvalVXm9uqq83tvXp5P9aqjjkcJnzZ+ZtuOJQVNdMK72A+m0kLwisy0xd99e/vqdhZ43G7Ye5c782O8/PN/VYjEW2CLLFEwUtEREQkTAJpHmFVtXxZ0/XmzDHVo8pKc7vL5fnaYm9gAfCr5C28wu+Iw80CbmcRtzTrdTRkwwbPfl3WejWHw3v/rsJCTxv5Z56p+71RIJNopeAlEojccA8giuSGewAiItGjvuYRvgGjrn2wrKDlW9Wy1n5NnWpatVvacIhFB0bSlT0U82v+wBMBjTXQ/cD8jdMeBKdMMUFy8uTamx3feac5ZtKkut8bdTqUaKXgJRKo3HAPIArkhnsAIiLRxV/ziIwME3KsCpE17c43qCQk1H1eh8MElxkzTFt5DzfPcCdns4mfOZpRLKWCpAbH2ZjqUvfudY8tJ8fzWn03O3a7PRWveWY7Mb+NNdTpUKKVgpeIxAw1YBGRWGDfBNlidTG0gkpmprm9riqY220CW2KidyVsAs8xjoVUEcdveZXt9PJ/Aj/nqy/o2W3fDklJtTdJtocuf6xKFtRf0VKnQ4lWCl4ijZEb7gFEsNxwD0BEJDZYgcXh8J6GZ1dYWHsdlz9WZ0OAX1PMU9wNQDazWc0lNfc1NI0wPx/S0wMZvWGt3bJrKChZlSxQRUtik4KXSGPlhnsAESg33AOQSHfCCSfgcDhqXSZNmgTAoEGDat03ceLEMI9aJDysTZDbtYNBg0x1Z/VqE46sToZduphjAwlfAEfzM0sZRSIuljGCR7jf6/66Nlm2szoSNkVGhufrQJpj7NjRvIqWGnBIJFLwkqgSMVPJcsM9gAiSG+4BtKyJPBvuIUSlf/3rX/z00081l5UrVwJw3XXX1Rxz2223eR3z8MMPh2u4ImHn20DCqh5ZnQy3bzffJyTUPQXQqmLFU8kSRtOL7XzFqYzjBSDAThk2aWm1N0L2JyPDsx8XmOrV2rWe7+tqjmGfathcasAhkUjBS6SpcsM9gAiQG+4BSLQ45phjSElJqbm89dZbnHTSSVx00UU1x7Rr187rmI4dO4ZxxCLh5dtAwgo8vlMCp0wx0wl911OBp4qVz1QuZRVlJDOC5eyn8f9txcWZ8GftJeZPcrJ5TitkWWNOs20PlpMD5eX+N0+2TzVsLjXgkEik4CUiTZMb7gFIJCgtLfW6lJeXN/iYiooKXn75ZW699VYctk+Rixcv5uijj+ZXv/oV2dnZHDx4MJRDF4lovg0k/vQnEySmTvVUuBwOswFxTk7d0wCvZTlTmAPArTzPl5zudX8gFSx/xyUkmOe1B6W0NM/0vsJCT0j78EPPMdbatMTE2lMJ8/LMFMNgUAMOiUQB9qcREb9yaZ0BJDfcA6gtYqahRqD3PrwakoNcPTpQCkAvn11ZH3roIXJzc+t96BtvvMHevXu55ZZbam773e9+x/HHH09qair//ve/mTx5Ml9//TXLl+vnKpHNChn33Qdnn93882RmmtDgKy/PXHJyTOBKSPBsQDxzpv81WqfyFQuPbIz8GFm8xvW1jglkbVevXnD88d7NMioroaDAtI63NkW27i8o8K7M2Z8jM9O8TlWipDVSxUuCptWufckN9wBaWG64ByCRZNu2bezbt6/mkp2d3eBj/vKXvzB8+HBSU1NrbpswYQJDhw7lzDPPZMyYMbz44ou8/vrrfPfdd6EcvkizWWuJrP2nGlJX0wf7mqT6GkMUFpqphUlJnn29/IWnZMpYxkg6sp81XMTkI1UvX3UFL3tw2rbNf4t7l8uz1sx+HofD3Gedw/73GVWipDVT8JKoE5GVjdxwD6CF5IZ7ABJpOnbs6HVJSqp/I9b//Oc/vPfee/z+97+v97j0I32rv/3226CNVSQUrLVERxp0Nshf0wffdU8FBeaYggLP/e3bm6YV5eVm2t/BgzBnjnl+33VfAy9w8zy3cgb/yw56MJolVNUzyclfK3m3u+EW89ZjExLMmBwO8xrS0821FcZ++qnh84i0BgpeIsGSS2wHk9xwD0BiwQsvvEC3bt244oor6j1u8+bNAPTo0aMFRiXSdFYF58EHAzveCmq+66Hs656swGNdW2GtqMgcV11tQo3LZcLXscd6P8d5HxZyPa9RgZNRLGUnKfWOybdaVZcOHfzfnpRk1qC1a2fGtGGD9/5hgQQ4kdZAwUsk2HLDPYAQyA33AOoXkVVQqaW6upoXXniBsWPHkmDrf/3dd9+Rl5fHxo0b+eGHH3jzzTe5+eabufDCC+nbt28YRywSfFZQ27TJU/ny7cBnbVRsXVv39+xpvnc6PeerrPRM9wO4iDU8zAMATEksZHDO+V7P7y8EWYGqVy8T6qwOib7TEPfvr/3YhITar8Nf50URUfASCY3ccA8gSHKJndciYffee++xdetWbr31Vq/bExMTee+99xgyZAh9+vThvvvuY+TIkfz9738P00hFmi41NbBNe+1hKy/PfO/boXDTJvN9QQFUVEBJibndXk2Kj7c9Nz/yKr8lgSr+ecyNzD18JzNmeFeq4uO9Q9zAgfCHP5jbxo41t23YUHu8CQnm2ORkz5qtjAzPOrO0NE/4mjzZhEOr86HWc4kYCl4SlaKiwpFLdIeW3HAPQGLNkCFDcLvdnHLKKV639+rViw8++IBffvmFw4cP88033/Dwww9rHy+JSoFu2mtvMpGTA/n5/itg1jREl8tUo+zt2x0O6N/ffO2kgte4ju7s4jP6cuOBZ2tKT/a9t6qqTIjbudN8X1zsee5Zs8xDKitrj7e62kx17NIFdu82be3XrvVfwcvLM8/hcjUcuuprJCISaxS8REItN9wDaKRcomrMURHCRSSm5eSYShfUvWmvv4Bh3WY10QBPBcwKZZmZnvvi4sz3VjXpggs83Qbnch/ns569dGIEy9l9uB0ZGeb8Xbp4zmGtDXO5vBtgQP2bI1v3bd/uP1zWtWFxQ8HKX7MRkVil4CXSEnKJ/DCTS+SPUUQkAlnhAcwGwP6qPP4ChnWbw2FCizUtzx5W8vJMdSk5GbKzTTMNl8tUpawpgWN4mbt4GoAbeZnvOammQnXggPcaMLvERLP+KjnZe90Y1P7eLiHBE7AyMsz4Z80y4cv3tTcUrOoKbCKxSMFLolZUVjpyibxwk0vkjUlEJIpY4SGQY+wBw7ptyhTvva18w4q9AmavUFVWQl8+4zkmADCDHP7BlV7PG1fPJz37+jLfRhqTJ9f92KQkc7zT6am4VVfXbpFvr7YdPOi/6qV9vaQ1UfASCYdcwh94wv38QdDS4bvVbhIuIvXKyzOVroaOsTfQsPMNPfaQlpNjKlNOp6ku2acDdmYPyxlBOw7xDsOYzkO1ntff9EFrz62CAnP+mTO913VlZJgQVdfUQ/vaM/s57aHSCo/2DZY1nVBaOwUvkXDLpeVCUEs+l4iIeLHCSEGBZ22Xv2l49ipQYaFnamFRkScMOajmRW7mJL5nCycwhsU8mBPvNUXQ4fDfPj4uzrPWKz/fO/gNHAiffuq9LswuIcGMKy3Nc66BA6FtW+/zWOFx4EDzGGtzaJHWTMFLolpUTjesTy7BDUa5KGyJiEQI+z5X9rVddTXjcDrNFL24OBNerBbwDgc83HEmV/EWh0liJMvYQ1cKCqB7d8853G5zsR5vqaryfi57OKtrXZg17dDqomi1vG/b1rujocUKj+vWmYBXURGa6YTqiijRJKHhQ0QCN5FnWcDt4R5GbMgN9wAiW8yFbhGJeXl55pKTY0JKVlbdYcQ+la9dOxNi2rc3318et4KsUjOt8K74+WyqOhswAcdfIw2326zL6t/fhCSr1bvl2GM9j3M4zPOlpZljrevyclNtswJXZqbnNVjTCO0B0nqNmZnmNYeKfT1cKJ9HJBhU8RIRERFpAVZ1BkyQWr3aBJ2MDO/7c3JMYPGdopeZCae33cKLVb8jDjd/SbidHtnj6u1ACCYYHThgqlmZmZCe7rkvIcE7rE2d6qlU2a+t7ofWWOzTIf01yGipNvHqiijRJKaC1wknnIDD4fC6FNg3xwD+/e9/k5GRQZs2bejVqxcPP/xwmEYrwaLKR+ujn7mIRCN7GMnI8HQEtK7nzDH3z5ljwow1Rc/tNoEswXWIL/qMpCt7+CTuPLbf/wRgmm9YUwn9renyHYPVhh5Mi3r7Y+zNP3zb2jem+2BLBSJ1RZRoElPBC2DGjBn89NNPNZe777675r7S0lKGDBnC8ccfz8aNG3nkkUfIzc3lueeeC+OIRUREpDWwhxErbIEJYTk5nul/9m6BOTmmAcaBA25OePhO2LSJX+KO5trqZeTOTjpyn3lMQoJnzy9LQoJpcGGFq65dPed3Ok1gefBBz15e9iqVPSg2di2VApFIbTEXvDp06EBKSkrNJdn2r8/ixYupqKjg+eef54wzzmD06NHcc889zJ07N4wjFpFooFbyItIUdVWNBg4092dkwNq13lPyEhI8j7Mm7kzgOca6F1JFHNdXL2E7vWo9V1KSObcV8Kwwt26dWbcFsG2b5/gpU7wfn55uwld5uWe6oxUUW2rqoEgsi7ngVVBQwFFHHUVaWhqPPPIIlbY/G61fv54LL7yQxMTEmtuGDh3K119/zZ49e+o8Z3l5OaWlpV4XiSyaetZ66GctItHECiz5+SZkWSFs3TozhXDtWnOcfU1Xejo1lSyHAy5qU8y8ODODJ9c5i//reanXcwwcWPf6K0tmpve4EhLMGjN7W/tNm8y0xcpKT7MKa+8xq7281UZeRBovpoLXPffcw5IlS1i9ejW33347s2bN4oEHHqi5v6SkhO72PqtQ831JSUmd5509ezadOnWqufTqVfuvTCIiIiJ2OTmmemSxWrX7Vo2sDoBTppg1XcXFnvuGnv0zL5ePIqHaxQdHXUu+6wGvx/bs6WmAUd+0vrw877Vc1r5gBw549gZLS6u9Nst3I2Srq6GINF7EB68pU6bUapjhe/nqq68AyMrKYtCgQfTt25eJEyfy2GOP8dRTT1Fu/1evCbKzs9m3b1/NZZu9Ti8RQ5WQ2KefsYhEE6slfEKCZzNhfw0n7FWxnBzPRsQJVHJv8Wh6urfzNadw1S8LAYdXF8Lt2+tfe2Wf6hgfX/v+5GTPHl2bNnlXuezTDesau4gELuL38brvvvu45ZZb6j3mxBNP9Ht7eno6lZWV/PDDD5x66qmkpKSwc+dOr2Os71NSUuo8f1JSEklJSY0beCumvbxERET873WVmeldmfKtilmVr8JCeOjQVC6tXkUZyYx0vM4BR0eoNpUrK5xZmzHXtY+VPdT5djx0Ok2lzL6vmP0xhYXmfu2PJRIcEV/xOuaYY+jTp0+9F/uaLbvNmzcTFxdHt27dABgwYABr167FZds1cOXKlZx66ql0sSYvS1RTRSR2tYaf7dWfvRvuIYhIM6WmeqpP9rVW/ppTWB0L7VWxrKwjj3txOfdXzwHgVp7nfzmd6mpzjNWFMCcHLrjAnMu+9sp3PzCLFdacTvN4q7mG75ow7Y0lEhoRH7wCtX79eh5//HE+++wzvv/+exYvXkxmZiY33nhjTaj63e9+R2JiIuPHj+eLL77g1Vdf5YknniBL/7KISD3U0VBEAlVX5z/fMGOFLkv//ib8uN1wdruvOHzDLQA8mZDFa1xPfLxPMDsSlKw1V/a1V/aQl5dnWszbWRsoW0HMl1rBi4RGzASvpKQklixZwkUXXcQZZ5zBzJkzyczM9Nqjq1OnTrz77rts2bKFc845h/vuu49p06YxYcKEMI5cgq01VEZaG/1MRSRa1FUpsocZ39AFnuD01OwyXj40gjYV+9ly3EX88sAckpPNRsf+wpC/6pTvbVb4sqpkmza1fGv4xu4DJhKLIn6NV6DOPvtsNti3Yq9D3759WbduXQuMSESCQaFLRKLJjh1mKl997IHH4TB7bGVlAW43f66+ldP5kh9JZeCPr7LviQSvdWHWeixrCqG/dWN5ebXXZVnfz51rpiVu2tSyUwl9q3AirVHMVLwkMC21hiTcU7P0YV1ERCJVZqYJZwkJphJVU8kqLGSU+zUqcHI9r/FTdXcOHIBZs7w3VLYCjG83xIZYx2/a1PJTCbVuTETBS0QimAK0iMQK+1S7vDyYPBmSkmzrrD74AI7sPZpJIf9ynl9zX3W1JzRZjTjS0syeXxYrfNU3pc8eflp66p/WjYkoeEkM04f26BYpP79wV29FJLr5q1SBT5fDH3+E66+Hqio2/+pGFrW706vxRUaGd3dCh8NsfuxyeU9rtFfB/K3faqjLooiEloKXxLRI+fAuIiKtQ36+dyXJCjgOh6k2paWZ+9PSzPf3/6ECrrsOdu2Cvn05q/hZyg44mDLF0wxj7VoTmgYONOesrPQ8n8sFPXuaKlhFhee8DU3p09Q/kZan4CUiEUeBWWJZQUEBDoeDe++9t+a2w4cPM2nSJI466ijat2/PyJEj2blzZ/gGKU32zDPelSQr4EyZYqpNVkdBa53VQ6X3wfr10KkTLF9Ozux2tG9vHus7Nc/qfGi1lrds326mLbpcga/f8p36p66DIqGn4CUhEylTtPQhPrro5yWx7F//+hfPPvssffv29bo9MzOTv//977z22mt88MEH7NixgxEjRoRplNIcFRWm+mRv5Z6ZaboJ5uR4NjpOSwNefhmefhqAUYdfJmfhSbWmAPpuhmxvLW9VwKypiM2pYGnqoUjoKXhJq6AP89IUkfLHA4kNZWVljBkzhv/+7/+mS5cuNbfv27ePv/zlL8ydO5dLLrmEc845hxdeeIGPPvoooG1SpGUEWhFyuUz1yV5xsncfLC42t1Vs/Dcc2Ud0tnMay8qvrGkNbw9Qvm3Y7VWqdetMcw5rKmJzmldo6qFI6MXMPl4iDRl+4XLeWau/IEey1hyQW2qrBwmfSZMmccUVVzB48GDybbvnbty4EZfLxeDBg2tu69OnD8cddxzr16+nf//+tc5VXl5OeXl5zfelpaUAuFwuXC5Xo8dmPaYpj20tFiww3QWffNJ8feedphW8xXrvjjrKxfjxJoBZ7rsPHnnEfJ2QAMd12MM77mthzyGqhw6l/Lxsjl7gYtIkePBBmDbNOqd57DPPwKRJ3ucMtmnTvJ83XPS72Hx6D5uvse9hoMcpeEWaOcDkcA9CpOW15tAlsW/JkiV8+umn/Otf/6p1X0lJCYmJiXTu3Nnr9u7du1NSUuL3fLNnz2b69Om1bn/33Xdp165dk8e5cuXKJj821v35z7Vve/vt2rc9/fTKWvedfTb89a9HvqmuJn3WLLp+8j0HunXjgxtvJK3DP2vO73vOs8+mzvtimX4Xm0/vYfMF+h4ePHgwoOMUvCSkJvIsC7g93MOooapXZIrE0KVphhIs27Zt4w9/+AMrV66kTZs2QTlndnY2WbY5YaWlpfTq1YshQ4bQsWPHRp/P5XKxcuVKLrvsMpz2/uRSS36+pwL14IOe26338K67LuOXX5wkJ8OOHbUfHzdrFvGffIK7TRsS//53Ljuy6Cs11UwprOtxjR2fb0WuofsihX4Xm0/vYfM19j20Zh00RMFLWh2Fr8gSiaFLJJg2btzIrl27OPvss2tuq6qqYu3atTz99NP885//pKKigr1793pVvXbu3ElKSorfcyYlJZGUlFTrdqfT2awPWs19fGswfbq51OXWW5089piTO+7w3mMLgBUrah7smD8f569/XXPXxIlmHdcdd3j22crMNGu3GuOxx0yAe+yx2uO032dtytyU52gJ+l1sPr2HzRfoexjo+6zmGq2Q1pLow76ItJxLL72Uzz//nM2bN9dczj33XMaMGVPztdPp5P333695zNdff83WrVsZMGBAGEcuDbE33LAt2/Pf5OKHH+B3vzPdMG6/HW65xevuQDY3DqTBR31NMuz3qYuhSMtT8JKQi9QpWwpf4RepP4NI/Z2V6NShQwd+9atfeV2Sk5M56qij+NWvfkWnTp0YP348WVlZrF69mo0bNzJu3DgGDBjgt7GGRA57eHnmGXObdW3JyYGjkw/x44CRsGcPnHcePPFEveetKzwFEpbq625ov09dDEVanoKXtGqR+sG/NdB7L+JRWFjIlVdeyciRI7nwwgtJSUlh+XL9NxLp7OHlzjvNbZMmeR9TONfNwwcncWzJp3D00bBsGTn5SfVWruoKT8EMS81tPy8ijac1XiLS4hS6pLVbs2aN1/dt2rRh3rx5zJs3LzwDkibJy/Osj3K5TNdBe8MNgMWD/ptr3n6BKuIYvnsJa07qVXO8tTdXU55PRKKPKl7S6ikEtKxIf7/DMc1Q6y5FYtTHH3PNe3cDkOucxcrqS3G5zDKv+ipXgW7WLCLRRcFLWkSkr5mJ9DAQK/Q+i0ir8fPPMGoUVFTAtdfCAw+QkGA6HWZn1z/NT40vRGKTgpfIEcMvXK5gEEJ6b0Wk1aishNGjYds2OOUUWLiQvHwHLpfJYQ2tq2qJxheqqom0PAUvaTGRXvWyKCAEVzQF2mj5HRWRCDd1KqxaZdLT669DIze1bonGF6qqibQ8Ba9WSmtK6hctQSHS6X0UkdbG8frrMGeO+eb55+H008M7oDqonbxIy1PwEqmDQkPzRNv7p2qXiDRX+x9/pPKm3wPwYXoWXH99mEdUN7WTF2l5Cl7SoqLtw220hYdIofctcKo+i8SIsjLOKyigTcV+PuBCrvi8INwjEpEIo+Al0oBoWqMUbnqvRKRVcruJnzCBjtu2UdohlXHt/sY99zm9DlEzCxFR8BIJkAJF/aL5/Ym2SqyItIyAw1JhIXFLl1IdH0+7t/7K9we615rCp2YWIqLg1YqFa4pTNH/IVUWnNr0nIhKrAgpLH3wADzwAwP/ceit57w/wG9bUzEJEFLwi0ZxwD0AaoqBhxML7EM1/CBCR0GowLP34o2mgUVVF9Q03sOXyy3nmGf9hrTnNLDRNUSQ2KHhJWMTCh93WXOlpza89mNRYQySy1RuWKirguutg1y7o25eq+fPB4eDOO4Nf2dI0RZHYoOAl0kytKYTE2muNhT8ARIvc3FwcDofXpU+fPjX3Hz58mEmTJnHUUUfRvn17Ro4cyc6dO8M4YpEG3HcfrF8PnTrBsmXQrh1g9k4Odpt2TVMUiQ0KXhI2sfahN9ZCiV0svzZpOWeccQY//fRTzaWoqKjmvszMTP7+97/z2muv8cEHH7Bjxw5GjBgRxtGK1OPll+Hppz1fn3xySJ9Oe26JxIaEcA9Awuvqz97lzX5Dwj2MmGIFlHfWRveHxlgPWrEW/KNBQkICKSkptW7ft28ff/nLX3jllVe45JJLAHjhhRc47bTT2LBhA/3792/poYrU7d//hgkTzNc5OXDlleEdj4hEDVW8JKxi+cOvVSWKtgATjWOORq1xfdc333xDamoqJ554ImPGjGHr1q0AbNy4EZfLxeDBg2uO7dOnD8cddxzr168P13BFatu7F0aMgEOHYOhQeOihcI9IRKKIKl4iLcAeZCKxEtbaglYsB/6WVlpa6vV9UlISSUlJtY5LT09n4cKFnHrqqfz0009Mnz6djIwM/ud//oeSkhISExPp3Lmz12O6d+9OSUlJKIcvErjqarjpJvjuOzjhBFi8GOLjwz0qEYkiCl4SdhN5lgXcHu5htBjfkBOOINbaglarN5vg/2tfaa569erldfNDDz1Ebm5urcOHDx9e83Xfvn1JT0/n+OOP529/+xtt27YN8uBEQmDWLHjrLUhKMs00jjoq3CMSkSij4CUSZv5CUDDDmEKWN1W7gmvbtm107Nix5nt/1S5/OnfuzCmnnMK3337LZZddRkVFBXv37vWqeu3cudPvmjCRFrdiBUybZr5+5hk4++ygnTonx7SJz8w0TTREJHYpeElENNhobVWvhigshUakhK5YWt/VsWNHr+AVqLKyMr777jtuuukmzjnnHJxOJ++//z4jR44E4Ouvv2br1q0MGDAg2EMWaZwtW+B3vwO32zTVuPXWoJ7evkeXgpdIbFNzjUg1J9wDEBEJnj/+8Y988MEH/PDDD3z00Udce+21xMfHc8MNN9CpUyfGjx9PVlYWq1evZuPGjYwbN44BAwaoo6G0uJwcaN/eXHPoEIwaBXv2wHnnwZNPBv35tEeXSOuhipdEDFW9JJQipdrVWm3fvp0bbriBX375hWOOOYaBAweyYcMGjjnmGAAKCwuJi4tj5MiRlJeXM3ToUJ555pkwj1pao5oK1Fw3eT9Ogk8/haOPhqVLzfquIMvLU6VLpLVQ8JKIovAlsS6Wphk2xpIlS+q9v02bNsybN4958+a10IhE/MvMNOFr8aD/hhdegLg4WLIEjjsu3EMTkSinqYYCtN4Pg9I6qNolIoHKy4OyVR9zzcq7zQ0zZ8Kll4Z3UCISExS8JOLoQ7IEk36fRKRRfv4ZRo6Eigq49lqYPLnOQ73Wg4mINEDBS0SkhaiyLBLhKith9GjYvh1OOQUWLgSHo87D7R0JRUQaouAlNSLpQ6GqFBIM+j0SkUaZOhVWrTJtBpcvhwa2SlBHQhFpDAWvSNbKW8rrQ7M0h35/RKRRXn8d5hz5H+/zz8MZZzT4kLw8KCuDGTNCPDYRiQkKXhLR9OFZYkUkVZRFxMdXX8HYsebrrCy4/vrwjkdEYpKCl3jRh0OJBQrsIhKwsjIYMQL274cLL4SCgnCPSERilIKXRDx9iJbG0O+LiATM7Ybx4+HLLyE1FV59FZzOcI9KRGKUgpdEBX2YlkBE6u+JKskiEaqwEP72N6riErh092vkzEsJ94hEJIYpeEktkfohMVI/VEtk0O+HiDTKBx/AAw8A8EBCIasOn6+28CISUgpeIiIi0rr8+KNpoFFVBTfeSLv7J6ktvIiEnIJXpGvlLeV9qaoh/kTy70WkVpBFWq2KChO6du2CM8+EZ58lL9+htvAiEnIKXuJXJH9YjOQP2dLy9PsgIo3yxz/CRx9Bp05mk+R27cI9IhFpJRS8JCrpw7ZA5P8eRPIfMERapcWL4amnzNcvvwwnnxze8YhIq6LgJVEr0j90S2jp5y8ijfLvf8Ntt5mvc3LgyivDOx4RaXUUvKRO0fDXen34bp30cxeRRtm7F0aOhEOHYOhQeOihcI9IRFohBa9ooAYb9dKH8NYlWn7e0fCHC5FWoboabr4Zvv0Wjj/eTDeMjw/3qESkFVLwknpFy4fHaPkwLs2jn7OINNqsWfD3v0NSkmmmcdRR4R6RiLRSCl4SM/ShPLZF0883Wv5gIRLz/vlPmDbNfD1/Ppx9dnjHIyKtmoKXxJRo+nAugdPPVUQa7Ycf4He/A7cbJkyAcePCPSIRaeUUvKJFGNd5Rdtf7/UhPbZE288z2v57EYlJhw6ZZhq7d8N558GTT4Z7RCIiCl7NsWFpuEcgdYm2D+vin36OItJobjdMmgSffgpHHw1Ll5r1XSIiYabgJQGJxr/i60N7dIvGn180/nciEnP++7/hhRcgLg6WLIHjjgv3iEREAAUviXHR+OG9tZvIs/q5iUjTfPwx3H23+XrWLLj00vCOR0TERsFLAhatf83Xh/joEc0/q2j970MkZvz8M4waBRUVcO218MAD4R6RiIgXBa9ooo2Um0xVlMinn4+INFllJdxwA2zbBqecAgsXgsMR7lGJiHhR8JJGifa/6uvDfeSJhVAc7f9diES9nBx4/31ITjabJHfsGO4RiYjUouAlrU60f8iPJfpZiEizvf46FBSYr59/Hs44I7zjERGpQ9QEr5kzZ3L++efTrl07Onfu7PeYrVu3csUVV9CuXTu6devG/fffT2Vlpdcxa9as4eyzzyYpKYmTTz6ZhQsXNmtcH/61WQ9vvAiYbhgLf92PhSpLtIuV9z8W/nsQiVpffw1jx5qvs7Lg+uvDOx4RkXpETfCqqKjguuuu44477vB7f1VVFVdccQUVFRV89NFHLFq0iIULFzJt2rSaY7Zs2cIVV1zBxRdfzObNm7n33nv5/e9/zz//+c+WehkSYWLlw380UegVkaAoK4MRI2D/frjwQk/VS0QkQiWEewCBmj59OkCdFap3332X//3f/+W9996je/funHXWWeTl5TF58mRyc3NJTExkwYIF9O7dm8ceewyA0047jaKiIgoLCxk6dGhLvZSYcPVn7/JmvyHhHkZQWCFgAbeHeSSxLRbDlqpdImHidsPvfw//+7/Qowe8+io4neEelYhIvaKm4tWQ9evXc+aZZ9K9e/ea24YOHUppaSlffPFFzTGDBw/2etzQoUNZv359vecuLy+ntLTU6yKxR5WY0InF91WhSySMHn/chK2EBHjtNUhJCfeIREQaFDPBq6SkxCt0ATXfl5SU1HtMaWkphw4dqvPcs2fPplOnTjWXXr16BXn0jRQB67wgdj94xmJICBeFWREJurVr4f77zdeFhXDBBeEdj4hIgMIavKZMmYLD4aj38tVXX4VziABkZ2ezb9++msu2bdvCPSQJMQWG5on19y9W/+ggEvF27DANNKqqYMwYmDQp3CMSEQlYWNd43Xfffdxyyy31HnPiiScGdK6UlBQ+/vhjr9t27txZc591bd1mP6Zjx460bdu2znMnJSWRlJQU0Dham1ha6+WP1n81TiyHLREJs4oKuO462LkTzjwTnn1WmySLSFQJa/A65phjOOaYY4JyrgEDBjBz5kx27dpFt27dAFi5ciUdO3bk9NNPrznm7bff9nrcypUrGTBgQLOe+8O/wgU3NOsUjTcHmNzCz1mHWA9foADWkNYUuFTtEgmTP/4RPvoIOnUymyQnJ4d7RCIijRI1XQ23bt3K7t272bp1K1VVVWzevBmAk08+mfbt2zNkyBBOP/10brrpJh5++GFKSkqYOnUqkyZNqqlWTZw4kaeffpoHHniAW2+9lVWrVvG3v/2Nf/zjH2F8ZRJNFMC8tabABQpdImGzeDE89ZT5+qWX4OSTwzseEZEmiJrgNW3aNBYtWlTzfVpaGgCrV69m0KBBxMfH89Zbb3HHHXcwYMAAkpOTGTt2LDNmzKh5TO/evfnHP/5BZmYmTzzxBP9/e/ceFlW1vwH85TqIOKCCDqKipuL9nkSevCSC5vFkapGat0TToFLUklOJ1ClMS80TZp1M+pWiYXrKSyYhYCrqiSCvUaKmJWCpXFTurN8fHOY4gjDAzKzZM+/neeZxLnvGdy/WzF7fWXv2btu2LT766CMeSt4ArGHW6053FhzWVoRZW7FFRJKdOAHMmVN5/eWXgXHj5OYhImogxRReMTEx9zyHVxVvb+9quxLebfjw4UhLSzNgMonMaHdDwPqKryrWUISx2OJsF5EUubmVJ0kuLAQCAoD/ntOTiEiJLOZw8kTmoOpofpZQqFjSujQWiy5qjKioKNx///1o1qwZWrVqhfHjxyMjI0NnmaKiIoSEhKBly5ZwcXHBxIkTqx0MyupUVADTpwOZmYC3N7BlC2BnJzsVEVGDsfAykMOxshOYBw5Q/+fOwkUpxYvS8hIpQXJyMkJCQnD06FHEx8ejtLQUAQEBuHXrlnaZhQsXYteuXYiLi0NycjKuXLmCCRMmSExtBt58E9i1C1CpgC++AFq2lJ2IiKhRFLOrId2Dme1uCFjvLod1ubuYkb1bIosr/fDLBGqsffv26dyOiYlBq1atkJqaiqFDhyIvLw8bN27Eli1b8PDDDwMANm3ahO7du+Po0aN44IEHZMSW65tvgGXLKq+vXw8MHCg3DxGRAbDwIpLkXoWPoQsyFlgNx6KLjCEvLw8A0KJFCwBAamoqSktL4e/vr12mW7duaN++PVJSUmosvIqLi1FcXKy9nZ+fDwAoLS1FaWlpvTNVPachzzW4ixdhP2UKbIRAxezZKJ82DTCHXHUwqzZUMLZj47ENG6++bajvciy8yCg469VwLJSILFdFRQUWLFiAIUOGoFevXgCA7OxsODo6ws3NTWfZ1q1bIzs7u8bXiYqKQmQNB5rYv38/nJ2dG5wvPj6+wc81BNviYjwUHg6369dxo0sXHBo9GhV1HDTL3MhuQ0vBdmw8tmHj6duGt2/f1ms5Fl6WwAx3NwRYfJGycbaLjCEkJASnTp3CoUOHGvU64eHhCAsL097Oz89Hu3btEBAQALVaXe/XKy0tRXx8PEaNGgUHB4dGZWswIWA3dy5sz5+HcHeHy9dfY3T79nKyNIBZtKEFYDs2Htuw8erbhlV7HdSFhZcBHY4FhkyWnYKIGotFFxlDaGgodu/ejYMHD6Jt27ba+zUaDUpKSpCbm6sz65WTkwONRlPja6lUKqhUqmr3Ozg4NGqg1djnN8qHHwKffALY2sJm61Y43HefnByNJLUNLQjbsfHYho2nbxvq2848qiEZFQewpDTss2RoQgiEhoZi586dOHDgADp27Kjz+MCBA+Hg4ICEhATtfRkZGbh06RL8/PxMHVeO48eB556rvP7GG8DIkXLzEBEZAQsvS/GW7AD3xoEsKQX7qvHocy6r4cOHw8bGRucyb948SYkNJyQkBJ999hm2bNmCZs2aITs7G9nZ2SgsLAQAuLq6Yvbs2QgLC0NiYiJSU1Mxa9Ys+Pn5WccRDf/4A5g0CSgpAcaPB14yw33niYgMgIUXmQQHtETWTZ9zWQHAnDlzkJWVpb2sXLlSUmLDef/995GXl4fhw4fD09NTe9m2bZt2mTVr1uCvf/0rJk6ciKFDh0Kj0WDHjh0SU5tIWRnw5JPA5ctA165ATAxgYyM7FRGRUfA3XgYm9XdeZnqQjSo82AaZM345YFx1ncuqirOz8z1/16RUQog6l3FyckJ0dDSio6NNkMiMvPoqcOAA0LQpsGMH4OoqOxERkdFwxouIrB6LLtO7+1xWVTZv3gx3d3f06tUL4eHheh+ilxRo505gxYrK6xs3Aj17ys1DRGRknPEik+KsF5kbFl2Nc/chdO91tL071XQuKwCYMmUKvL290aZNG5w4cQIvvfQSMjIyrGOXO2uTkQHMmFF5feFCIChIbh4iIhNg4WVpzHx3Q4DFF5kPqym6vvseQFMDv2jlb7PatWunc29ERASWL19e6zPvdS6ruXPnaq/37t0bnp6eGDlyJDIzM3GfQg8tTjW4eROYMAEoKACGDgXeMuOjQxERGRB3NTSCw7GyE5g/qxnwktliHzSMy5cvIy8vT3sJDw+vdfmqc1klJibqnMuqJr6+vgCAc+fOGSwvSSYEMHs2cOYM4OkJbNsG8DxDRGQlWHhZIoV8eciBL8nCvmc4arVa53Kv3QzrOpdVTdLT0wEAnp6ehoxMMq1dC3z+OWBvD2zfDljYgVSIiGrDwouk4gCYTI19To66zmWVmZmJ119/Hampqbh48SK++uorTJ8+HUOHDkWfPn0kpyeDSE4GliypvL5mDfDgg3LzEBGZGAsvS6WQWS+AA2EyHfY1eeo6l5WjoyO+/fZbBAQEoFu3bli0aBEmTpyIXbt2SU5OBnHlSuUBNMrLgalTgZAQ2YmIiEyOB9cwEqnn81IgHnCDjI1Fl1x1ncuqXbt2SE5ONlEaMqmSEuDxx4GcHKB3b+DDD3mSZCKySpzxsmQKmvUCODAm42HfIpJo8WLgyJHKkyPv2AE4O8tOREQkBQsvMiscIJOhKbJPrZUdgMhANm8G/vnPyuuffgp07iw3DxGRRCy8LJ3CZr0AhQ6UySyxLxFJdOIEMGdO5fVXXgHGjZObh4hIMhZeRsTzeTUcB8zUWIrtQwr8soSomtxcYOJEoLAQCAwE6jipNhGRNWDhZQ0UOpBT7MCZpGPfIZKoogKYPh04dw7w9q7c3dDOTnYqIiLpWHiRWeMAmupL0X1GoV+SEOmIigJ27QJUKuCLL4CWLWUnIiIyCyy8jMxsdjdU8IBO0QNpMin2FSLJvvkGePXVyuvr1wMDB8rNQ0RkRlh4kSJwQE21+duP+5XfRxT85QgRAODiRWDKFEAIYO5c4OmnZSciIjIrLLysicIHdhYxuCaDY58gMgNFRZUH07h+Hbj/fmDdOtmJiIjMDgsvEzCb3Q0tBAfaVMVi+oLCvxQhKycEEBIC/PAD4O4ObN9e+fsuIiLSwcLL2ljIAM9iBtzUYBbTByzkPUlW7KOPgI8/BmxtgdhYoH172YmIiMySvewAJMFbAF6SHaLxqgbeX/UNkJyETMliCi4iS3D8OBAaWnn9jTcAf3+5eYiIzBhnvEyEuxsaDwfi1sPi/tac7SIl++MPYNIkoKQEeOwx4CUL+EaPiMiIWHhZKwsb8FncgJx0WOSBVSzsPUhWprwcmDwZuHwZ6NoViIkBbGxkpyIiMmssvMhiWOTgnPg3JTJHr7wCJCQATZsCO3YAarXsREREZo+FlwmZ3e6GFvqNOwfqlsGiC2kLfe+Rldi5E1ixovL6xo1Az55y8xARKQQLL2tnoQNAix60WwH+7YjMVEYGMGNG5fWFC4GgILl5iIgUhEc1JIvGIx8qi1UUXBb6ZQdZgZs3gQkTgIICYOhQ4C12ZiKi+uCMl4mZ3e6GgFUMBK1iQK9gVjNDaQXvNbJQQgDBwcCZM4CnJ7BtG+DgIDsVEZGicMaLKlnIub1qw9kv82MVxVYVFl2kZGvXVhZb9vZAXByg0chORESkOCy8JDgcCwyZLDuF9WIBJp9VFVxESnfwILBkSeX1d94BhgyRm4eISKFYeNH/WMGs151YgJme1RZcnO0ipbpyBXjiicrzdk2ZAjz3nOxERESKxcKLdFlZ8QWwADMFqy24ABZdpFwlJcDjjwM5OUDv3sCHH/IkyUREjcDCSxLubmh+7iwOWIQZhlUXXERKt2QJcOQI4OpaeZLkpk1lJyIiUjQWXlSdFc563Y2zYA3HYusOnO0ipdqyBVi3rvL6//0f0Lmz3DxERBaAhZdEZj3rxeILAGfB6oMF111YdJFSnThReeh4AHj5ZeBvf5Obh4jIQrDwontj8aWDRVh1LLbugUUXKVVuLjBxIlBYCAQEAJGRshMREVkMFl5UOxZfNbLmIozFVh1YdJFSVVTAbtYs4Nw5wNu7cndDOzvZqYiILAYLL8nMendD0svdhYilFWIstIisQ9cvvoDtnj2ASgV88QXQsqXsSEREFoWFF9WNs171ovRCjIVWI3C2ixTKJj4e3bZsqbyxfj0wcKDcQEREFoiFlxlQxKwXi68Gq6mQMYdijAWWgbHoIqUSArbLl8NGCFTMng3bp5+WnYiIyCKx8CL9sfgymNqKHkMWZSyuTIRFFymZjQ3Kd+9G5jPPoMOaNbCVnYeIyEKx8DITipj1Alh8mQCLJYVh0UWWoHlznJk5Ex2cnGQnISKyWPxii4iooVh0ERERkZ5YeJmRw7GyE+iJg00ivg+IiIioXlh4UcNw0ElEREREpDcWXmZGMbNeAIsvsl7s+0RERFRPLLyocTgAJWvDPk9EREQNwMLLDClq1gvgQJSsB/s6ERERNRALLzIMDkjJ0rGPExERUSOw8DJTipv1AjgwJcvFvk1ERESNxMKLDIsDVLI07NNERERkACy8zJgiZ70ADlTJcrAvExERkYGw8CLj4ICVlI59mIiIiAyIhZeZU+ysF8CBKynTW2DfJSIiIoNj4aUALL6ITIT9lYiIiIyEhRcZHwezpATsp0RERGREiim83njjDTz44INwdnaGm5tbjcvY2NhUu2zdulVnmaSkJAwYMAAqlQqdO3dGTEyM8cMbgKJnvQAOasm8sX+aTHR0NDp06AAnJyf4+vri+PHjsiMRERGZhGIKr5KSEjz++OOYP39+rctt2rQJWVlZ2sv48eO1j124cAFjx47FiBEjkJ6ejgULFiA4OBjffPONkdMTAP52hswT+6TJbNu2DWFhYYiIiMAPP/yAvn37IjAwEFevXpUdjYiIyOgUU3hFRkZi4cKF6N27d63Lubm5QaPRaC9OTk7axzZs2ICOHTvinXfeQffu3REaGopJkyZhzZo1xo5vEIqf9arCgS6ZC/ZFk1q9ejXmzJmDWbNmoUePHtiwYQOcnZ3x8ccfy45GRERkdPayAxhaSEgIgoOD0alTJ8ybNw+zZs2CjY0NACAlJQX+/v46ywcGBmLBggW1vmZxcTGKi4u1t/Py8gAAtwwbXS/7Y4EHJkn4jw3tHwAWyA5BVmut7AC1yy+t/FcIYaBXNManVeVr5ufn69yrUqmgUqmqLV1SUoLU1FSEh4dr77O1tYW/vz9SUlKMkM+6VPWVu/8e+iotLcXt27eRn58PBwcHQ0azGmxDw2A7Nh7bsPHq24ZVn711bbctqvB67bXX8PDDD8PZ2Rn79+/Hs88+i5s3b+L5558HAGRnZ6N169Y6z2ndujXy8/NRWFiIJk2a1Pi6UVFRiIyMrHb/BMOvgn62y/qPDcxS1oPISK5duwZXV9cGP9/R0REajQbZ2X8zYKr/cXFxQbt27XTui4iIwPLly6st++eff6K8vLzGz+CffvrJKPmsSUFBAQBU+3sQEZHpFBQU1Lrdllp4LV26FG+9Vfu+PmfPnkW3bt30er1XX31Ve71///64desWVq1apS28Gio8PBxhYWHa27m5ufD29salS5caNSiSIT8/H+3atcPly5ehVqtlx6kXZpeD2U0vLy8P7du3R4sWLRr1Ok5OTrhw4QJKSkoMlEyXEEK7R0GVmma7yPjatGmDy5cvo1mzZtX+JvpQ6nvFnLANDYPt2Hhsw8arbxsKIVBQUIA2bdrUupzUwmvRokWYOXNmrct06tSpwa/v6+uL119/HcXFxVCpVNBoNMjJydFZJicnB2q1+p6zXcC9d51xdXVVbIdWq9XMLgGzy6HU7La2jf8ZrpOTk85vXWVxd3eHnZ1djZ/BGo1GUirLYWtri7Zt2zb6dZT6XjEnbEPDYDs2Htuw8erThvpMxkgtvDw8PODh4WG0109PT0fz5s21RZOfnx/27t2rs0x8fDz8/PyMloGIiCp3exw4cCASEhK0R5utqKhAQkICQkND5YYjIiIyAcX8xuvSpUu4fv06Ll26hPLycqSnpwMAOnfuDBcXF+zatQs5OTl44IEH4OTkhPj4eLz55ptYvHix9jXmzZuH9957Dy+++CKefvppHDhwAJ9//jn27Nkjaa2IiKxHWFgYZsyYgUGDBmHw4MFYu3Ytbt26hVmzZsmORkREZHSKKbyWLVuGTz75RHu7f//+AIDExEQMHz4cDg4OiI6OxsKFCyGEQOfOnbWHLq7SsWNH7NmzBwsXLsS7776Ltm3b4qOPPkJgYGC9sqhUKkRERCjytwzMLgezy6HU7ErNXZegoCD88ccfWLZsGbKzs9GvXz/s27ev2gE3yPQstc+ZEtvQMNiOjcc2bDxjtaGNMNzxiomIiIiIiKgGijmBMhERERERkVKx8CIiIiIiIjIyFl5ERERERERGxsKLiIiIiIjIyFh41eKNN97Agw8+CGdnZ7i5udW4zKVLlzB27Fg4OzujVatWWLJkCcrKynSWSUpKwoABA6BSqdC5c2fExMQYP3wNOnToABsbG53LihUrdJY5ceIEHnroITg5OaFdu3ZYuXKllKx3i46ORocOHeDk5ARfX18cP35cdqRqli9fXq19u3Xrpn28qKgIISEhaNmyJVxcXDBx4sRqJ5M1lYMHD2LcuHFo06YNbGxs8O9//1vncSEEli1bBk9PTzRp0gT+/v745ZdfdJa5fv06pk6dCrVaDTc3N8yePRs3b96Unn3mzJnV/g6jR4+Wnj0qKgr3338/mjVrhlatWmH8+PHIyMjQWUafPqLPZw7R3ep639xtx44dGDVqFDw8PKBWq+Hn54dvvvnGNGHNVH3b8E6HDx+Gvb09+vXrZ7R8StCQNiwuLsbLL78Mb29vqFQqdOjQAR9//LHxw5qphrTh5s2b0bdvXzg7O8PT0xNPP/00rl27ZvywZkqf7XFN4uLi0K1bNzg5OaF3797Vzg2sDxZetSgpKcHjjz+O+fPn1/h4eXk5xo4di5KSEhw5cgSffPIJYmJisGzZMu0yFy5cwNixYzFixAikp6djwYIFCA4OlrYBe+2115CVlaW9PPfcc9rH8vPzERAQAG9vb6SmpmLVqlVYvnw5PvzwQylZq2zbtg1hYWGIiIjADz/8gL59+yIwMBBXr16VmqsmPXv21GnfQ4cOaR9buHAhdu3ahbi4OCQnJ+PKlSuYMGGClJy3bt1C3759ER0dXePjK1euxLp167BhwwYcO3YMTZs2RWBgIIqKirTLTJ06FadPn0Z8fDx2796NgwcPYu7cudKzA8Do0aN1/g6xsbE6j8vInpycjJCQEBw9ehTx8fEoLS1FQEAAbt26pV2mrj6iz2cOUU30ed/c6eDBgxg1ahT27t2L1NRUjBgxAuPGjUNaWpqRk5qv+rZhldzcXEyfPh0jR440UjLlaEgbPvHEE0hISMDGjRuRkZGB2NhY+Pj4GDGleatvGx4+fBjTp0/H7Nmzcfr0acTFxeH48eM6p1uyNvpsj+925MgRTJ48GbNnz0ZaWhrGjx+P8ePH49SpU/X7zwXVadOmTcLV1bXa/Xv37hW2trYiOztbe9/7778v1Gq1KC4uFkII8eKLL4qePXvqPC8oKEgEBgYaNXNNvL29xZo1a+75+Pr160Xz5s212YUQ4qWXXhI+Pj4mSHdvgwcPFiEhIdrb5eXlok2bNiIqKkpiquoiIiJE3759a3wsNzdXODg4iLi4OO19Z8+eFQBESkqKiRLWDIDYuXOn9nZFRYXQaDRi1apV2vtyc3OFSqUSsbGxQgghzpw5IwCI//znP9plvv76a2FjYyN+//13admFEGLGjBni0UcfvedzzCX71atXBQCRnJwshNCvj+jzmUNUl5reN/ro0aOHiIyMNHwgBapPGwYFBYlXXnml1m2ENdKnDb/++mvh6uoqrl27ZppQCqNPG65atUp06tRJ575169YJLy8vIyZTlru3xzV54oknxNixY3Xu8/X1Fc8880y9/i/OeDVCSkoKevfurXPyz8DAQOTn5+P06dPaZfz9/XWeFxgYiJSUFJNmrbJixQq0bNkS/fv3x6pVq3R2UUpJScHQoUPh6OiovS8wMBAZGRm4ceOGjLgoKSlBamqqThva2trC399fWhvW5pdffkGbNm3QqVMnTJ06FZcuXQIApKamorS0VGc9unXrhvbt25vdely4cAHZ2dk6WV1dXeHr66vNmpKSAjc3NwwaNEi7jL+/P2xtbXHs2DGTZ75bUlISWrVqBR8fH8yfP19nlwpzyZ6XlwcAaNGiBQD9+og+nzlExlBRUYGCggJtfyX9bNq0CefPn0dERITsKIr01VdfYdCgQVi5ciW8vLzQtWtXLF68GIWFhbKjKYafnx8uX76MvXv3QgiBnJwcbN++HY888ojsaGbj7u1xTQw1nrevfzyqkp2drTMAAqC9nZ2dXesy+fn5KCwsRJMmTUwTFsDzzz+PAQMGoEWLFjhy5AjCw8ORlZWF1atXa7N27NixWtaqx5o3b26yrFX+/PNPlJeX19iGP/30k8nz1MbX1xcxMTHw8fFBVlYWIiMj8dBDD+HUqVPIzs6Go6Njtd8Ktm7dWttXzEVVnpra/M5+3apVK53H7e3t0aJFC+nrM3r0aEyYMAEdO3ZEZmYm/v73v2PMmDFISUmBnZ2dWWSvqKjAggULMGTIEPTq1QsA9Ooj+nzmEBnD22+/jZs3b+KJJ56QHUUxfvnlFyxduhTfffcd7O053GqI8+fP49ChQ3BycsLOnTvx559/4tlnn8W1a9ewadMm2fEUYciQIdi8eTOCgoJQVFSEsrIyjBs3rt67zFqqmrbHNbnX9re+216r+yRYunQp3nrrrVqXOXv2rM5BEcxZfdYnLCxMe1+fPn3g6OiIZ555BlFRUVCpVMaOavHGjBmjvd6nTx/4+vrC29sbn3/+uUkLbGv35JNPaq/37t0bffr0wX333YekpCSz+Y1FSEgITp06pfMbQCJztWXLFkRGRuLLL7+s9qUF1ay8vBxTpkxBZGQkunbtKjuOYlVUVMDGxgabN2+Gq6srAGD16tWYNGkS1q9fz22rHs6cOYMXXngBy5YtQ2BgILKysrBkyRLMmzcPGzdulB1POlNvj62u8Fq0aBFmzpxZ6zKdOnXS67U0Gk21o+tVHYFMo9Fo/737qGQ5OTlQq9UG+cBozPr4+vqirKwMFy9ehI+Pzz2zAv9bH1Nzd3eHnZ1djblkZdKXm5sbunbtinPnzmHUqFEoKSlBbm6uzoyGOa5HVZ6cnBx4enpq78/JydEekUuj0VQ7uElZWRmuX79uduvTqVMnuLu749y5cxg5cqT07KGhodoDerRt21Z7v0ajqbOP6POZQ2RIW7duRXBwMOLi4qrtZkP3VlBQgO+//x5paWkIDQ0FUFlECCFgb2+P/fv34+GHH5ac0vx5enrCy8tLW3QBQPfu3SGEwG+//YYuXbpITKcMUVFRGDJkCJYsWQKg8ovhpk2b4qGHHsI//vEPne28tbnX9rgm9xoj13fba3W/8fLw8EC3bt1qvdz5G6fa+Pn54eTJkzqDuPj4eKjVavTo0UO7TEJCgs7z4uPj4efnJ3190tPTYWtrq/0G08/PDwcPHkRpaalOVh8fHym7GQKAo6MjBg4cqNOGFRUVSEhIMFgbGsvNmzeRmZkJT09PDBw4EA4ODjrrkZGRgUuXLpndenTs2BEajUYna35+Po4dO6bN6ufnh9zcXKSmpmqXOXDgACoqKuDr62vyzLX57bffcO3aNe3GRVZ2IQRCQ0Oxc+dOHDhwoNpuvfr0EX0+c4gMJTY2FrNmzUJsbCzGjh0rO46iqNVqnDx5Eunp6drLvHnz4OPjg/T0dLP7nDRXQ4YMwZUrV3RO9/Hzzz/D1ta2zoEyVbp9+zZsbXWH+3Z2dgAqt0vWqK7tcU0MNp6v54E/rMqvv/4q0tLSRGRkpHBxcRFpaWkiLS1NFBQUCCGEKCsrE7169RIBAQEiPT1d7Nu3T3h4eIjw8HDta5w/f144OzuLJUuWiLNnz4ro6GhhZ2cn9u3bZ9J1OXLkiFizZo1IT08XmZmZ4rPPPhMeHh5i+vTp2mVyc3NF69atxbRp08SpU6fE1q1bhbOzs/jggw9MmvVuW7duFSqVSsTExIgzZ86IuXPnCjc3N50ju5mDRYsWiaSkJHHhwgVx+PBh4e/vL9zd3cXVq1eFEELMmzdPtG/fXhw4cEB8//33ws/PT/j5+UnJWlBQoO3PAMTq1atFWlqa+PXXX4UQQqxYsUK4ubmJL7/8Upw4cUI8+uijomPHjqKwsFD7GqNHjxb9+/cXx44dE4cOHRJdunQRkydPlpq9oKBALF68WKSkpIgLFy6Ib7/9VgwYMEB06dJFFBUVSc0+f/584erqKpKSkkRWVpb2cvv2be0ydfURfT5ziGpS13t+6dKlYtq0adrlN2/eLOzt7UV0dLROf83NzZW1CtLVtw3vxqMa1r8NCwoKRNu2bcWkSZPE6dOnRXJysujSpYsIDg6WtQrS1bcNN23aJOzt7cX69etFZmamOHTokBg0aJAYPHiwrFWQTp/t8bRp08TSpUu1tw8fPizs7e3F22+/Lc6ePSsiIiKEg4ODOHnyZL3+bxZetZgxY4YAUO2SmJioXebixYtizJgxokmTJsLd3V0sWrRIlJaW6rxOYmKi6Nevn3B0dBSdOnUSmzZtMu2KCCFSU1OFr6+vcHV1FU5OTqJ79+7izTff1BmMCiHEjz/+KP7yl78IlUolvLy8xIoVK0yetSb//Oc/Rfv27YWjo6MYPHiwOHr0qOxI1QQFBQlPT0/h6OgovLy8RFBQkDh37pz28cLCQvHss8+K5s2bC2dnZ/HYY4+JrKwsKVkTExNr7NszZswQQlQeUv7VV18VrVu3FiqVSowcOVJkZGTovMa1a9fE5MmThYuLi1Cr1WLWrFnaLyVkZb99+7YICAgQHh4ewsHBQXh7e4s5c+ZUK9JlZK8pMwCdzwN9+og+nzlEd6vrPT9jxgwxbNgw7fLDhg2rdXlrVN82vBsLr4a14dmzZ4W/v79o0qSJaNu2rQgLC9MZIFubhrThunXrRI8ePUSTJk2Ep6enmDp1qvjtt99MH95M6LM9HjZsWLXPu88//1x07dpVODo6ip49e4o9e/bU+/+2+W8AIiIiIiIiMhKr+40XERERERGRqbHwIiIiIiIiMjIWXkREREREREbGwouIiIiIiMjIWHgREREREREZGQsvIiIiIiIiI2PhRUREREREZGQsvIiIiIiIiIyMhRcREREREZGRsfAiMpAHHngA69at095+8sknYWNjg6KiIgDA5cuX4ejoiJ9//llWRCIiIiKShIUXkYG4ubmhoKAAQGWRtX//fjRt2hS5ubkAgA8++ACjRo1C165dJaYkIiIiIhlYeBEZyJ2F13vvvYennnoK7u7uuHHjBkpKSvCvf/0LL7zwAgBg9+7d8PHxQZcuXfDRRx/JjE1ERCTFH3/8AY1GgzfffFN735EjR+Do6IiEhASJyYiMw152ACJLUVV43bp1Cxs3bsTRo0eRnJyMGzduYPv27WjZsiVGjRqFsrIyhIWFITExEa6urhg4cCAee+wxtGzZUvYqEBERmYyHhwc+/vhjjB8/HgEBAfDx8cG0adMQGhqKkSNHyo5HZHCc8SIykKrC65NPPsGDDz6Izp07Q61W48aNG4iOjsbzzz8PGxsbHD9+HD179oSXlxdcXFwwZswY7N+/X3Z8IiIik3vkkUcwZ84cTJ06FfPmzUPTpk0RFRUlOxaRUbDwIjIQNzc35OXl4d1339XuUujq6orExEScPXsW06dPBwBcuXIFXl5e2ud5eXnh999/l5KZiIhItrfffhtlZWWIi4vD5s2boVKpZEciMgoWXkQG4ubmhgMHDkClUml3kVCr1diwYQOCg4Ph7OwsOSEREZH5yczMxJUrV1BRUYGLFy/KjkNkNPyNF5GBuLm54ebNm9rZLqByxquoqAghISHa+9q0aaMzw/X7779j8ODBJs1KRERkDkpKSvDUU08hKCgIPj4+CA4OxsmTJ9GqVSvZ0YgMzkYIIWSHILImZWVl6N69O5KSkrQH1zhy5AgPrkFERFZnyZIl2L59O3788Ue4uLhg2LBhcHV1xe7du2VHIzI47mpIZGL29vZ45513MGLECPTr1w+LFi1i0UVERFYnKSkJa9euxaeffgq1Wg1bW1t8+umn+O677/D+++/LjkdkcJzxIiIiIiIiMjLOeBERERERERkZCy8iIiIiIiIjY+FFRERERERkZCy8iIiIiIiIjIyFFxERERERkZGx8CIiIiIiIjIyFl5ERERERERGxsKLiIiIiIjIyFh4ERERERERGRkLLyIiIiIiIiNj4UVERERERGRkLLyIiIiIiIiM7P8BIQNuX47gD/YAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 1000x600 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from grid_search import generate_w, get_best_parameters, grid_search\n",
    "from plots import grid_visualization\n",
    "\n",
    "# Generate the grid of parameters to be swept\n",
    "grid_w0, grid_w1 = generate_w(num_intervals=100)\n",
    "\n",
    "# Start the grid search\n",
    "start_time = datetime.datetime.now()\n",
    "grid_losses = grid_search(y, tx, grid_w0, grid_w1)\n",
    "\n",
    "# Select the best combinaison\n",
    "loss_star, w0_star, w1_star = get_best_parameters(grid_w0, grid_w1, grid_losses)\n",
    "end_time = datetime.datetime.now()\n",
    "execution_time = (end_time - start_time).total_seconds()\n",
    "\n",
    "# Print the results\n",
    "print(\n",
    "    \"Grid Search: loss*={l}, w0*={w0}, w1*={w1}, execution time={t:.3f} seconds\".format(\n",
    "        l=loss_star, w0=w0_star, w1=w1_star, t=execution_time\n",
    "    )\n",
    ")\n",
    "\n",
    "# Plot the results\n",
    "fig = grid_visualization(grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight)\n",
    "fig.set_size_inches(10.0, 6.0)\n",
    "fig.savefig(\"grid_plot\")  # Optional saving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, please fill in the functions `compute_gradient` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(y, tx, w):\n",
    "    \"\"\"Computes the gradient at w.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        w: numpy array of shape=(2, ). The vector of model parameters.\n",
    "\n",
    "    Returns:\n",
    "        An numpy array of shape (2, ) (same shape as w), containing the gradient of the loss at w.\n",
    "    \"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO: compute gradient vector\n",
    "    # ***************************************************\n",
    "    e = y - np.dot(tx, w)\n",
    "    gradient = -np.dot(tx.T, e) / len(y)\n",
    "    return gradient\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[26.706078    6.52028757]\n",
      "[-23.293922    -3.47971243]\n"
     ]
    }
   ],
   "source": [
    "# test 1\n",
    "w = np.array([100, 20])\n",
    "print(compute_gradient(y, tx, w))\n",
    "\n",
    "# test 2\n",
    "w = np.array([50, 10])\n",
    "print(compute_gradient(y, tx, w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please fill in the functions `gradient_descent` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"The Gradient Descent (GD) algorithm.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        initial_w: numpy array of shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        max_iters: a scalar denoting the total number of iterations of GD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "\n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of GD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of GD\n",
    "    \"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO: compute gradient and loss\n",
    "        # ***************************************************\n",
    "        loss = compute_loss(y, tx, w)\n",
    "        gradient = compute_gradient(y, tx, w)\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO: update w by gradient\n",
    "        # ***************************************************\n",
    "        w = w - gamma * gradient\n",
    "\n",
    "        # store w and loss\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        print(\n",
    "            \"GD iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}\".format(\n",
    "                bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your gradient descent function through gradient descent demo shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD iter. 0/49: loss=1210.7945490694617, w0=-892.6706077997898, w1=901.3479712434981\n",
      "GD iter. 1/49: loss=1089.7320335556349, w0=-796.0741548196002, w1=812.561145362647\n",
      "GD iter. 2/49: loss=980.7757695931908, w0=-709.1373471374294, w1=732.6530020698813\n",
      "GD iter. 3/49: loss=882.7154432536324, w0=-630.8942202234758, w1=660.7356731063917\n",
      "GD iter. 4/49: loss=794.461467797371, w0=-560.4754060009178, w1=596.0100770392514\n",
      "GD iter. 5/49: loss=715.0330363875212, w0=-497.09847320061544, w1=537.757040578825\n",
      "GD iter. 6/49: loss=643.5474481186561, w0=-440.05923368034337, w1=485.3293077644414\n",
      "GD iter. 7/49: loss=579.2105864349213, w0=-388.7239181120987, w1=438.1443482314959\n",
      "GD iter. 8/49: loss=521.3077048385679, w0=-342.52213410067833, w1=395.67788465184526\n",
      "GD iter. 9/49: loss=469.1954759759039, w0=-300.94052849039974, w1=357.45806743015953\n",
      "GD iter. 10/49: loss=422.29475065419535, w0=-263.51708344114905, w1=323.0602319306423\n",
      "GD iter. 11/49: loss=380.08427728103163, w0=-229.83598289682348, w1=292.10217998107703\n",
      "GD iter. 12/49: loss=342.09508433708214, w0=-199.5229924069306, w1=264.2399332264683\n",
      "GD iter. 13/49: loss=307.90513663530567, w0=-172.2413009660271, w1=239.16391114732033\n",
      "GD iter. 14/49: loss=277.13474851410825, w0=-147.68777866921388, w1=216.59549127608716\n",
      "GD iter. 15/49: loss=249.44153779995094, w0=-125.58960860208197, w1=196.28391339197734\n",
      "GD iter. 16/49: loss=224.51840616124952, w0=-105.70125554166322, w1=178.00349329627846\n",
      "GD iter. 17/49: loss=202.08804549301902, w0=-87.80173778728634, w1=161.55111521014948\n",
      "GD iter. 18/49: loss=181.90154181396946, w0=-71.69217180834717, w1=146.74397493263336\n",
      "GD iter. 19/49: loss=163.7352479831256, w0=-57.19356242730192, w1=133.41754868286893\n",
      "GD iter. 20/49: loss=147.3860453479754, w0=-44.14481398436123, w1=121.4237650580809\n",
      "GD iter. 21/49: loss=132.67313764165334, w0=-32.4009403857145, w1=110.62935979577169\n",
      "GD iter. 22/49: loss=119.43243417005549, w0=-21.83145414693252, w1=100.91439505969342\n",
      "GD iter. 23/49: loss=107.51655006917771, w0=-12.318916532028734, w1=92.17092679722298\n",
      "GD iter. 24/49: loss=96.79326071297383, w0=-3.757632678615364, w1=84.30180536099957\n",
      "GD iter. 25/49: loss=87.14324723194028, w0=3.947522789456695, w1=77.2195960683985\n",
      "GD iter. 26/49: loss=78.45994903766099, w0=10.882162710721543, w1=70.84560770505755\n",
      "GD iter. 27/49: loss=70.64639436809446, w0=17.12333863985991, w1=65.10901817805069\n",
      "GD iter. 28/49: loss=63.616168100256715, w0=22.74039697608445, w1=59.94608760374452\n",
      "GD iter. 29/49: loss=57.29080452572065, w0=27.795749478686524, w1=55.29945008686898\n",
      "GD iter. 30/49: loss=51.600074328582195, w0=32.34556673102839, w1=51.117476321680975\n",
      "GD iter. 31/49: loss=46.48094422579983, w0=36.440402258136096, w1=47.35369993301178\n",
      "GD iter. 32/49: loss=41.87624896900554, w0=40.12575423253301, w1=43.9663011832095\n",
      "GD iter. 33/49: loss=37.735150643573725, w0=43.44257100949024, w1=40.91764230838745\n",
      "GD iter. 34/49: loss=34.01204717531731, w0=46.42770610875172, w1=38.173849321047605\n",
      "GD iter. 35/49: loss=30.665193519498505, w0=49.11432769808708, w1=35.70443563244175\n",
      "GD iter. 36/49: loss=27.658581103061813, w0=51.532287128488896, w1=33.48196331269648\n",
      "GD iter. 37/49: loss=24.958730322411856, w0=53.708450615850516, w1=31.481738224925735\n",
      "GD iter. 38/49: loss=22.535690840942976, w0=55.666997754475986, w1=29.68153564593206\n",
      "GD iter. 39/49: loss=20.363250503136015, w0=57.42969017923891, w1=28.061353324837757\n",
      "GD iter. 40/49: loss=18.415846037190626, w0=59.01611336152554, w1=26.603189235852888\n",
      "GD iter. 41/49: loss=16.671444625502104, w0=60.44389422558351, w1=25.2908415557665\n",
      "GD iter. 42/49: loss=15.112436507653081, w0=61.72889700323568, w1=24.109728643688758\n",
      "GD iter. 43/49: loss=13.722440917848406, w0=62.885399503122635, w1=23.046727022818786\n",
      "GD iter. 44/49: loss=12.4824725385144, w0=63.926251753020885, w1=22.090025564035813\n",
      "GD iter. 45/49: loss=11.3790224259329, w0=64.86301877792931, w1=21.228994251131137\n",
      "GD iter. 46/49: loss=10.401804087833996, w0=65.7061091003469, w1=20.454066069516927\n",
      "GD iter. 47/49: loss=9.538970106707557, w0=66.46489039052273, w1=19.756630706064136\n",
      "GD iter. 48/49: loss=8.781262846361436, w0=67.14779355168098, w1=19.128938878956625\n",
      "GD iter. 49/49: loss=8.116820197500319, w0=67.76240639672339, w1=18.564016234559865\n",
      "GD: execution time=0.024 seconds\n"
     ]
    }
   ],
   "source": [
    "# from gradient_descent import *\n",
    "from plots import gradient_descent_visualization\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([-1000, 1000])\n",
    "\n",
    "# Start gradient descent.\n",
    "start_time = datetime.datetime.now()\n",
    "gd_losses, gd_ws = gradient_descent(y, tx, w_initial, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"GD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f28dd7440edc43be99daba355de48cfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=51, min=1), Output()), _dom_classes=('widge…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time Visualization\n",
    "from ipywidgets import IntSlider, interact\n",
    "\n",
    "\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        gd_losses,\n",
    "        gd_ws,\n",
    "        grid_losses,\n",
    "        grid_w0,\n",
    "        grid_w1,\n",
    "        mean_x,\n",
    "        std_x,\n",
    "        height,\n",
    "        weight,\n",
    "        n_iter,\n",
    "    )\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(gd_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 4. Stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_stoch_gradient(y, tx, w):\n",
    "    \"\"\"Compute a stochastic gradient at w from a data sample batch of size B, where B < N, and their corresponding labels.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(B, )\n",
    "        tx: numpy array of shape=(B,2)\n",
    "        w: numpy array of shape=(2, ). The vector of model parameters.\n",
    "\n",
    "    Returns:\n",
    "        A numpy array of shape (2, ) (same shape as w), containing the stochastic gradient of the loss at w.\n",
    "    \"\"\"\n",
    "\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO: implement stochastic gradient computation. It's the same as the usual gradient.\n",
    "    # ***************************************************\n",
    "    e = y - np.dot(tx, w)\n",
    "    gradient = -np.dot(tx.T, e) / len(y)\n",
    "    return gradient\n",
    "\n",
    "\n",
    "def stochastic_gradient_descent(y, tx, initial_w, batch_size, max_iters, gamma):\n",
    "    \"\"\"The Stochastic Gradient Descent algorithm (SGD).\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        initial_w: numpy array of shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        batch_size: a scalar denoting the number of data points in a mini-batch used for computing the stochastic gradient\n",
    "        max_iters: a scalar denoting the total number of iterations of SGD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "\n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of SGD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of SGD\n",
    "    \"\"\"\n",
    "\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "\n",
    "    for n_iter in range(max_iters):\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO: implement stochastic gradient descent.\n",
    "        # ***************************************************\n",
    "        # since we have only one minibatch, we can iterate over it only once\n",
    "        for minibatch_y, minibatch_tx in batch_iter(y, tx, batch_size=batch_size, num_batches=1):\n",
    "            loss = compute_loss(minibatch_y, minibatch_tx, w)\n",
    "            gradient = compute_stoch_gradient(minibatch_y, minibatch_tx, w)\n",
    "            w = w - gamma * gradient\n",
    "\n",
    "        # store w and loss\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        print(\n",
    "            \"SGD iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}\".format(\n",
    "                bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]\n",
    "            )\n",
    "        )\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD iter. 0/49: loss=68.80718111242173, w0=6.880718111242174, w1=-6.200647033855713\n",
      "SGD iter. 1/49: loss=75.69484233030872, w0=14.450202344273045, w1=-2.7378813355788516\n",
      "SGD iter. 2/49: loss=65.59586910688854, w0=21.0097892549619, w1=-3.034184344194595\n",
      "SGD iter. 3/49: loss=36.18270530046449, w0=24.628059785008347, w1=-6.761701504444037\n",
      "SGD iter. 4/49: loss=43.30072472759329, w0=28.958132257767677, w1=-6.640261493672092\n",
      "SGD iter. 5/49: loss=30.720442511137605, w0=32.03017650888144, w1=-9.55835828611417\n",
      "SGD iter. 6/49: loss=24.41238562861168, w0=34.471415071742605, w1=-11.004255896146034\n",
      "SGD iter. 7/49: loss=105.39503093906455, w0=45.01091816564906, w1=18.398891205168653\n",
      "SGD iter. 8/49: loss=31.81960766663321, w0=48.19287893231238, w1=16.536459011835724\n",
      "SGD iter. 9/49: loss=27.506221666292973, w0=50.94350109894168, w1=18.09086373417882\n",
      "SGD iter. 10/49: loss=19.201612261811846, w0=52.86366232512287, w1=17.89718103994015\n",
      "SGD iter. 11/49: loss=26.25640044056815, w0=55.48930236917968, w1=17.69867897596668\n",
      "SGD iter. 12/49: loss=16.16347441090487, w0=57.10564981027017, w1=15.692937191398336\n",
      "SGD iter. 13/49: loss=18.751953096068483, w0=58.98084511987702, w1=16.393699032777775\n",
      "SGD iter. 14/49: loss=16.261163068385528, w0=60.60696142671557, w1=16.005151803855878\n",
      "SGD iter. 15/49: loss=16.440211047350637, w0=62.25098253145063, w1=13.676488448987975\n",
      "SGD iter. 16/49: loss=1.478019208712908, w0=62.39878445232192, w1=13.617476165850112\n",
      "SGD iter. 17/49: loss=4.144825641667623, w0=62.81326701648868, w1=13.556323260081763\n",
      "SGD iter. 18/49: loss=6.2508832783262065, w0=63.4383553443213, w1=13.102323550939007\n",
      "SGD iter. 19/49: loss=13.755356831716625, w0=64.81389102749296, w1=13.168974655041723\n",
      "SGD iter. 20/49: loss=2.223052013454094, w0=64.59158582614755, w1=13.37126749997068\n",
      "SGD iter. 21/49: loss=4.862400311976586, w0=64.1053457949499, w1=12.60510359783058\n",
      "SGD iter. 22/49: loss=10.913102335551365, w0=65.19665602850503, w1=10.914339651524696\n",
      "SGD iter. 23/49: loss=2.1634779783612856, w0=65.41300382634117, w1=10.78772864836722\n",
      "SGD iter. 24/49: loss=6.153140561446264, w0=66.0283178824858, w1=11.221642617955995\n",
      "SGD iter. 25/49: loss=2.1029413437556457, w0=65.81802374811024, w1=11.064754640858464\n",
      "SGD iter. 26/49: loss=0.43838452583103305, w0=65.77418529552713, w1=11.123223359130982\n",
      "SGD iter. 27/49: loss=18.44735012611276, w0=67.61892030813841, w1=9.885600101874893\n",
      "SGD iter. 28/49: loss=1.7638445099456632, w0=67.79530475913297, w1=9.664226920297587\n",
      "SGD iter. 29/49: loss=0.39801783117145106, w0=67.75550297601583, w1=9.7148324880358\n",
      "SGD iter. 30/49: loss=4.782180219641361, w0=68.23372099797997, w1=9.6354805952883\n",
      "SGD iter. 31/49: loss=14.3366232065421, w0=69.66738331863418, w1=10.759766224174761\n",
      "SGD iter. 32/49: loss=4.145817816069496, w0=70.08196510024113, w1=11.584998724422674\n",
      "SGD iter. 33/49: loss=6.373147835220095, w0=70.71927988376315, w1=12.044412688002499\n",
      "SGD iter. 34/49: loss=16.9084921826978, w0=72.41012910203293, w1=12.709939006708048\n",
      "SGD iter. 35/49: loss=0.2061114450677053, w0=72.38951795752615, w1=12.701342760846757\n",
      "SGD iter. 36/49: loss=0.9534976567541236, w0=72.48486772320156, w1=12.8351231265342\n",
      "SGD iter. 37/49: loss=2.398833194780991, w0=72.72475104267966, w1=12.60867644572607\n",
      "SGD iter. 38/49: loss=5.009583035510758, w0=73.22570934623073, w1=13.541328762610993\n",
      "SGD iter. 39/49: loss=1.7290775012855306, w0=73.39861709635929, w1=13.612337908279045\n",
      "SGD iter. 40/49: loss=0.4011771718648731, w0=73.43873481354578, w1=13.656774083950303\n",
      "SGD iter. 41/49: loss=10.058551059701742, w0=74.44458991951595, w1=13.825968310331538\n",
      "SGD iter. 42/49: loss=6.769501642369519, w0=73.76763975527899, w1=13.473879838114506\n",
      "SGD iter. 43/49: loss=9.923095241668499, w0=72.77533023111214, w1=14.45178281708461\n",
      "SGD iter. 44/49: loss=3.9576165396461818, w0=72.37956857714752, w1=14.737316237207974\n",
      "SGD iter. 45/49: loss=4.893474722336322, w0=72.86891604938116, w1=14.244847091074861\n",
      "SGD iter. 46/49: loss=8.769310575876105, w0=73.74584710696877, w1=13.872603219575607\n",
      "SGD iter. 47/49: loss=8.453433797449108, w0=72.90050372722385, w1=14.773677379915513\n",
      "SGD iter. 48/49: loss=9.571372322093694, w0=73.85764095943321, w1=15.658539055093167\n",
      "SGD iter. 49/49: loss=9.068220143495566, w0=72.95081894508365, w1=16.31581036357681\n",
      "SGD: execution time=0.002 seconds\n"
     ]
    }
   ],
   "source": [
    "# from stochastic_gradient_descent import *\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.1\n",
    "batch_size = 1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start SGD.\n",
    "start_time = datetime.datetime.now()\n",
    "sgd_losses, sgd_ws = stochastic_gradient_descent(\n",
    "    y, tx, w_initial, batch_size, max_iters, gamma\n",
    ")\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "509e5791093c403fae7dfc7a14c96735",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=51, min=1), Output()), _dom_classes=('widge…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time Visualization\n",
    "from ipywidgets import IntSlider, interact\n",
    "\n",
    "\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        sgd_losses,\n",
    "        sgd_ws,\n",
    "        grid_losses,\n",
    "        grid_w0,\n",
    "        grid_w1,\n",
    "        mean_x,\n",
    "        std_x,\n",
    "        height,\n",
    "        weight,\n",
    "        n_iter,\n",
    "    )\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(sgd_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Effect of Outliers and MAE Cost Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from helpers import *\n",
    "\n",
    "# ***************************************************\n",
    "# INSERT YOUR CODE HERE\n",
    "# TODO: reload the data by subsampling first, then by subsampling and adding outliers\n",
    "# ***************************************************\n",
    "height, weight, gender = load_data(sub_sample=True, add_outlier=True)\n",
    "\n",
    "x, mean_x, std_x = standardize(height)\n",
    "y, tx = build_model_data(x, weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((205,), (205, 2))"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape, tx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD iter. 0/49: loss=175.0960168733116, w0=-82.49778624761623, w1=90.85764065873265\n",
      "GD iter. 1/49: loss=157.57154546378123, w0=-66.74579387047083, w1=82.62951725159203\n",
      "GD iter. 2/49: loss=141.7995211952039, w0=-52.56900073103998, w1=75.22420618516547\n",
      "GD iter. 3/49: loss=127.60469935348436, w0=-39.809886905552204, w1=68.55942622538156\n",
      "GD iter. 4/49: loss=114.83202442938993, w0=-28.32668446261321, w1=62.56112426157604\n",
      "GD iter. 5/49: loss=103.34882198645093, w0=-17.991802263968108, w1=57.16265249415107\n",
      "GD iter. 6/49: loss=93.01393978780584, w0=-8.690408285187523, w1=52.304027903468594\n",
      "GD iter. 7/49: loss=83.71254580902527, w0=-0.3191537042850019, w1=47.93126577185436\n",
      "GD iter. 8/49: loss=75.34129122812276, w0=7.214975418527276, w1=43.99577985340156\n",
      "GD iter. 9/49: loss=67.8071621053105, w0=13.99569162905832, w1=40.45384252679403\n",
      "GD iter. 10/49: loss=61.02644589477945, w0=20.09833621853626, w1=37.26609893284725\n",
      "GD iter. 11/49: loss=54.92380130530153, w0=25.590716349066412, w1=34.397129698295146\n",
      "GD iter. 12/49: loss=49.43142117477138, w0=30.53385846654355, w1=31.815057387198255\n",
      "GD iter. 13/49: loss=44.488279057294235, w0=34.98268637227297, w1=29.49119230721105\n",
      "GD iter. 14/49: loss=40.03945115156482, w0=38.986631487429456, w1=27.39971373522257\n",
      "GD iter. 15/49: loss=36.03550603640834, w0=42.59018209107029, w1=25.517383020432934\n",
      "GD iter. 16/49: loss=32.43195543276751, w0=45.83337763434704, w1=23.82328537712226\n",
      "GD iter. 17/49: loss=29.188759889490765, w0=48.75225362329611, w1=22.298597498142655\n",
      "GD iter. 18/49: loss=26.269883900541693, w0=51.379242013350286, w1=20.92637840706101\n",
      "GD iter. 19/49: loss=23.64289551048752, w0=53.743531564399035, w1=19.691381225087525\n",
      "GD iter. 20/49: loss=21.278605959438778, w0=55.871392160342914, w1=18.57988376131139\n",
      "GD iter. 21/49: loss=19.150745363494895, w0=57.7864666966924, w1=17.57953604391287\n",
      "GD iter. 22/49: loss=17.235670827145416, w0=59.51003377940694, w1=16.6792230982542\n",
      "GD iter. 23/49: loss=15.515818815731034, w0=61.061244153850026, w1=15.868941447161399\n",
      "GD iter. 24/49: loss=13.998883603592056, w0=62.457333490848804, w1=15.139687961177875\n",
      "GD iter. 25/49: loss=12.645835503745147, w0=63.713813894147705, w1=14.483359823792705\n",
      "GD iter. 26/49: loss=11.477156992143286, w0=64.84464625711672, w1=13.89266450014605\n",
      "GD iter. 27/49: loss=10.485831363624458, w0=65.86239538378884, w1=13.361038708864063\n",
      "GD iter. 28/49: loss=9.709515355503752, w0=66.77836959779374, w1=12.882575496710272\n",
      "GD iter. 29/49: loss=9.216508822506936, w0=67.60274639039815, w1=12.45195860577186\n",
      "GD iter. 30/49: loss=8.88540775025309, w0=68.34468550374211, w1=12.06440340392729\n",
      "GD iter. 31/49: loss=8.671412647205132, w0=69.01243070575168, w1=11.715603722267177\n",
      "GD iter. 32/49: loss=8.550471711276233, w0=69.6134013875603, w1=11.401684008773076\n",
      "GD iter. 33/49: loss=8.491681008661416, w0=70.15427500118805, w1=11.119156266628384\n",
      "GD iter. 34/49: loss=8.461311223802968, w0=70.64106125345303, w1=10.864881298698162\n",
      "GD iter. 35/49: loss=8.447610999881228, w0=71.07916888049151, w1=10.636033827560961\n",
      "GD iter. 36/49: loss=8.450386779675199, w0=71.47346574482614, w1=10.43007110353748\n",
      "GD iter. 37/49: loss=8.463794225223332, w0=71.82833292272731, w1=10.244704651916347\n",
      "GD iter. 38/49: loss=8.496324873327906, w0=72.14771338283836, w1=10.077874845457329\n",
      "GD iter. 39/49: loss=8.531526198042219, w0=72.4351557969383, w1=9.927728019644212\n",
      "GD iter. 40/49: loss=8.568957341877308, w0=72.69385396962825, w1=9.792595876412406\n",
      "GD iter. 41/49: loss=8.612286122281905, w0=72.92668232504921, w1=9.67097694750378\n",
      "GD iter. 42/49: loss=8.660080580283541, w0=73.13622784492807, w1=9.561519911486018\n",
      "GD iter. 43/49: loss=8.706297632366027, w0=73.32481881281905, w1=9.463008579070031\n",
      "GD iter. 44/49: loss=8.749668068857297, w0=73.49455068392092, w1=9.374348379895643\n",
      "GD iter. 45/49: loss=8.79346080665718, w0=73.64730936791261, w1=9.294554200638695\n",
      "GD iter. 46/49: loss=8.834464678217012, w0=73.78479218350513, w1=9.222739439307441\n",
      "GD iter. 47/49: loss=8.87136816262086, w0=73.90852671753841, w1=9.158106154109312\n",
      "GD iter. 48/49: loss=8.9071541542051, w0=74.01988779816836, w1=9.099936197430996\n",
      "GD iter. 49/49: loss=8.940984847369569, w0=74.1201127707353, w1=9.047583236420511\n",
      "GD: execution time=0.002 seconds\n"
     ]
    }
   ],
   "source": [
    "from plots import gradient_descent_visualization\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([-100, 100])\n",
    "\n",
    "# Start gradient descent.\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "# ***************************************************\n",
    "# INSERT YOUR CODE HERE\n",
    "# TODO: fit the model to the subsampled data / subsampled data with outliers and visualize the cloud of points\n",
    "#       and the model fit\n",
    "# ***************************************************\n",
    "gd_losses, gd_ws = gradient_descent(y, tx, w_initial, max_iters, gamma)\n",
    "\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"GD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8d1808f58554868be4b5efeadd7bd4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=51, min=1), Output()), _dom_classes=('widge…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time Visualization\n",
    "from ipywidgets import IntSlider, interact\n",
    "\n",
    "\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        gd_losses,\n",
    "        gd_ws,\n",
    "        grid_losses,\n",
    "        grid_w0,\n",
    "        grid_w1,\n",
    "        mean_x,\n",
    "        std_x,\n",
    "        height,\n",
    "        weight,\n",
    "        n_iter,\n",
    "    )\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(gd_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 6. Subgradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_subgradient_mae(y, tx, w):\n",
    "    \"\"\"Compute the subgradient of the MAE at w.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N, 2)\n",
    "        w: numpy array of shape=(2, ). The vector of model parameters.\n",
    "\n",
    "    Returns:\n",
    "        A numpy array of shape (2, ) (same shape as w), containing the subgradient of the MAE at w.\n",
    "    \"\"\"\n",
    "    # Compute the errors: e = y - tx @ w\n",
    "    e = y - np.dot(tx, w)\n",
    "    \n",
    "    # Compute the sign of each error:\n",
    "    # For e > 0, np.sign(e) = 1\n",
    "    # For e < 0, np.sign(e) = -1\n",
    "    # For e == 0, np.sign(e) = 0\n",
    "    sign_e = np.sign(e)\n",
    "    sign_e[sign_e == 0] = 1\n",
    "\n",
    "    \n",
    "    # Calculate the subgradient by taking the dot product of tx.T and sign_e,\n",
    "    # then averaging across all examples by dividing by the number of data points\n",
    "    gradient = np.dot(tx.T, sign_e) / len(y)\n",
    "    \n",
    "    return -gradient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subgradient_descent(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"The SubGradient Descent (SubGD) algorithm.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        initial_w: numpy array of shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        max_iters: a scalar denoting the total number of iterations of GD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "\n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of SubGD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of SubGD\n",
    "    \"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO: compute subgradient and loss\n",
    "        # ***************************************************\n",
    "        loss = compute_loss(y, tx, w)\n",
    "        gradient = compute_subgradient_mae(y, tx, w)\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO: update w by subgradient\n",
    "        # ***************************************************\n",
    "        w = w - gamma * gradient\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        print(\n",
    "            \"SubGD iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}\".format(\n",
    "                bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SubGD iter. 0/499: loss=75.02213752383784, w0=1.0, w1=1.972405978382839e-15\n",
      "SubGD iter. 1/499: loss=74.02213752383784, w0=2.0, w1=3.944811956765678e-15\n",
      "SubGD iter. 2/499: loss=73.02213752383784, w0=3.0, w1=5.917217935148517e-15\n",
      "SubGD iter. 3/499: loss=72.02213752383784, w0=4.0, w1=7.889623913531356e-15\n",
      "SubGD iter. 4/499: loss=71.02213752383784, w0=5.0, w1=9.862029891914195e-15\n",
      "SubGD iter. 5/499: loss=70.02213752383786, w0=6.0, w1=1.1834435870297034e-14\n",
      "SubGD iter. 6/499: loss=69.02213752383786, w0=7.0, w1=1.3806841848679873e-14\n",
      "SubGD iter. 7/499: loss=68.02213752383786, w0=8.0, w1=1.5779247827062712e-14\n",
      "SubGD iter. 8/499: loss=67.02213752383786, w0=9.0, w1=1.775165380544555e-14\n",
      "SubGD iter. 9/499: loss=66.02213752383786, w0=10.0, w1=1.9724059783828387e-14\n",
      "SubGD iter. 10/499: loss=65.02213752383786, w0=11.0, w1=2.1696465762211225e-14\n",
      "SubGD iter. 11/499: loss=64.02213752383786, w0=12.0, w1=2.3668871740594062e-14\n",
      "SubGD iter. 12/499: loss=63.022137523837856, w0=13.0, w1=2.56412777189769e-14\n",
      "SubGD iter. 13/499: loss=62.02213752383784, w0=14.0, w1=2.7613683697359737e-14\n",
      "SubGD iter. 14/499: loss=61.02213752383785, w0=15.0, w1=2.9586089675742575e-14\n",
      "SubGD iter. 15/499: loss=60.02213752383785, w0=16.0, w1=3.155849565412541e-14\n",
      "SubGD iter. 16/499: loss=59.02213752383784, w0=17.0, w1=3.353090163250825e-14\n",
      "SubGD iter. 17/499: loss=58.02213752383784, w0=18.0, w1=3.550330761089109e-14\n",
      "SubGD iter. 18/499: loss=57.02213752383784, w0=19.0, w1=3.7475713589273925e-14\n",
      "SubGD iter. 19/499: loss=56.02213752383785, w0=20.0, w1=3.944811956765676e-14\n",
      "SubGD iter. 20/499: loss=55.02213752383785, w0=21.0, w1=4.14205255460396e-14\n",
      "SubGD iter. 21/499: loss=54.02213752383785, w0=22.0, w1=4.339293152442244e-14\n",
      "SubGD iter. 22/499: loss=53.02213752383785, w0=23.0, w1=4.5365337502805275e-14\n",
      "SubGD iter. 23/499: loss=52.02213752383785, w0=24.0, w1=4.733774348118811e-14\n",
      "SubGD iter. 24/499: loss=51.02213752383785, w0=25.0, w1=4.931014945957095e-14\n",
      "SubGD iter. 25/499: loss=50.02213752383784, w0=26.0, w1=5.128255543795379e-14\n",
      "SubGD iter. 26/499: loss=49.02213752383784, w0=27.0, w1=5.3254961416336625e-14\n",
      "SubGD iter. 27/499: loss=48.02213752383784, w0=28.0, w1=5.522736739471946e-14\n",
      "SubGD iter. 28/499: loss=47.02213752383784, w0=29.0, w1=5.71997733731023e-14\n",
      "SubGD iter. 29/499: loss=46.02213752383784, w0=30.0, w1=5.917217935148514e-14\n",
      "SubGD iter. 30/499: loss=45.02213752383784, w0=31.0, w1=6.114458532986797e-14\n",
      "SubGD iter. 31/499: loss=44.02213752383784, w0=32.0, w1=6.311699130825081e-14\n",
      "SubGD iter. 32/499: loss=43.02213752383784, w0=33.0, w1=6.508939728663365e-14\n",
      "SubGD iter. 33/499: loss=42.02213752383784, w0=34.0, w1=6.706180326501649e-14\n",
      "SubGD iter. 34/499: loss=41.02213752383784, w0=35.0, w1=6.903420924339932e-14\n",
      "SubGD iter. 35/499: loss=40.02213752383784, w0=36.0, w1=7.100661522178216e-14\n",
      "SubGD iter. 36/499: loss=39.02213752383784, w0=37.0, w1=7.2979021200165e-14\n",
      "SubGD iter. 37/499: loss=38.02213752383784, w0=38.0, w1=7.495142717854784e-14\n",
      "SubGD iter. 38/499: loss=37.02213752383785, w0=39.0, w1=7.692383315693067e-14\n",
      "SubGD iter. 39/499: loss=36.02213752383784, w0=40.0, w1=7.889623913531351e-14\n",
      "SubGD iter. 40/499: loss=35.02213752383784, w0=41.0, w1=8.086864511369635e-14\n",
      "SubGD iter. 41/499: loss=34.02213752383784, w0=42.0, w1=8.284105109207919e-14\n",
      "SubGD iter. 42/499: loss=33.02213752383784, w0=43.0, w1=8.481345707046202e-14\n",
      "SubGD iter. 43/499: loss=32.02213752383785, w0=44.0, w1=8.678586304884486e-14\n",
      "SubGD iter. 44/499: loss=31.022137523837845, w0=45.0, w1=8.87582690272277e-14\n",
      "SubGD iter. 45/499: loss=30.02213752383785, w0=46.0, w1=9.073067500561054e-14\n",
      "SubGD iter. 46/499: loss=29.02213752383785, w0=47.0, w1=9.270308098399337e-14\n",
      "SubGD iter. 47/499: loss=28.02849752216956, w0=47.990243902439026, w1=0.014899291859372448\n",
      "SubGD iter. 48/499: loss=27.05503596353009, w0=48.97073170731707, w1=0.04420974854060675\n",
      "SubGD iter. 49/499: loss=26.104410749365325, w0=49.921951219512195, w1=0.11136702822713689\n",
      "SubGD iter. 50/499: loss=25.202936984694386, w0=50.86341463414634, w1=0.19420797940573675\n",
      "SubGD iter. 51/499: loss=24.319170401894514, w0=51.78536585365853, w1=0.3047996354897653\n",
      "SubGD iter. 52/499: loss=23.46701638533665, w0=52.68780487804878, w1=0.43748492457145466\n",
      "SubGD iter. 53/499: loss=22.644908138513937, w0=53.5609756097561, w1=0.6039605034964969\n",
      "SubGD iter. 54/499: loss=21.860106232430713, w0=54.41463414634146, w1=0.7904086623079751\n",
      "SubGD iter. 55/499: loss=21.097945289094078, w0=55.25853658536585, w1=0.9864636167187474\n",
      "SubGD iter. 56/499: loss=20.356177812216508, w0=56.073170731707314, w1=1.2107447294950318\n",
      "SubGD iter. 57/499: loss=19.659274222647106, w0=56.8390243902439, w1=1.4695499112590786\n",
      "SubGD iter. 58/499: loss=19.007956932906886, w0=57.58536585365854, w1=1.7466069782192402\n",
      "SubGD iter. 59/499: loss=18.385455593865487, w0=58.29268292682927, w1=2.0495394374627907\n",
      "SubGD iter. 60/499: loss=17.796609753486923, w0=58.97073170731708, w1=2.369793063694113\n",
      "SubGD iter. 61/499: loss=17.2357364304025, w0=59.62926829268293, w1=2.7037413054215937\n",
      "SubGD iter. 62/499: loss=16.702530429930338, w0=60.22926829268293, w1=3.0669937749963108\n",
      "SubGD iter. 63/499: loss=16.212205019593018, w0=60.81951219512196, w1=3.4351412045457623\n",
      "SubGD iter. 64/499: loss=15.72971119221175, w0=61.400000000000006, w1=3.8100972455511855\n",
      "SubGD iter. 65/499: loss=15.255162109683669, w0=61.960975609756105, w1=4.19481414142407\n",
      "SubGD iter. 66/499: loss=14.79624382688472, w0=62.50243902439025, w1=4.590235961626551\n",
      "SubGD iter. 67/499: loss=14.3551351014283, w0=63.024390243902445, w1=4.988966869126215\n",
      "SubGD iter. 68/499: loss=13.925644471508397, w0=63.53658536585367, w1=5.394723311322422\n",
      "SubGD iter. 69/499: loss=13.504018297561288, w0=64.02926829268294, w1=5.806973921072139\n",
      "SubGD iter. 70/499: loss=13.091331265933215, w0=64.52195121951222, w1=6.219224530821856\n",
      "SubGD iter. 71/499: loss=12.68107782667686, w0=64.98536585365856, w1=6.65223970547911\n",
      "SubGD iter. 72/499: loss=12.282667872987272, w0=65.41951219512198, w1=7.09245237403098\n",
      "SubGD iter. 73/499: loss=11.904116361896536, w0=65.83414634146345, w1=7.539321499352554\n",
      "SubGD iter. 74/499: loss=11.532802215052726, w0=66.25853658536589, w1=7.976087984472192\n",
      "SubGD iter. 75/499: loss=11.162056249363333, w0=66.67317073170736, w1=8.41417291021312\n",
      "SubGD iter. 76/499: loss=10.798216371889582, w0=67.08780487804883, w1=8.852257835954047\n",
      "SubGD iter. 77/499: loss=10.437215203503841, w0=67.49268292682932, w1=9.286511903716923\n",
      "SubGD iter. 78/499: loss=10.086417172783387, w0=67.87804878048786, w1=9.727060907583345\n",
      "SubGD iter. 79/499: loss=9.752206992127919, w0=68.23414634146347, w1=10.160551990401034\n",
      "SubGD iter. 80/499: loss=9.442071098085815, w0=68.57073170731714, w1=10.583607960860938\n",
      "SubGD iter. 81/499: loss=9.153276883764049, w0=68.87804878048787, w1=11.015936693280766\n",
      "SubGD iter. 82/499: loss=8.873954713928448, w0=69.17560975609763, w1=11.442235149007491\n",
      "SubGD iter. 83/499: loss=8.606626560520008, w0=69.48292682926837, w1=11.850554264404332\n",
      "SubGD iter. 84/499: loss=8.351376571556136, w0=69.7902439024391, w1=12.229568001190128\n",
      "SubGD iter. 85/499: loss=8.121339447335934, w0=70.09756097560984, w1=12.5825971628511\n",
      "SubGD iter. 86/499: loss=7.904224050980764, w0=70.3951219512196, w1=12.926440127939236\n",
      "SubGD iter. 87/499: loss=7.69883365183531, w0=70.68292682926837, w1=13.265178268500748\n",
      "SubGD iter. 88/499: loss=7.504042592124453, w0=70.96097560975618, w1=13.59907902590862\n",
      "SubGD iter. 89/499: loss=7.325222954375614, w0=71.20975609756105, w1=13.909489165387575\n",
      "SubGD iter. 90/499: loss=7.172207744980926, w0=71.43902439024399, w1=14.189671585907725\n",
      "SubGD iter. 91/499: loss=7.043558502708238, w0=71.65853658536594, w1=14.455275351929902\n",
      "SubGD iter. 92/499: loss=6.945599968319611, w0=71.80975609756106, w1=14.67548296096918\n",
      "SubGD iter. 93/499: loss=6.879223987975134, w0=71.92195121951228, w1=14.8518064159344\n",
      "SubGD iter. 94/499: loss=6.839289570397842, w0=72.01463414634155, w1=14.992813596065272\n",
      "SubGD iter. 95/499: loss=6.810981131725575, w0=72.1170731707318, w1=15.12340008383299\n",
      "SubGD iter. 96/499: loss=6.786256377971244, w0=72.20000000000009, w1=15.240566367599413\n",
      "SubGD iter. 97/499: loss=6.769333688821585, w0=72.25365853658545, w1=15.328447168197647\n",
      "SubGD iter. 98/499: loss=6.759359815076719, w0=72.28780487804886, w1=15.411909984588924\n",
      "SubGD iter. 99/499: loss=6.751227800721418, w0=72.32195121951227, w1=15.495372800980201\n",
      "SubGD iter. 100/499: loss=6.744588573068024, w0=72.34634146341472, w1=15.554870311196275\n",
      "SubGD iter. 101/499: loss=6.741674579626078, w0=72.3512195121952, w1=15.597404931586\n",
      "SubGD iter. 102/499: loss=6.739841590334476, w0=72.35609756097568, w1=15.639939551975726\n",
      "SubGD iter. 103/499: loss=6.738429695902145, w0=72.3512195121952, w1=15.667406230634016\n",
      "SubGD iter. 104/499: loss=6.73765148210572, w0=72.34634146341472, w1=15.694872909292307\n",
      "SubGD iter. 105/499: loss=6.7368732683092984, w0=72.34146341463423, w1=15.722339587950598\n",
      "SubGD iter. 106/499: loss=6.7362257249736395, w0=72.32682926829277, w1=15.742364255888372\n",
      "SubGD iter. 107/499: loss=6.735610579408477, w0=72.3121951219513, w1=15.762388923826146\n",
      "SubGD iter. 108/499: loss=6.734995433843317, w0=72.29756097560984, w1=15.78241359176392\n",
      "SubGD iter. 109/499: loss=6.734380288278155, w0=72.28292682926838, w1=15.802438259701693\n",
      "SubGD iter. 110/499: loss=6.733765142712993, w0=72.26829268292691, w1=15.822462927639467\n",
      "SubGD iter. 111/499: loss=6.733149997147833, w0=72.25365853658545, w1=15.842487595577241\n",
      "SubGD iter. 112/499: loss=6.732534851582671, w0=72.23902439024398, w1=15.862512263515015\n",
      "SubGD iter. 113/499: loss=6.73191970601751, w0=72.22439024390252, w1=15.882536931452789\n",
      "SubGD iter. 114/499: loss=6.7313045604523465, w0=72.20975609756105, w1=15.902561599390562\n",
      "SubGD iter. 115/499: loss=6.730689414887186, w0=72.19512195121959, w1=15.922586267328336\n",
      "SubGD iter. 116/499: loss=6.730133790448801, w0=72.19024390243911, w1=15.944410095470811\n",
      "SubGD iter. 117/499: loss=6.729633715614104, w0=72.18536585365862, w1=15.966233923613286\n",
      "SubGD iter. 118/499: loss=6.7291336407794065, w0=72.18048780487814, w1=15.988057751755761\n",
      "SubGD iter. 119/499: loss=6.72863356594471, w0=72.17560975609766, w1=16.009881579898234\n",
      "SubGD iter. 120/499: loss=6.728133491110013, w0=72.17073170731717, w1=16.031705408040708\n",
      "SubGD iter. 121/499: loss=6.7276334162753155, w0=72.16585365853669, w1=16.05352923618318\n",
      "SubGD iter. 122/499: loss=6.727133341440617, w0=72.1609756097562, w1=16.075353064325654\n",
      "SubGD iter. 123/499: loss=6.726633266605922, w0=72.15609756097572, w1=16.097176892468127\n",
      "SubGD iter. 124/499: loss=6.726180983825642, w0=72.1609756097562, w1=16.11782181868801\n",
      "SubGD iter. 125/499: loss=6.725730975487112, w0=72.16585365853669, w1=16.138466744907895\n",
      "SubGD iter. 126/499: loss=6.725324305267181, w0=72.1609756097562, w1=16.157312510923077\n",
      "SubGD iter. 127/499: loss=6.7249453470105784, w0=72.15609756097572, w1=16.17615827693826\n",
      "SubGD iter. 128/499: loss=6.724566388753974, w0=72.15121951219524, w1=16.195004042953443\n",
      "SubGD iter. 129/499: loss=6.724208928316052, w0=72.15609756097572, w1=16.203566037298923\n",
      "SubGD iter. 130/499: loss=6.7241118252089755, w0=72.1609756097562, w1=16.212128031644404\n",
      "SubGD iter. 131/499: loss=6.7240147221018995, w0=72.16585365853669, w1=16.220690025989885\n",
      "SubGD iter. 132/499: loss=6.723917618994821, w0=72.17073170731717, w1=16.229252020335366\n",
      "SubGD iter. 133/499: loss=6.72383916339912, w0=72.18536585365864, w1=16.225378847310633\n",
      "SubGD iter. 134/499: loss=6.723868906626966, w0=72.18048780487815, w1=16.235119743578704\n",
      "SubGD iter. 135/499: loss=6.723750226206956, w0=72.17560975609767, w1=16.244860639846774\n",
      "SubGD iter. 136/499: loss=6.7238282322034495, w0=72.19024390243914, w1=16.24098746682204\n",
      "SubGD iter. 137/499: loss=6.723740660043321, w0=72.18536585365865, w1=16.25072836309011\n",
      "SubGD iter. 138/499: loss=6.723776480790018, w0=72.18048780487817, w1=16.237931578240566\n",
      "SubGD iter. 139/499: loss=6.723754234244196, w0=72.18536585365865, w1=16.235237307138423\n",
      "SubGD iter. 140/499: loss=6.723772876392419, w0=72.18048780487817, w1=16.244978203406493\n",
      "SubGD iter. 141/499: loss=6.723773219762748, w0=72.18536585365865, w1=16.24228393230435\n",
      "SubGD iter. 142/499: loss=6.723742165306071, w0=72.19024390243914, w1=16.239589661202206\n",
      "SubGD iter. 143/499: loss=6.723754275922866, w0=72.18536585365865, w1=16.249330557470277\n",
      "SubGD iter. 144/499: loss=6.723761150824625, w0=72.19024390243914, w1=16.246636286368133\n",
      "SubGD iter. 145/499: loss=6.7237479107245255, w0=72.18536585365865, w1=16.233839501518588\n",
      "SubGD iter. 146/499: loss=6.723786492271966, w0=72.18048780487817, w1=16.24358039778666\n",
      "SubGD iter. 147/499: loss=6.72376945369546, w0=72.18536585365865, w1=16.240886126684515\n",
      "SubGD iter. 148/499: loss=6.723738399238783, w0=72.19024390243914, w1=16.23819185558237\n",
      "SubGD iter. 149/499: loss=6.723767891802413, w0=72.18536585365865, w1=16.247932751850442\n",
      "SubGD iter. 150/499: loss=6.723757384757336, w0=72.19024390243914, w1=16.2452384807483\n",
      "SubGD iter. 151/499: loss=6.7237300233067465, w0=72.18536585365865, w1=16.232441695898753\n",
      "SubGD iter. 152/499: loss=6.723800108151511, w0=72.18048780487817, w1=16.242182592166824\n",
      "SubGD iter. 153/499: loss=6.7237656876281715, w0=72.18536585365865, w1=16.23948832106468\n",
      "SubGD iter. 154/499: loss=6.723734633171497, w0=72.19024390243914, w1=16.236794049962537\n",
      "SubGD iter. 155/499: loss=6.723781507681959, w0=72.18536585365865, w1=16.246534946230607\n",
      "SubGD iter. 156/499: loss=6.723753618690048, w0=72.19024390243914, w1=16.243840675128464\n",
      "SubGD iter. 157/499: loss=6.7237225642333724, w0=72.19512195121962, w1=16.24114640402632\n",
      "SubGD iter. 158/499: loss=6.723772850676965, w0=72.18048780487815, w1=16.240784786546993\n",
      "SubGD iter. 159/499: loss=6.723761921560883, w0=72.18536585365864, w1=16.23809051544485\n",
      "SubGD iter. 160/499: loss=6.723745083586276, w0=72.18048780487815, w1=16.24783141171292\n",
      "SubGD iter. 161/499: loss=6.723780907079435, w0=72.18536585365864, w1=16.245137140610776\n",
      "SubGD iter. 162/499: loss=6.72374985262276, w0=72.19024390243912, w1=16.242442869508633\n",
      "SubGD iter. 163/499: loss=6.723726483116722, w0=72.18536585365864, w1=16.252183765776703\n",
      "SubGD iter. 164/499: loss=6.7237951052650695, w0=72.18048780487815, w1=16.239386980927158\n",
      "SubGD iter. 165/499: loss=6.723758155493597, w0=72.18536585365864, w1=16.236692709825014\n",
      "SubGD iter. 166/499: loss=6.723758699465821, w0=72.18048780487815, w1=16.246433606093085\n",
      "SubGD iter. 167/499: loss=6.723777141012149, w0=72.18536585365864, w1=16.24373933499094\n",
      "SubGD iter. 168/499: loss=6.723746086555472, w0=72.19024390243912, w1=16.241045063888798\n",
      "SubGD iter. 169/499: loss=6.723740098996269, w0=72.18536585365864, w1=16.25078596015687\n",
      "SubGD iter. 170/499: loss=6.723777217847291, w0=72.18048780487815, w1=16.237989175307323\n",
      "SubGD iter. 171/499: loss=6.723754389426309, w0=72.18536585365864, w1=16.23529490420518\n",
      "SubGD iter. 172/499: loss=6.723772315345367, w0=72.18048780487815, w1=16.24503580047325\n",
      "SubGD iter. 173/499: loss=6.723773374944861, w0=72.18536585365864, w1=16.242341529371107\n",
      "SubGD iter. 174/499: loss=6.723742320488185, w0=72.19024390243912, w1=16.239647258268963\n",
      "SubGD iter. 175/499: loss=6.723753714875815, w0=72.18536585365864, w1=16.249388154537034\n",
      "SubGD iter. 176/499: loss=6.723761306006737, w0=72.19024390243912, w1=16.24669388343489\n",
      "SubGD iter. 177/499: loss=6.723748647781798, w0=72.18536585365864, w1=16.233897098585345\n",
      "SubGD iter. 178/499: loss=6.723785931224913, w0=72.18048780487815, w1=16.243637994853415\n",
      "SubGD iter. 179/499: loss=6.723769608877573, w0=72.18536585365864, w1=16.240943723751272\n",
      "SubGD iter. 180/499: loss=6.723738554420896, w0=72.19024390243912, w1=16.23824945264913\n",
      "SubGD iter. 181/499: loss=6.7237673307553605, w0=72.18536585365864, w1=16.2479903489172\n",
      "SubGD iter. 182/499: loss=6.723757539939448, w0=72.19024390243912, w1=16.245296077815055\n",
      "SubGD iter. 183/499: loss=6.723730760364017, w0=72.18536585365864, w1=16.23249929296551\n",
      "SubGD iter. 184/499: loss=6.723799547104458, w0=72.18048780487815, w1=16.24224018923358\n",
      "SubGD iter. 185/499: loss=6.723765842810285, w0=72.18536585365864, w1=16.239545918131437\n",
      "SubGD iter. 186/499: loss=6.7237347883536085, w0=72.19024390243912, w1=16.236851647029294\n",
      "SubGD iter. 187/499: loss=6.723780946634906, w0=72.18536585365864, w1=16.246592543297364\n",
      "SubGD iter. 188/499: loss=6.7237537738721604, w0=72.19024390243912, w1=16.24389827219522\n",
      "SubGD iter. 189/499: loss=6.723722719415485, w0=72.1951219512196, w1=16.241204001093077\n",
      "SubGD iter. 190/499: loss=6.723772871505071, w0=72.18048780487814, w1=16.24084238361375\n",
      "SubGD iter. 191/499: loss=6.723762076742997, w0=72.18536585365862, w1=16.238148112511606\n",
      "SubGD iter. 192/499: loss=6.723744522539223, w0=72.18048780487814, w1=16.247889008779676\n",
      "SubGD iter. 193/499: loss=6.7237810622615495, w0=72.18536585365862, w1=16.245194737677533\n",
      "SubGD iter. 194/499: loss=6.723750007804874, w0=72.19024390243911, w1=16.24250046657539\n",
      "SubGD iter. 195/499: loss=6.72372592206967, w0=72.18536585365862, w1=16.25224136284346\n",
      "SubGD iter. 196/499: loss=6.723795842322342, w0=72.18048780487814, w1=16.239444577993915\n",
      "SubGD iter. 197/499: loss=6.723758310675708, w0=72.18536585365862, w1=16.23675030689177\n",
      "SubGD iter. 198/499: loss=6.72375813841877, w0=72.18048780487814, w1=16.24649120315984\n",
      "SubGD iter. 199/499: loss=6.72377729619426, w0=72.18536585365862, w1=16.2437969320577\n",
      "SubGD iter. 200/499: loss=6.723746241737584, w0=72.19024390243911, w1=16.241102660955555\n",
      "SubGD iter. 201/499: loss=6.723739537949216, w0=72.18536585365862, w1=16.250843557223625\n",
      "SubGD iter. 202/499: loss=6.723777954904563, w0=72.18048780487814, w1=16.23804677237408\n",
      "SubGD iter. 203/499: loss=6.723754544608421, w0=72.18536585365862, w1=16.235352501271937\n",
      "SubGD iter. 204/499: loss=6.723771754298313, w0=72.18048780487814, w1=16.245093397540007\n",
      "SubGD iter. 205/499: loss=6.723773530126973, w0=72.18536585365862, w1=16.242399126437864\n",
      "SubGD iter. 206/499: loss=6.723742475670296, w0=72.19024390243911, w1=16.23970485533572\n",
      "SubGD iter. 207/499: loss=6.723753153828762, w0=72.18536585365862, w1=16.24944575160379\n",
      "SubGD iter. 208/499: loss=6.723761461188848, w0=72.19024390243911, w1=16.246751480501647\n",
      "SubGD iter. 209/499: loss=6.723749384839067, w0=72.18536585365862, w1=16.233954695652102\n",
      "SubGD iter. 210/499: loss=6.72378537017786, w0=72.18048780487814, w1=16.243695591920172\n",
      "SubGD iter. 211/499: loss=6.723769764059685, w0=72.18536585365862, w1=16.24100132081803\n",
      "SubGD iter. 212/499: loss=6.7237387096030075, w0=72.19024390243911, w1=16.238307049715885\n",
      "SubGD iter. 213/499: loss=6.723766769708307, w0=72.18536585365862, w1=16.248047945983956\n",
      "SubGD iter. 214/499: loss=6.723757695121562, w0=72.19024390243911, w1=16.245353674881812\n",
      "SubGD iter. 215/499: loss=6.72373149742129, w0=72.18536585365862, w1=16.232556890032267\n",
      "SubGD iter. 216/499: loss=6.723798986057407, w0=72.18048780487814, w1=16.242297786300337\n",
      "SubGD iter. 217/499: loss=6.7237659979923965, w0=72.18536585365862, w1=16.239603515198194\n",
      "SubGD iter. 218/499: loss=6.723734943535722, w0=72.19024390243911, w1=16.23690924409605\n",
      "SubGD iter. 219/499: loss=6.723780385587853, w0=72.18536585365862, w1=16.24665014036412\n",
      "SubGD iter. 220/499: loss=6.723753929054273, w0=72.19024390243911, w1=16.243955869261978\n",
      "SubGD iter. 221/499: loss=6.7237228745975965, w0=72.19512195121959, w1=16.241261598159834\n",
      "SubGD iter. 222/499: loss=6.723772892333177, w0=72.18048780487813, w1=16.240899980680506\n",
      "SubGD iter. 223/499: loss=6.72376223192511, w0=72.18536585365861, w1=16.238205709578363\n",
      "SubGD iter. 224/499: loss=6.723743961492171, w0=72.18048780487813, w1=16.247946605846433\n",
      "SubGD iter. 225/499: loss=6.723781217443662, w0=72.18536585365861, w1=16.24525233474429\n",
      "SubGD iter. 226/499: loss=6.723750162986986, w0=72.1902439024391, w1=16.242558063642146\n",
      "SubGD iter. 227/499: loss=6.723725361022617, w0=72.18536585365861, w1=16.252298959910217\n",
      "SubGD iter. 228/499: loss=6.723796579379611, w0=72.18048780487813, w1=16.23950217506067\n",
      "SubGD iter. 229/499: loss=6.723758465857822, w0=72.18536585365861, w1=16.23680790395853\n",
      "SubGD iter. 230/499: loss=6.723757577371717, w0=72.18048780487813, w1=16.2465488002266\n",
      "SubGD iter. 231/499: loss=6.723777451376374, w0=72.18536585365861, w1=16.243854529124455\n",
      "SubGD iter. 232/499: loss=6.7237463969196964, w0=72.1902439024391, w1=16.241160258022312\n",
      "SubGD iter. 233/499: loss=6.723738976902163, w0=72.18536585365861, w1=16.250901154290382\n",
      "SubGD iter. 234/499: loss=6.723778691961833, w0=72.18048780487813, w1=16.238104369440837\n",
      "SubGD iter. 235/499: loss=6.7237546997905335, w0=72.18536585365861, w1=16.235410098338694\n",
      "SubGD iter. 236/499: loss=6.723771193251261, w0=72.18048780487813, w1=16.245150994606764\n",
      "SubGD iter. 237/499: loss=6.7237736853090855, w0=72.18536585365861, w1=16.24245672350462\n",
      "SubGD iter. 238/499: loss=6.723742630852409, w0=72.1902439024391, w1=16.239762452402477\n",
      "SubGD iter. 239/499: loss=6.72375259278171, w0=72.18536585365861, w1=16.249503348670547\n",
      "SubGD iter. 240/499: loss=6.723761616370962, w0=72.1902439024391, w1=16.246809077568404\n",
      "SubGD iter. 241/499: loss=6.723750121896339, w0=72.18536585365861, w1=16.23401229271886\n",
      "SubGD iter. 242/499: loss=6.723784809130808, w0=72.18048780487813, w1=16.24375318898693\n",
      "SubGD iter. 243/499: loss=6.723769919241798, w0=72.18536585365861, w1=16.241058917884786\n",
      "SubGD iter. 244/499: loss=6.723738864785122, w0=72.1902439024391, w1=16.238364646782642\n",
      "SubGD iter. 245/499: loss=6.723766208661255, w0=72.18536585365861, w1=16.248105543050713\n",
      "SubGD iter. 246/499: loss=6.723757850303674, w0=72.1902439024391, w1=16.24541127194857\n",
      "SubGD iter. 247/499: loss=6.723732234478561, w0=72.18536585365861, w1=16.232614487099024\n",
      "SubGD iter. 248/499: loss=6.7237984250103535, w0=72.18048780487813, w1=16.242355383367094\n",
      "SubGD iter. 249/499: loss=6.72376615317451, w0=72.18536585365861, w1=16.23966111226495\n",
      "SubGD iter. 250/499: loss=6.7237350987178335, w0=72.1902439024391, w1=16.236966841162808\n",
      "SubGD iter. 251/499: loss=6.7237798245408005, w0=72.18536585365861, w1=16.246707737430878\n",
      "SubGD iter. 252/499: loss=6.723754084236385, w0=72.1902439024391, w1=16.244013466328735\n",
      "SubGD iter. 253/499: loss=6.723723029779708, w0=72.19512195121958, w1=16.24131919522659\n",
      "SubGD iter. 254/499: loss=6.723772913161282, w0=72.18048780487811, w1=16.240957577747263\n",
      "SubGD iter. 255/499: loss=6.723762387107221, w0=72.1853658536586, w1=16.23826330664512\n",
      "SubGD iter. 256/499: loss=6.723743400445118, w0=72.18048780487811, w1=16.24800420291319\n",
      "SubGD iter. 257/499: loss=6.723781372625775, w0=72.1853658536586, w1=16.245309931811047\n",
      "SubGD iter. 258/499: loss=6.723750318169097, w0=72.19024390243908, w1=16.242615660708903\n",
      "SubGD iter. 259/499: loss=6.723724799975565, w0=72.1853658536586, w1=16.252356556976974\n",
      "SubGD iter. 260/499: loss=6.723797316436882, w0=72.18048780487811, w1=16.23955977212743\n",
      "SubGD iter. 261/499: loss=6.723758621039934, w0=72.1853658536586, w1=16.236865501025285\n",
      "SubGD iter. 262/499: loss=6.7237570163246625, w0=72.18048780487811, w1=16.246606397293355\n",
      "SubGD iter. 263/499: loss=6.723777606558487, w0=72.1853658536586, w1=16.243912126191212\n",
      "SubGD iter. 264/499: loss=6.723746552101811, w0=72.19024390243908, w1=16.24121785508907\n",
      "SubGD iter. 265/499: loss=6.723738415855111, w0=72.1853658536586, w1=16.25095875135714\n",
      "SubGD iter. 266/499: loss=6.723779429019105, w0=72.18048780487811, w1=16.238161966507594\n",
      "SubGD iter. 267/499: loss=6.723754854972648, w0=72.1853658536586, w1=16.23546769540545\n",
      "SubGD iter. 268/499: loss=6.723770632204209, w0=72.18048780487811, w1=16.24520859167352\n",
      "SubGD iter. 269/499: loss=6.7237738404912, w0=72.1853658536586, w1=16.242514320571377\n",
      "SubGD iter. 270/499: loss=6.7237427860345225, w0=72.19024390243908, w1=16.239820049469234\n",
      "SubGD iter. 271/499: loss=6.723752031734657, w0=72.1853658536586, w1=16.249560945737304\n",
      "SubGD iter. 272/499: loss=6.7237617715530735, w0=72.19024390243908, w1=16.24686667463516\n",
      "SubGD iter. 273/499: loss=6.72375085895361, w0=72.1853658536586, w1=16.234069889785616\n",
      "SubGD iter. 274/499: loss=6.723784248083754, w0=72.18048780487811, w1=16.243810786053686\n",
      "SubGD iter. 275/499: loss=6.723770074423911, w0=72.1853658536586, w1=16.241116514951543\n",
      "SubGD iter. 276/499: loss=6.723739019967234, w0=72.19024390243908, w1=16.2384222438494\n",
      "SubGD iter. 277/499: loss=6.723765647614202, w0=72.1853658536586, w1=16.24816314011747\n",
      "SubGD iter. 278/499: loss=6.723758005485786, w0=72.19024390243908, w1=16.245468869015326\n",
      "SubGD iter. 279/499: loss=6.723732971535831, w0=72.1853658536586, w1=16.23267208416578\n",
      "SubGD iter. 280/499: loss=6.7237978639633, w0=72.18048780487811, w1=16.24241298043385\n",
      "SubGD iter. 281/499: loss=6.723766308356623, w0=72.1853658536586, w1=16.239718709331708\n",
      "SubGD iter. 282/499: loss=6.723735253899945, w0=72.19024390243908, w1=16.237024438229565\n",
      "SubGD iter. 283/499: loss=6.723779263493747, w0=72.1853658536586, w1=16.246765334497635\n",
      "SubGD iter. 284/499: loss=6.723754239418499, w0=72.19024390243908, w1=16.24407106339549\n",
      "SubGD iter. 285/499: loss=6.723723184961821, w0=72.19512195121956, w1=16.241376792293348\n",
      "SubGD iter. 286/499: loss=6.7237729339893875, w0=72.1804878048781, w1=16.24101517481402\n",
      "SubGD iter. 287/499: loss=6.723762542289336, w0=72.18536585365858, w1=16.238320903711877\n",
      "SubGD iter. 288/499: loss=6.7237428393980645, w0=72.1804878048781, w1=16.248061799979947\n",
      "SubGD iter. 289/499: loss=6.723781527807887, w0=72.18536585365858, w1=16.245367528877804\n",
      "SubGD iter. 290/499: loss=6.723750473351211, w0=72.19024390243906, w1=16.24267325777566\n",
      "SubGD iter. 291/499: loss=6.723724238928512, w0=72.18536585365858, w1=16.25241415404373\n",
      "SubGD iter. 292/499: loss=6.7237980534941535, w0=72.1804878048781, w1=16.239617369194185\n",
      "SubGD iter. 293/499: loss=6.723758776222047, w0=72.18536585365858, w1=16.236923098092042\n",
      "SubGD iter. 294/499: loss=6.72375645527761, w0=72.1804878048781, w1=16.246663994360112\n",
      "SubGD iter. 295/499: loss=6.723777761740599, w0=72.18536585365858, w1=16.24396972325797\n",
      "SubGD iter. 296/499: loss=6.723746707283922, w0=72.19024390243906, w1=16.241275452155826\n",
      "SubGD iter. 297/499: loss=6.723737854808058, w0=72.18536585365858, w1=16.251016348423896\n",
      "SubGD iter. 298/499: loss=6.723780166076375, w0=72.1804878048781, w1=16.23821956357435\n",
      "SubGD iter. 299/499: loss=6.7237550101547585, w0=72.18536585365858, w1=16.235525292472207\n",
      "SubGD iter. 300/499: loss=6.723770071157156, w0=72.1804878048781, w1=16.245266188740278\n",
      "SubGD iter. 301/499: loss=6.7237739956733105, w0=72.18536585365858, w1=16.242571917638134\n",
      "SubGD iter. 302/499: loss=6.723742941216636, w0=72.19024390243906, w1=16.23987764653599\n",
      "SubGD iter. 303/499: loss=6.723751470687603, w0=72.18536585365858, w1=16.24961854280406\n",
      "SubGD iter. 304/499: loss=6.7237622786585955, w0=72.1804878048781, w1=16.236821757954516\n",
      "SubGD iter. 305/499: loss=6.723751244087472, w0=72.18536585365858, w1=16.234127486852373\n",
      "SubGD iter. 306/499: loss=6.723783687036702, w0=72.1804878048781, w1=16.243868383120443\n",
      "SubGD iter. 307/499: loss=6.723770229606022, w0=72.18536585365858, w1=16.2411741120183\n",
      "SubGD iter. 308/499: loss=6.723739175149348, w0=72.19024390243906, w1=16.238479840916156\n",
      "SubGD iter. 309/499: loss=6.723765086567148, w0=72.18536585365858, w1=16.248220737184226\n",
      "SubGD iter. 310/499: loss=6.7237581606678996, w0=72.19024390243906, w1=16.245526466082083\n",
      "SubGD iter. 311/499: loss=6.723733708593103, w0=72.18536585365858, w1=16.232729681232538\n",
      "SubGD iter. 312/499: loss=6.723797302916248, w0=72.1804878048781, w1=16.242470577500608\n",
      "SubGD iter. 313/499: loss=6.723766463538735, w0=72.18536585365858, w1=16.239776306398465\n",
      "SubGD iter. 314/499: loss=6.723735409082059, w0=72.19024390243906, w1=16.23708203529632\n",
      "SubGD iter. 315/499: loss=6.723778702446697, w0=72.18536585365858, w1=16.24682293156439\n",
      "SubGD iter. 316/499: loss=6.723754394600611, w0=72.19024390243906, w1=16.24412866046225\n",
      "SubGD iter. 317/499: loss=6.723723340143934, w0=72.19512195121955, w1=16.241434389360105\n",
      "SubGD iter. 318/499: loss=6.723772954817495, w0=72.18048780487808, w1=16.241072771880777\n",
      "SubGD iter. 319/499: loss=6.723762697471448, w0=72.18536585365857, w1=16.238378500778634\n",
      "SubGD iter. 320/499: loss=6.723742278351011, w0=72.18048780487808, w1=16.248119397046704\n",
      "SubGD iter. 321/499: loss=6.723781682989999, w0=72.18536585365857, w1=16.24542512594456\n",
      "SubGD iter. 322/499: loss=6.723750628533324, w0=72.19024390243905, w1=16.242730854842417\n",
      "SubGD iter. 323/499: loss=6.723723677881459, w0=72.18536585365857, w1=16.252471751110487\n",
      "SubGD iter. 324/499: loss=6.723798790551426, w0=72.18048780487808, w1=16.239674966260942\n",
      "SubGD iter. 325/499: loss=6.723758931404158, w0=72.18536585365857, w1=16.2369806951588\n",
      "SubGD iter. 326/499: loss=6.723755894230558, w0=72.18048780487808, w1=16.24672159142687\n",
      "SubGD iter. 327/499: loss=6.72377791692271, w0=72.18536585365857, w1=16.244027320324726\n",
      "SubGD iter. 328/499: loss=6.723746862466035, w0=72.19024390243905, w1=16.241333049222582\n",
      "SubGD iter. 329/499: loss=6.7237372937610065, w0=72.18536585365857, w1=16.251073945490653\n",
      "SubGD iter. 330/499: loss=6.7237809031336475, w0=72.18048780487808, w1=16.238277160641108\n",
      "SubGD iter. 331/499: loss=6.723755165336872, w0=72.18536585365857, w1=16.235582889538964\n",
      "SubGD iter. 332/499: loss=6.723769510110104, w0=72.18048780487808, w1=16.245323785807035\n",
      "SubGD iter. 333/499: loss=6.723774150855425, w0=72.18536585365857, w1=16.24262951470489\n",
      "SubGD iter. 334/499: loss=6.7237430963987475, w0=72.19024390243905, w1=16.239935243602748\n",
      "SubGD iter. 335/499: loss=6.723750909640552, w0=72.18536585365857, w1=16.249676139870818\n",
      "SubGD iter. 336/499: loss=6.723763015715868, w0=72.18048780487808, w1=16.236879355021273\n",
      "SubGD iter. 337/499: loss=6.723751399269584, w0=72.18536585365857, w1=16.23418508391913\n",
      "SubGD iter. 338/499: loss=6.72378312598965, w0=72.18048780487808, w1=16.2439259801872\n",
      "SubGD iter. 339/499: loss=6.723770384788136, w0=72.18536585365857, w1=16.241231709085056\n",
      "SubGD iter. 340/499: loss=6.72373933033146, w0=72.19024390243905, w1=16.238537437982913\n",
      "SubGD iter. 341/499: loss=6.723764525520097, w0=72.18536585365857, w1=16.248278334250983\n",
      "SubGD iter. 342/499: loss=6.723758315850012, w0=72.19024390243905, w1=16.24558406314884\n",
      "SubGD iter. 343/499: loss=6.723734445650374, w0=72.18536585365857, w1=16.232787278299295\n",
      "SubGD iter. 344/499: loss=6.723796741869195, w0=72.18048780487808, w1=16.242528174567365\n",
      "SubGD iter. 345/499: loss=6.723766618720848, w0=72.18536585365857, w1=16.23983390346522\n",
      "SubGD iter. 346/499: loss=6.723735564264171, w0=72.19024390243905, w1=16.23713963236308\n",
      "SubGD iter. 347/499: loss=6.723778141399643, w0=72.18536585365857, w1=16.24688052863115\n",
      "SubGD iter. 348/499: loss=6.723754549782723, w0=72.19024390243905, w1=16.244186257529005\n",
      "SubGD iter. 349/499: loss=6.7237234953260465, w0=72.19512195121953, w1=16.241491986426862\n",
      "SubGD iter. 350/499: loss=6.7237729756456, w0=72.18048780487807, w1=16.241130368947534\n",
      "SubGD iter. 351/499: loss=6.72376285265356, w0=72.18536585365855, w1=16.23843609784539\n",
      "SubGD iter. 352/499: loss=6.723741717303959, w0=72.18048780487807, w1=16.24817699411346\n",
      "SubGD iter. 353/499: loss=6.723781838172115, w0=72.18536585365855, w1=16.245482723011317\n",
      "SubGD iter. 354/499: loss=6.7237507837154356, w0=72.19024390243904, w1=16.242788451909174\n",
      "SubGD iter. 355/499: loss=6.723723116834408, w0=72.18536585365855, w1=16.252529348177244\n",
      "SubGD iter. 356/499: loss=6.723799527608695, w0=72.18048780487807, w1=16.2397325633277\n",
      "SubGD iter. 357/499: loss=6.723759086586273, w0=72.18536585365855, w1=16.237038292225556\n",
      "SubGD iter. 358/499: loss=6.723755333183505, w0=72.18048780487807, w1=16.246779188493626\n",
      "SubGD iter. 359/499: loss=6.723778072104824, w0=72.18536585365855, w1=16.244084917391483\n",
      "SubGD iter. 360/499: loss=6.723747017648149, w0=72.19024390243904, w1=16.24139064628934\n",
      "SubGD iter. 361/499: loss=6.723736732713953, w0=72.18536585365855, w1=16.25113154255741\n",
      "SubGD iter. 362/499: loss=6.723781640190919, w0=72.18048780487807, w1=16.238334757707864\n",
      "SubGD iter. 363/499: loss=6.723755320518984, w0=72.18536585365855, w1=16.23564048660572\n",
      "SubGD iter. 364/499: loss=6.723768949063051, w0=72.18048780487807, w1=16.24538138287379\n",
      "SubGD iter. 365/499: loss=6.7237743060375355, w0=72.18536585365855, w1=16.242687111771648\n",
      "SubGD iter. 366/499: loss=6.723743251580859, w0=72.19024390243904, w1=16.239992840669505\n",
      "SubGD iter. 367/499: loss=6.723750348593499, w0=72.18536585365855, w1=16.249733736937575\n",
      "SubGD iter. 368/499: loss=6.72376375277314, w0=72.18048780487807, w1=16.23693695208803\n",
      "SubGD iter. 369/499: loss=6.723751554451696, w0=72.18536585365855, w1=16.234242680985886\n",
      "SubGD iter. 370/499: loss=6.723782564942596, w0=72.18048780487807, w1=16.243983577253957\n",
      "SubGD iter. 371/499: loss=6.723770539970248, w0=72.18536585365855, w1=16.241289306151813\n",
      "SubGD iter. 372/499: loss=6.723739485513571, w0=72.19024390243904, w1=16.23859503504967\n",
      "SubGD iter. 373/499: loss=6.723763964473044, w0=72.18536585365855, w1=16.24833593131774\n",
      "SubGD iter. 374/499: loss=6.7237584710321245, w0=72.19024390243904, w1=16.245641660215597\n",
      "SubGD iter. 375/499: loss=6.723735182707645, w0=72.18536585365855, w1=16.23284487536605\n",
      "SubGD iter. 376/499: loss=6.723796180822144, w0=72.18048780487807, w1=16.242585771634122\n",
      "SubGD iter. 377/499: loss=6.723766773902959, w0=72.18536585365855, w1=16.23989150053198\n",
      "SubGD iter. 378/499: loss=6.723735719446284, w0=72.19024390243904, w1=16.237197229429835\n",
      "SubGD iter. 379/499: loss=6.72377758035259, w0=72.18536585365855, w1=16.246938125697906\n",
      "SubGD iter. 380/499: loss=6.723754704964837, w0=72.19024390243904, w1=16.244243854595762\n",
      "SubGD iter. 381/499: loss=6.72372365050816, w0=72.19512195121952, w1=16.24154958349362\n",
      "SubGD iter. 382/499: loss=6.723772996473706, w0=72.18048780487806, w1=16.24118796601429\n",
      "SubGD iter. 383/499: loss=6.723763007835673, w0=72.18536585365854, w1=16.238493694912147\n",
      "SubGD iter. 384/499: loss=6.7237411562569065, w0=72.18048780487806, w1=16.248234591180218\n",
      "SubGD iter. 385/499: loss=6.723781993354224, w0=72.18536585365854, w1=16.245540320078074\n",
      "SubGD iter. 386/499: loss=6.723750938897548, w0=72.19024390243902, w1=16.24284604897593\n",
      "SubGD iter. 387/499: loss=6.723722555787354, w0=72.18536585365854, w1=16.252586945244\n",
      "SubGD iter. 388/499: loss=6.723800264665965, w0=72.18048780487806, w1=16.239790160394456\n",
      "SubGD iter. 389/499: loss=6.723759241768385, w0=72.18536585365854, w1=16.237095889292313\n",
      "SubGD iter. 390/499: loss=6.723754772136454, w0=72.18048780487806, w1=16.246836785560383\n",
      "SubGD iter. 391/499: loss=6.723778227286937, w0=72.18536585365854, w1=16.24414251445824\n",
      "SubGD iter. 392/499: loss=6.72374717283026, w0=72.19024390243902, w1=16.241448243356096\n",
      "SubGD iter. 393/499: loss=6.7237361716669, w0=72.18536585365854, w1=16.251189139624167\n",
      "SubGD iter. 394/499: loss=6.723782377248189, w0=72.18048780487806, w1=16.23839235477462\n",
      "SubGD iter. 395/499: loss=6.723755475701097, w0=72.18536585365854, w1=16.235698083672478\n",
      "SubGD iter. 396/499: loss=6.723768388015997, w0=72.18048780487806, w1=16.24543897994055\n",
      "SubGD iter. 397/499: loss=6.723774461219649, w0=72.18536585365854, w1=16.242744708838405\n",
      "SubGD iter. 398/499: loss=6.7237434067629716, w0=72.19024390243902, w1=16.24005043773626\n",
      "SubGD iter. 399/499: loss=6.723749787546446, w0=72.18536585365854, w1=16.249791334004332\n",
      "SubGD iter. 400/499: loss=6.723764489830411, w0=72.18048780487806, w1=16.236994549154787\n",
      "SubGD iter. 401/499: loss=6.7237517096338095, w0=72.18536585365854, w1=16.234300278052643\n",
      "SubGD iter. 402/499: loss=6.723782003895544, w0=72.18048780487806, w1=16.244041174320714\n",
      "SubGD iter. 403/499: loss=6.723770695152361, w0=72.18536585365854, w1=16.24134690321857\n",
      "SubGD iter. 404/499: loss=6.723739640695684, w0=72.19024390243902, w1=16.238652632116427\n",
      "SubGD iter. 405/499: loss=6.723763403425991, w0=72.18536585365854, w1=16.248393528384497\n",
      "SubGD iter. 406/499: loss=6.723758626214235, w0=72.19024390243902, w1=16.245699257282354\n",
      "SubGD iter. 407/499: loss=6.723735919764917, w0=72.18536585365854, w1=16.23290247243281\n",
      "SubGD iter. 408/499: loss=6.72379561977509, w0=72.18048780487806, w1=16.24264336870088\n",
      "SubGD iter. 409/499: loss=6.723766929085073, w0=72.18536585365854, w1=16.239949097598736\n",
      "SubGD iter. 410/499: loss=6.723735874628396, w0=72.19024390243902, w1=16.237254826496592\n",
      "SubGD iter. 411/499: loss=6.723777019305538, w0=72.18536585365854, w1=16.246995722764662\n",
      "SubGD iter. 412/499: loss=6.723754860146948, w0=72.19024390243902, w1=16.24430145166252\n",
      "SubGD iter. 413/499: loss=6.7237238056902715, w0=72.1951219512195, w1=16.241607180560376\n",
      "SubGD iter. 414/499: loss=6.723773017301812, w0=72.18048780487804, w1=16.241245563081048\n",
      "SubGD iter. 415/499: loss=6.723763163017785, w0=72.18536585365852, w1=16.238551291978904\n",
      "SubGD iter. 416/499: loss=6.723740595209853, w0=72.18048780487804, w1=16.248292188246975\n",
      "SubGD iter. 417/499: loss=6.723782148536339, w0=72.18536585365852, w1=16.24559791714483\n",
      "SubGD iter. 418/499: loss=6.72375109407966, w0=72.19024390243901, w1=16.242903646042688\n",
      "SubGD iter. 419/499: loss=6.723721994740301, w0=72.18536585365852, w1=16.252644542310758\n",
      "SubGD iter. 420/499: loss=6.723801001723238, w0=72.18048780487804, w1=16.239847757461213\n",
      "SubGD iter. 421/499: loss=6.723759396950497, w0=72.18536585365852, w1=16.23715348635907\n",
      "SubGD iter. 422/499: loss=6.723754211089401, w0=72.18048780487804, w1=16.24689438262714\n",
      "SubGD iter. 423/499: loss=6.723778382469049, w0=72.18536585365852, w1=16.244200111524997\n",
      "SubGD iter. 424/499: loss=6.723747328012373, w0=72.19024390243901, w1=16.241505840422853\n",
      "SubGD iter. 425/499: loss=6.723735610619847, w0=72.18536585365852, w1=16.251246736690923\n",
      "SubGD iter. 426/499: loss=6.723783114305459, w0=72.18048780487804, w1=16.23844995184138\n",
      "SubGD iter. 427/499: loss=6.72375563088321, w0=72.18536585365852, w1=16.235755680739235\n",
      "SubGD iter. 428/499: loss=6.723767826968945, w0=72.18048780487804, w1=16.245496577007305\n",
      "SubGD iter. 429/499: loss=6.723774616401762, w0=72.18536585365852, w1=16.242802305905162\n",
      "SubGD iter. 430/499: loss=6.723743561945085, w0=72.19024390243901, w1=16.24010803480302\n",
      "SubGD iter. 431/499: loss=6.723749226499393, w0=72.18536585365852, w1=16.24984893107109\n",
      "SubGD iter. 432/499: loss=6.723765226887681, w0=72.18048780487804, w1=16.237052146221544\n",
      "SubGD iter. 433/499: loss=6.723751864815922, w0=72.18536585365852, w1=16.2343578751194\n",
      "SubGD iter. 434/499: loss=6.723781442848491, w0=72.18048780487804, w1=16.24409877138747\n",
      "SubGD iter. 435/499: loss=6.723770850334475, w0=72.18536585365852, w1=16.241404500285327\n",
      "SubGD iter. 436/499: loss=6.723739795877798, w0=72.19024390243901, w1=16.238710229183184\n",
      "SubGD iter. 437/499: loss=6.723762842378939, w0=72.18536585365852, w1=16.248451125451254\n",
      "SubGD iter. 438/499: loss=6.7237587813963495, w0=72.19024390243901, w1=16.24575685434911\n",
      "SubGD iter. 439/499: loss=6.7237366568221875, w0=72.18536585365852, w1=16.232960069499565\n",
      "SubGD iter. 440/499: loss=6.723795058728037, w0=72.18048780487804, w1=16.242700965767636\n",
      "SubGD iter. 441/499: loss=6.723767084267186, w0=72.18536585365852, w1=16.240006694665492\n",
      "SubGD iter. 442/499: loss=6.723736029810509, w0=72.19024390243901, w1=16.23731242356335\n",
      "SubGD iter. 443/499: loss=6.7237764582584845, w0=72.18536585365852, w1=16.24705331983142\n",
      "SubGD iter. 444/499: loss=6.723755015329061, w0=72.19024390243901, w1=16.244359048729276\n",
      "SubGD iter. 445/499: loss=6.723723960872385, w0=72.19512195121949, w1=16.241664777627133\n",
      "SubGD iter. 446/499: loss=6.7237730381299174, w0=72.18048780487803, w1=16.241303160147805\n",
      "SubGD iter. 447/499: loss=6.7237633181998975, w0=72.18536585365851, w1=16.23860888904566\n",
      "SubGD iter. 448/499: loss=6.723740034162801, w0=72.18048780487803, w1=16.24834978531373\n",
      "SubGD iter. 449/499: loss=6.72378230371845, w0=72.18536585365851, w1=16.245655514211588\n",
      "SubGD iter. 450/499: loss=6.723751249261773, w0=72.190243902439, w1=16.242961243109445\n",
      "SubGD iter. 451/499: loss=6.7237214336932505, w0=72.18536585365851, w1=16.252702139377515\n",
      "SubGD iter. 452/499: loss=6.7238017387805105, w0=72.18048780487803, w1=16.23990535452797\n",
      "SubGD iter. 453/499: loss=6.72375955213261, w0=72.18536585365851, w1=16.237211083425827\n",
      "SubGD iter. 454/499: loss=6.723753650042347, w0=72.18048780487803, w1=16.246951979693897\n",
      "SubGD iter. 455/499: loss=6.723778537651162, w0=72.18536585365851, w1=16.244257708591753\n",
      "SubGD iter. 456/499: loss=6.723747483194485, w0=72.190243902439, w1=16.24156343748961\n",
      "SubGD iter. 457/499: loss=6.723735049572795, w0=72.18536585365851, w1=16.25130433375768\n",
      "SubGD iter. 458/499: loss=6.7237838513627315, w0=72.18048780487803, w1=16.238507548908135\n",
      "SubGD iter. 459/499: loss=6.723755786065322, w0=72.18536585365851, w1=16.235813277805992\n",
      "SubGD iter. 460/499: loss=6.723767265921893, w0=72.18048780487803, w1=16.245554174074062\n",
      "SubGD iter. 461/499: loss=6.723774771583874, w0=72.18536585365851, w1=16.24285990297192\n",
      "SubGD iter. 462/499: loss=6.723743717127197, w0=72.190243902439, w1=16.240165631869775\n",
      "SubGD iter. 463/499: loss=6.72374866545234, w0=72.18536585365851, w1=16.249906528137846\n",
      "SubGD iter. 464/499: loss=6.7237659639449525, w0=72.18048780487803, w1=16.2371097432883\n",
      "SubGD iter. 465/499: loss=6.723752019998034, w0=72.18536585365851, w1=16.234415472186157\n",
      "SubGD iter. 466/499: loss=6.723780881801439, w0=72.18048780487803, w1=16.244156368454227\n",
      "SubGD iter. 467/499: loss=6.7237710055165865, w0=72.18536585365851, w1=16.241462097352084\n",
      "SubGD iter. 468/499: loss=6.723739951059909, w0=72.190243902439, w1=16.23876782624994\n",
      "SubGD iter. 469/499: loss=6.723762281331886, w0=72.18536585365851, w1=16.24850872251801\n",
      "SubGD iter. 470/499: loss=6.723758936578461, w0=72.190243902439, w1=16.245814451415868\n",
      "SubGD iter. 471/499: loss=6.72373739387946, w0=72.18536585365851, w1=16.233017666566322\n",
      "SubGD iter. 472/499: loss=6.723794497680984, w0=72.18048780487803, w1=16.242758562834393\n",
      "SubGD iter. 473/499: loss=6.7237672394493, w0=72.18536585365851, w1=16.24006429173225\n",
      "SubGD iter. 474/499: loss=6.723736184992621, w0=72.190243902439, w1=16.237370020630106\n",
      "SubGD iter. 475/499: loss=6.723775897211432, w0=72.18536585365851, w1=16.247110916898176\n",
      "SubGD iter. 476/499: loss=6.723755170511175, w0=72.190243902439, w1=16.244416645796033\n",
      "SubGD iter. 477/499: loss=6.723724116054497, w0=72.19512195121948, w1=16.24172237469389\n",
      "SubGD iter. 478/499: loss=6.723773058958024, w0=72.18048780487801, w1=16.24136075721456\n",
      "SubGD iter. 479/499: loss=6.723763473382012, w0=72.1853658536585, w1=16.238666486112418\n",
      "SubGD iter. 480/499: loss=6.7237394731157485, w0=72.18048780487801, w1=16.24840738238049\n",
      "SubGD iter. 481/499: loss=6.723782458900563, w0=72.1853658536585, w1=16.245713111278345\n",
      "SubGD iter. 482/499: loss=6.723751404443886, w0=72.19024390243898, w1=16.2430188401762\n",
      "SubGD iter. 483/499: loss=6.7237208726461954, w0=72.1853658536585, w1=16.252759736444272\n",
      "SubGD iter. 484/499: loss=6.723802475837781, w0=72.18048780487801, w1=16.239962951594727\n",
      "SubGD iter. 485/499: loss=6.723759707314722, w0=72.1853658536585, w1=16.237268680492583\n",
      "SubGD iter. 486/499: loss=6.723753088995294, w0=72.18048780487801, w1=16.247009576760654\n",
      "SubGD iter. 487/499: loss=6.7237786928332754, w0=72.1853658536585, w1=16.24431530565851\n",
      "SubGD iter. 488/499: loss=6.723747638376598, w0=72.19024390243898, w1=16.241621034556367\n",
      "SubGD iter. 489/499: loss=6.723734488525742, w0=72.1853658536585, w1=16.251361930824437\n",
      "SubGD iter. 490/499: loss=6.723784588420003, w0=72.18048780487801, w1=16.238565145974892\n",
      "SubGD iter. 491/499: loss=6.7237559412474335, w0=72.1853658536585, w1=16.23587087487275\n",
      "SubGD iter. 492/499: loss=6.72376670487484, w0=72.18048780487801, w1=16.24561177114082\n",
      "SubGD iter. 493/499: loss=6.723774926765987, w0=72.1853658536585, w1=16.242917500038676\n",
      "SubGD iter. 494/499: loss=6.72374387230931, w0=72.19024390243898, w1=16.240223228936532\n",
      "SubGD iter. 495/499: loss=6.723748104405287, w0=72.1853658536585, w1=16.249964125204603\n",
      "SubGD iter. 496/499: loss=6.723766701002223, w0=72.18048780487801, w1=16.237167340355057\n",
      "SubGD iter. 497/499: loss=6.723752175180147, w0=72.1853658536585, w1=16.234473069252914\n",
      "SubGD iter. 498/499: loss=6.723780320754386, w0=72.18048780487801, w1=16.244213965520984\n",
      "SubGD iter. 499/499: loss=6.723771160698699, w0=72.1853658536585, w1=16.24151969441884\n",
      "SubGD: execution time=0.011 seconds\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = 500\n",
    "gamma = 1\n",
    "batch_size = 1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start SubSGD.\n",
    "start_time = datetime.datetime.now()\n",
    "subgd_losses, subgd_ws = subgradient_descent(y, tx, w_initial, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SubGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d76a8a7dec1a49c0a91a2ac5fef693d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=501, min=1), Output()), _dom_classes=('widg…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ipywidgets import IntSlider, interact\n",
    "\n",
    "\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        subgd_losses,\n",
    "        subgd_ws,\n",
    "        grid_losses,\n",
    "        grid_w0,\n",
    "        grid_w1,\n",
    "        mean_x,\n",
    "        std_x,\n",
    "        height,\n",
    "        weight,\n",
    "        n_iter,\n",
    "    )\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(subgd_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Subgradient Descent\n",
    "\n",
    "**NB** for the computation of the subgradient you can reuse the `compute_subgradient` method that you implemented above, just making sure that you pass in a minibatch as opposed to the full data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_subgradient_descent(y, tx, initial_w, batch_size, max_iters, gamma):\n",
    "    \"\"\"Compute a stochastic subgradient at w from a data sample batch of size B, where B < N, and their corresponding labels.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(B, )\n",
    "        tx: numpy array of shape=(B,2)\n",
    "        initial_w: numpy array of shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        batch_size: a scalar denoting the number of data points in a mini-batch used for computing the stochastic subgradient\n",
    "        max_iters: a scalar denoting the total number of iterations of SubSGD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "\n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of SubSGD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of SubSGD\n",
    "    \"\"\"\n",
    "\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "\n",
    "    for n_iter in range(max_iters):\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO: implement stochastic subgradient descent.\n",
    "        # ***************************************************\n",
    "        for minibatch_y, minibatch_tx in batch_iter(y, tx, batch_size=batch_size, num_batches=1):\n",
    "            loss = compute_loss(minibatch_y, minibatch_tx, w)\n",
    "            gradient = compute_subgradient_mae(minibatch_y, minibatch_tx, w)\n",
    "            w = w - gamma * gradient\n",
    "\n",
    "        # store w and loss\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        \n",
    "\n",
    "        print(\n",
    "            \"SubSGD iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}\".format(\n",
    "                bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]\n",
    "            )\n",
    "        )\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SubSGD iter. 0/499: loss=54.42553892739938, w0=0.7, w1=-0.4411171141672126\n",
      "SubSGD iter. 1/499: loss=93.00613363600779, w0=1.4, w1=0.2904685239786266\n",
      "SubSGD iter. 2/499: loss=84.05622606628796, w0=2.0999999999999996, w1=0.912702049176964\n",
      "SubSGD iter. 3/499: loss=82.205907482835, w0=2.8, w1=1.8526075687234762\n",
      "SubSGD iter. 4/499: loss=86.1461102589445, w0=3.5, w1=2.479061544799211\n",
      "SubSGD iter. 5/499: loss=85.19399729699695, w0=4.2, w1=3.633558290742007\n",
      "SubSGD iter. 6/499: loss=57.12619346203136, w0=4.9, w1=3.0538472315398977\n",
      "SubSGD iter. 7/499: loss=66.02858156589518, w0=5.6000000000000005, w1=2.5675818080129282\n",
      "SubSGD iter. 8/499: loss=55.48300877779523, w0=6.300000000000001, w1=2.063499693536005\n",
      "SubSGD iter. 9/499: loss=76.46070597167187, w0=7.000000000000001, w1=3.003405213082517\n",
      "SubSGD iter. 10/499: loss=46.79753746555228, w0=7.700000000000001, w1=2.1641707521906275\n",
      "SubSGD iter. 11/499: loss=60.13666528744937, w0=8.4, w1=1.8441338691314506\n",
      "SubSGD iter. 12/499: loss=54.515684321359956, w0=9.1, w1=1.520598524351593\n",
      "SubSGD iter. 13/499: loss=60.57570811706512, w0=9.799999999999999, w1=1.7954625842768657\n",
      "SubSGD iter. 14/499: loss=51.358785228857656, w0=10.499999999999998, w1=1.1425473691828318\n",
      "SubSGD iter. 15/499: loss=73.14515429964739, w0=11.199999999999998, w1=1.8440592217101461\n",
      "SubSGD iter. 16/499: loss=46.78705679416163, w0=11.899999999999997, w1=1.0963745446538393\n",
      "SubSGD iter. 17/499: loss=72.15928635099557, w0=12.599999999999996, w1=2.0362800642003513\n",
      "SubSGD iter. 18/499: loss=75.4164787700878, w0=13.299999999999995, w1=2.6846944708124045\n",
      "SubSGD iter. 19/499: loss=75.73032201649075, w0=13.999999999999995, w1=3.2430333517931547\n",
      "SubSGD iter. 20/499: loss=75.10363069482499, w0=14.699999999999994, w1=4.3156788273630395\n",
      "SubSGD iter. 21/499: loss=139.14778368408147, w0=15.399999999999993, w1=3.405757373020732\n",
      "SubSGD iter. 22/499: loss=56.69744836606437, w0=16.099999999999994, w1=3.468969563633596\n",
      "SubSGD iter. 23/499: loss=64.72155267646856, w0=16.799999999999994, w1=4.0770520210630625\n",
      "SubSGD iter. 24/499: loss=60.085402123594974, w0=17.499999999999993, w1=4.295437635731358\n",
      "SubSGD iter. 25/499: loss=52.07588157347947, w0=18.199999999999992, w1=3.6521075803824656\n",
      "SubSGD iter. 26/499: loss=67.30420794747873, w0=18.89999999999999, w1=4.5269024901388875\n",
      "SubSGD iter. 27/499: loss=67.46107078673404, w0=19.59999999999999, w1=4.785872721402191\n",
      "SubSGD iter. 28/499: loss=48.74631127388531, w0=20.29999999999999, w1=4.271041966909654\n",
      "SubSGD iter. 29/499: loss=48.29571090104177, w0=20.99999999999999, w1=4.545906026834927\n",
      "SubSGD iter. 30/499: loss=43.86198066475431, w0=21.69999999999999, w1=3.9106662163883312\n",
      "SubSGD iter. 31/499: loss=35.18991292146286, w0=22.399999999999988, w1=3.4695491022211185\n",
      "SubSGD iter. 32/499: loss=64.24086894708807, w0=23.099999999999987, w1=4.306992407760272\n",
      "SubSGD iter. 33/499: loss=66.44466446395909, w0=23.799999999999986, w1=5.1590040981352905\n",
      "SubSGD iter. 34/499: loss=62.96434744061746, w0=24.499999999999986, w1=6.339094604154656\n",
      "SubSGD iter. 35/499: loss=64.59728829376292, w0=25.199999999999985, w1=7.00398199919416\n",
      "SubSGD iter. 36/499: loss=46.57251742186784, w0=25.899999999999984, w1=7.067194189807024\n",
      "SubSGD iter. 37/499: loss=46.93840042400046, w0=26.599999999999984, w1=7.071378366623613\n",
      "SubSGD iter. 38/499: loss=66.92041108419353, w0=27.299999999999983, w1=8.58614452380431\n",
      "SubSGD iter. 39/499: loss=40.67764846440317, w0=27.999999999999982, w1=8.41377573130195\n",
      "SubSGD iter. 40/499: loss=54.374068043206066, w0=28.69999999999998, w1=9.067917580901472\n",
      "SubSGD iter. 41/499: loss=41.67281733010624, w0=29.39999999999998, w1=8.735470552793977\n",
      "SubSGD iter. 42/499: loss=54.065072950672366, w0=30.09999999999998, w1=10.07839304719298\n",
      "SubSGD iter. 43/499: loss=47.49880073445078, w0=30.79999999999998, w1=10.977848349290483\n",
      "SubSGD iter. 44/499: loss=32.44292669112562, w0=31.49999999999998, w1=10.184565240754175\n",
      "SubSGD iter. 45/499: loss=46.60638182330005, w0=32.19999999999998, w1=9.819615266242472\n",
      "SubSGD iter. 46/499: loss=46.09464513667115, w0=32.899999999999984, w1=9.471462663929122\n",
      "SubSGD iter. 47/499: loss=27.49414503776356, w0=33.59999999999999, w1=9.03034554976191\n",
      "SubSGD iter. 48/499: loss=36.59043206038944, w0=34.29999999999999, w1=8.854018701678228\n",
      "SubSGD iter. 49/499: loss=30.857137282906514, w0=34.99999999999999, w1=8.730392018551335\n",
      "SubSGD iter. 50/499: loss=27.963754833049506, w0=35.699999999999996, w1=8.542024757104922\n",
      "SubSGD iter. 51/499: loss=33.503364184814714, w0=36.4, w1=9.095700582879028\n",
      "SubSGD iter. 52/499: loss=46.02027701285023, w0=37.1, w1=10.176825402109376\n",
      "SubSGD iter. 53/499: loss=32.08865727335792, w0=37.800000000000004, w1=10.609497754839557\n",
      "SubSGD iter. 54/499: loss=29.16697721690779, w0=38.50000000000001, w1=10.2859624100597\n",
      "SubSGD iter. 55/499: loss=26.334640056215374, w0=39.20000000000001, w1=9.75576742465771\n",
      "SubSGD iter. 56/499: loss=38.67642142939777, w0=39.90000000000001, w1=10.630562334414133\n",
      "SubSGD iter. 57/499: loss=38.2944018033684, w0=40.600000000000016, w1=11.089051797791514\n",
      "SubSGD iter. 58/499: loss=33.96299045377492, w0=41.30000000000002, w1=11.271936266257393\n",
      "SubSGD iter. 59/499: loss=24.28416460945389, w0=42.00000000000002, w1=11.1483095831305\n",
      "SubSGD iter. 60/499: loss=31.658291295559167, w0=42.700000000000024, w1=11.04740230360667\n",
      "SubSGD iter. 61/499: loss=39.39610006823858, w0=43.40000000000003, w1=12.050813440678144\n",
      "SubSGD iter. 62/499: loss=23.980832694723176, w0=44.10000000000003, w1=11.68976046200733\n",
      "SubSGD iter. 63/499: loss=31.304145643783123, w0=44.80000000000003, w1=12.223724731204483\n",
      "SubSGD iter. 64/499: loss=16.348764818283037, w0=45.500000000000036, w1=12.300488820752278\n",
      "SubSGD iter. 65/499: loss=25.295116003870945, w0=46.20000000000004, w1=11.575624386263403\n",
      "SubSGD iter. 66/499: loss=29.402133420115675, w0=46.90000000000004, w1=12.450419296019824\n",
      "SubSGD iter. 67/499: loss=22.189467428832472, w0=47.600000000000044, w1=11.697441628175339\n",
      "SubSGD iter. 68/499: loss=32.53563105844527, w0=48.30000000000005, w1=12.323895604251073\n",
      "SubSGD iter. 69/499: loss=20.482597659375607, w0=49.00000000000005, w1=12.16760406571379\n",
      "SubSGD iter. 70/499: loss=33.3019194941789, w0=49.70000000000005, w1=12.459345899410295\n",
      "SubSGD iter. 71/499: loss=28.661527272064866, w0=50.400000000000055, w1=13.107760306022348\n",
      "SubSGD iter. 72/499: loss=26.682869258274145, w0=51.10000000000006, w1=13.32041155475257\n",
      "SubSGD iter. 73/499: loss=22.98981430195908, w0=51.80000000000006, w1=13.675716745232352\n",
      "SubSGD iter. 74/499: loss=21.75801151433722, w0=52.500000000000064, w1=14.29795027043069\n",
      "SubSGD iter. 75/499: loss=25.24229207780023, w0=53.20000000000007, w1=14.345700484513882\n",
      "SubSGD iter. 76/499: loss=17.808385087400218, w0=53.90000000000007, w1=14.624511847111833\n",
      "SubSGD iter. 77/499: loss=18.893227347804242, w0=54.60000000000007, w1=14.628696023928422\n",
      "SubSGD iter. 78/499: loss=19.13670423693293, w0=55.300000000000075, w1=15.245871490863856\n",
      "SubSGD iter. 79/499: loss=15.291084869588474, w0=56.00000000000008, w1=15.853953948293322\n",
      "SubSGD iter. 80/499: loss=15.982397801426686, w0=56.70000000000008, w1=16.93507876752367\n",
      "SubSGD iter. 81/499: loss=17.97135138996036, w0=57.400000000000084, w1=17.09618059124954\n",
      "SubSGD iter. 82/499: loss=20.574495611053386, w0=58.10000000000009, w1=17.154508919076758\n",
      "SubSGD iter. 83/499: loss=17.151914626876604, w0=58.80000000000009, w1=17.78096289515249\n",
      "SubSGD iter. 84/499: loss=19.467495573664685, w0=59.50000000000009, w1=17.24781199497476\n",
      "SubSGD iter. 85/499: loss=19.235761594899216, w0=60.200000000000095, w1=17.912699390014264\n",
      "SubSGD iter. 86/499: loss=0.598239779893305, w0=59.50000000000009, w1=18.006703660022062\n",
      "SubSGD iter. 87/499: loss=10.177545349621589, w0=60.200000000000095, w1=17.423344151565487\n",
      "SubSGD iter. 88/499: loss=19.911463422638008, w0=60.9000000000001, w1=17.71508598526199\n",
      "SubSGD iter. 89/499: loss=10.016757453528086, w0=61.6000000000001, w1=18.58934027739935\n",
      "SubSGD iter. 90/499: loss=6.620708057155866, w0=62.300000000000104, w1=18.081240156655223\n",
      "SubSGD iter. 91/499: loss=15.898612878758826, w0=63.00000000000011, w1=18.360728571575134\n",
      "SubSGD iter. 92/499: loss=14.711431725985264, w0=63.70000000000011, w1=19.52731459221417\n",
      "SubSGD iter. 93/499: loss=22.51924133715557, w0=64.4000000000001, w1=18.5542380029114\n",
      "SubSGD iter. 94/499: loss=13.514701842074977, w0=65.10000000000011, w1=18.25043735104846\n",
      "SubSGD iter. 95/499: loss=11.233488820825215, w0=65.80000000000011, w1=18.432979250849232\n",
      "SubSGD iter. 96/499: loss=6.46789008264917, w0=66.50000000000011, w1=18.096318911264362\n",
      "SubSGD iter. 97/499: loss=3.7730574058899577, w0=67.20000000000012, w1=18.82117427264025\n",
      "SubSGD iter. 98/499: loss=8.984655389462631, w0=67.90000000000012, w1=19.00371617244102\n",
      "SubSGD iter. 99/499: loss=6.282076680660353, w0=68.60000000000012, w1=19.478102257700424\n",
      "SubSGD iter. 100/499: loss=10.777890213375002, w0=69.30000000000013, w1=19.610603749534906\n",
      "SubSGD iter. 101/499: loss=4.3927607748379955, w0=70.00000000000013, w1=19.535609385423893\n",
      "SubSGD iter. 102/499: loss=5.86230925924994, w0=69.30000000000013, w1=18.462963909854007\n",
      "SubSGD iter. 103/499: loss=9.318417113980516, w0=70.00000000000013, w1=17.337660480298005\n",
      "SubSGD iter. 104/499: loss=2.3108931164332063, w0=70.70000000000013, w1=17.253074267352215\n",
      "SubSGD iter. 105/499: loss=9.482427491557502, w0=71.40000000000013, w1=17.544816101048717\n",
      "SubSGD iter. 106/499: loss=2.107378220899733, w0=70.70000000000013, w1=18.12817560950529\n",
      "SubSGD iter. 107/499: loss=3.8916983837402626, w0=71.40000000000013, w1=18.564701201917256\n",
      "SubSGD iter. 108/499: loss=3.691946739679125, w0=70.70000000000013, w1=17.956618744487788\n",
      "SubSGD iter. 109/499: loss=1.5256994282924978, w0=70.00000000000013, w1=17.222793219328732\n",
      "SubSGD iter. 110/499: loss=2.2970128498662916, w0=70.70000000000013, w1=17.13820700638294\n",
      "SubSGD iter. 111/499: loss=3.623056718782223, w0=71.40000000000013, w1=16.390522329326636\n",
      "SubSGD iter. 112/499: loss=6.374934068431486, w0=70.70000000000013, w1=16.57888959077305\n",
      "SubSGD iter. 113/499: loss=4.178576766233213, w0=70.00000000000013, w1=16.702516273899942\n",
      "SubSGD iter. 114/499: loss=2.0573929161107998, w0=70.70000000000013, w1=16.83160601858721\n",
      "SubSGD iter. 115/499: loss=4.840891492445223, w0=71.40000000000013, w1=17.458059994662943\n",
      "SubSGD iter. 116/499: loss=7.982342403838416, w0=70.70000000000013, w1=17.18319593473767\n",
      "SubSGD iter. 117/499: loss=0.08722377659697145, w0=70.00000000000013, w1=17.944201207839395\n",
      "SubSGD iter. 118/499: loss=2.407293747305019, w0=69.30000000000013, w1=18.03879932241691\n",
      "SubSGD iter. 119/499: loss=15.2831687708468, w0=70.00000000000013, w1=18.39483683930343\n",
      "SubSGD iter. 120/499: loss=109.13233252642894, w0=70.70000000000013, w1=16.90065974918704\n",
      "SubSGD iter. 121/499: loss=13.295158727113197, w0=71.40000000000013, w1=16.32280020150397\n",
      "SubSGD iter. 122/499: loss=4.655313296243669, w0=70.70000000000013, w1=16.635143204821663\n",
      "SubSGD iter. 123/499: loss=5.045268975477171, w0=71.40000000000013, w1=17.253042219525717\n",
      "SubSGD iter. 124/499: loss=4.069834433312991, w0=72.10000000000014, w1=17.89833830233059\n",
      "SubSGD iter. 125/499: loss=10.357282452763108, w0=72.80000000000014, w1=17.20399315033643\n",
      "SubSGD iter. 126/499: loss=9.47845887224154, w0=72.10000000000014, w1=17.44491164930878\n",
      "SubSGD iter. 127/499: loss=4.8546223538658, w0=71.40000000000013, w1=17.75725465262647\n",
      "SubSGD iter. 128/499: loss=1.297098466384341, w0=72.10000000000014, w1=18.550096528762605\n",
      "SubSGD iter. 129/499: loss=6.753301700973495, w0=72.80000000000014, w1=18.016945628584875\n",
      "SubSGD iter. 130/499: loss=2.6927774624317635, w0=72.10000000000014, w1=17.752191036501788\n",
      "SubSGD iter. 131/499: loss=110.34530397156146, w0=72.80000000000014, w1=15.673758310611314\n",
      "SubSGD iter. 132/499: loss=1.0180576952607936, w0=72.10000000000014, w1=15.051524785412976\n",
      "SubSGD iter. 133/499: loss=99.89590356066736, w0=72.80000000000014, w1=13.557347695296585\n",
      "SubSGD iter. 134/499: loss=8.435209025363918, w0=73.50000000000014, w1=12.979488147613516\n",
      "SubSGD iter. 135/499: loss=8.413652144489966, w0=72.80000000000014, w1=13.476304642375023\n",
      "SubSGD iter. 136/499: loss=3.7446332636419015, w0=72.10000000000014, w1=12.817195038274049\n",
      "SubSGD iter. 137/499: loss=15.139007375832847, w0=72.80000000000014, w1=13.173232555160569\n",
      "SubSGD iter. 138/499: loss=7.048566615611847, w0=73.50000000000014, w1=14.04756016822635\n",
      "SubSGD iter. 139/499: loss=5.038500900049584, w0=74.20000000000014, w1=13.404230112877457\n",
      "SubSGD iter. 140/499: loss=4.1963413065837045, w0=73.50000000000014, w1=13.037958953092158\n",
      "SubSGD iter. 141/499: loss=7.963261035922507, w0=72.80000000000014, w1=13.62131846154873\n",
      "SubSGD iter. 142/499: loss=10.362293453935976, w0=72.10000000000014, w1=13.875509016892707\n",
      "SubSGD iter. 143/499: loss=0.556059451892267, w0=72.80000000000014, w1=13.543061988785212\n",
      "SubSGD iter. 144/499: loss=0.15995973100842775, w0=72.10000000000014, w1=13.413972244097945\n",
      "SubSGD iter. 145/499: loss=3.2848682881203928, w0=72.80000000000014, w1=13.929648386372804\n",
      "SubSGD iter. 146/499: loss=4.419549270715223, w0=73.50000000000014, w1=14.583790235972327\n",
      "SubSGD iter. 147/499: loss=6.170597895919897, w0=72.80000000000014, w1=15.377073344508634\n",
      "SubSGD iter. 148/499: loss=8.258707268379155, w0=73.50000000000014, w1=15.028920742195284\n",
      "SubSGD iter. 149/499: loss=0.6146001930767113, w0=72.80000000000014, w1=15.370734859866305\n",
      "SubSGD iter. 150/499: loss=1.515428058897058, w0=72.10000000000014, w1=16.108595477167448\n",
      "SubSGD iter. 151/499: loss=8.197955527749173, w0=71.40000000000013, w1=15.168689957620936\n",
      "SubSGD iter. 152/499: loss=7.0038306506103964, w0=70.70000000000013, w1=14.217907658350963\n",
      "SubSGD iter. 153/499: loss=9.673581115879983, w0=71.40000000000013, w1=14.350409150185447\n",
      "SubSGD iter. 154/499: loss=9.148084681299792, w0=72.10000000000014, w1=14.002256547872097\n",
      "SubSGD iter. 155/499: loss=1.3412485786334472, w0=71.40000000000013, w1=14.755234215716582\n",
      "SubSGD iter. 156/499: loss=0.5988393193865136, w0=72.10000000000014, w1=14.670648002770793\n",
      "SubSGD iter. 157/499: loss=2.2424004895171876, w0=71.40000000000013, w1=15.28508367754108\n",
      "SubSGD iter. 158/499: loss=1.708865241471159, w0=70.70000000000013, w1=16.354107868444256\n",
      "SubSGD iter. 159/499: loss=1.5960936167141924, w0=71.40000000000013, w1=15.502467661465996\n",
      "SubSGD iter. 160/499: loss=3.434422609784903, w0=70.70000000000013, w1=15.55859935579634\n",
      "SubSGD iter. 161/499: loss=5.569355532638006, w0=71.40000000000013, w1=15.043768601303803\n",
      "SubSGD iter. 162/499: loss=5.360765514448396, w0=70.70000000000013, w1=15.562492135097154\n",
      "SubSGD iter. 163/499: loss=4.255667879366186, w0=71.40000000000013, w1=16.907951960488646\n",
      "SubSGD iter. 164/499: loss=0.009757504711842557, w0=72.10000000000014, w1=16.403869846011723\n",
      "SubSGD iter. 165/499: loss=0.01520165415485053, w0=72.80000000000014, w1=16.47661260636583\n",
      "SubSGD iter. 166/499: loss=3.332080629689017, w0=73.50000000000014, w1=16.513639018905508\n",
      "SubSGD iter. 167/499: loss=5.951253951064281, w0=74.20000000000014, w1=16.02489668530485\n",
      "SubSGD iter. 168/499: loss=0.23217894758249713, w0=74.90000000000015, w1=16.13288525306129\n",
      "SubSGD iter. 169/499: loss=3.5261779465667757, w0=74.20000000000014, w1=15.510651727862951\n",
      "SubSGD iter. 170/499: loss=3.420950047855982, w0=73.50000000000014, w1=15.4822312981048\n",
      "SubSGD iter. 171/499: loss=8.756922457538892, w0=72.80000000000014, w1=14.542325778558288\n",
      "SubSGD iter. 172/499: loss=3.1675336905334035, w0=73.50000000000014, w1=14.316960740405817\n",
      "SubSGD iter. 173/499: loss=0.4700991331444726, w0=74.20000000000014, w1=14.852519440643517\n",
      "SubSGD iter. 174/499: loss=2.472120752770124, w0=73.50000000000014, w1=13.66149921567823\n",
      "SubSGD iter. 175/499: loss=7.934837746821856, w0=74.20000000000014, w1=14.207966919183106\n",
      "SubSGD iter. 176/499: loss=3.548430255351576, w0=74.90000000000015, w1=14.255717133266298\n",
      "SubSGD iter. 177/499: loss=9.652644523834425, w0=75.60000000000015, w1=15.422303153905336\n",
      "SubSGD iter. 178/499: loss=1.9655838818327211, w0=74.90000000000015, w1=15.985261697993769\n",
      "SubSGD iter. 179/499: loss=6.69269765054613, w0=74.20000000000014, w1=16.45251675174322\n",
      "SubSGD iter. 180/499: loss=3.8487356080952395, w0=74.90000000000015, w1=17.01085563272397\n",
      "SubSGD iter. 181/499: loss=7.157426166683095, w0=74.20000000000014, w1=16.309343780196656\n",
      "SubSGD iter. 182/499: loss=4.950845399383681, w0=73.50000000000014, w1=15.11832355523137\n",
      "SubSGD iter. 183/499: loss=9.163614608270905, w0=72.80000000000014, w1=14.843459495306098\n",
      "SubSGD iter. 184/499: loss=10.82714130452839, w0=72.10000000000014, w1=15.480882042716736\n",
      "SubSGD iter. 185/499: loss=3.330616602640326, w0=71.40000000000013, w1=16.011077028118724\n",
      "SubSGD iter. 186/499: loss=3.422826581794226, w0=72.10000000000014, w1=16.27517001566063\n",
      "SubSGD iter. 187/499: loss=3.87412436458564, w0=72.80000000000014, w1=17.14949762872641\n",
      "SubSGD iter. 188/499: loss=5.617269933471064, w0=72.10000000000014, w1=17.730890028599926\n",
      "SubSGD iter. 189/499: loss=9.029352446675048, w0=72.80000000000014, w1=17.413932979386228\n",
      "SubSGD iter. 190/499: loss=2.297677583152449, w0=73.50000000000014, w1=17.25831501021702\n",
      "SubSGD iter. 191/499: loss=8.700854089282736, w0=72.80000000000014, w1=17.507421805467175\n",
      "SubSGD iter. 192/499: loss=4.956595711706939, w0=72.10000000000014, w1=17.00488714476807\n",
      "SubSGD iter. 193/499: loss=4.373063644768841, w0=71.40000000000013, w1=17.319078247888797\n",
      "SubSGD iter. 194/499: loss=122.37051928823556, w0=72.10000000000014, w1=14.072134250450157\n",
      "SubSGD iter. 195/499: loss=3.848240175565749, w0=72.80000000000014, w1=14.901967100254344\n",
      "SubSGD iter. 196/499: loss=2.8347869095104485, w0=72.10000000000014, w1=15.406049214731269\n",
      "SubSGD iter. 197/499: loss=6.563038484830493, w0=72.80000000000014, w1=15.011177400379326\n",
      "SubSGD iter. 198/499: loss=6.599471945238378, w0=72.10000000000014, w1=14.578505047649145\n",
      "SubSGD iter. 199/499: loss=1.0536478572783068, w0=72.80000000000014, w1=14.503510683538131\n",
      "SubSGD iter. 200/499: loss=0.005729662210583797, w0=72.10000000000014, w1=13.769685158379076\n",
      "SubSGD iter. 201/499: loss=8.358424187194345, w0=72.80000000000014, w1=13.90218665021356\n",
      "SubSGD iter. 202/499: loss=1.2108483433233062, w0=73.50000000000014, w1=12.612168981544436\n",
      "SubSGD iter. 203/499: loss=3.489218067369549, w0=74.20000000000014, w1=13.693293800774784\n",
      "SubSGD iter. 204/499: loss=0.5402190206623061, w0=74.90000000000015, w1=14.082352278057135\n",
      "SubSGD iter. 205/499: loss=4.055091012749131, w0=74.20000000000014, w1=14.83532994590162\n",
      "SubSGD iter. 206/499: loss=3.6630605009980357, w0=74.90000000000015, w1=13.851997650101326\n",
      "SubSGD iter. 207/499: loss=2.509819475536233, w0=75.60000000000015, w1=14.486967011710808\n",
      "SubSGD iter. 208/499: loss=5.90362928049494, w0=74.90000000000015, w1=15.101402686481094\n",
      "SubSGD iter. 209/499: loss=1.2438266826818563, w0=75.60000000000015, w1=16.61616884366179\n",
      "SubSGD iter. 210/499: loss=6.106582274712693, w0=74.90000000000015, w1=16.019923914745462\n",
      "SubSGD iter. 211/499: loss=8.655449322621678, w0=74.20000000000014, w1=16.601316314618977\n",
      "SubSGD iter. 212/499: loss=6.000296092546876, w0=74.90000000000015, w1=16.38995762488821\n",
      "SubSGD iter. 213/499: loss=6.686310611351146, w0=75.60000000000015, w1=16.83850800128068\n",
      "SubSGD iter. 214/499: loss=6.574929147109572, w0=74.90000000000015, w1=15.963713091524259\n",
      "SubSGD iter. 215/499: loss=8.585122643090187, w0=74.20000000000014, w1=16.471813212268387\n",
      "SubSGD iter. 216/499: loss=7.333858676309433, w0=74.90000000000015, w1=16.920363588660855\n",
      "SubSGD iter. 217/499: loss=4.870123997512039, w0=74.20000000000014, w1=17.092732381163216\n",
      "SubSGD iter. 218/499: loss=1.4215261823912755, w0=73.50000000000014, w1=17.088548204346626\n",
      "SubSGD iter. 219/499: loss=0.06354718529888714, w0=72.80000000000014, w1=16.085137067275152\n",
      "SubSGD iter. 220/499: loss=6.246113702599587, w0=73.50000000000014, w1=15.69026525292321\n",
      "SubSGD iter. 221/499: loss=5.93610906249566, w0=72.80000000000014, w1=16.142658929370583\n",
      "SubSGD iter. 222/499: loss=8.714199639492463, w0=73.50000000000014, w1=15.16958234006781\n",
      "SubSGD iter. 223/499: loss=12.542524327709941, w0=74.20000000000014, w1=15.52561985695433\n",
      "SubSGD iter. 224/499: loss=7.2274824009360685, w0=73.50000000000014, w1=15.679685285400687\n",
      "SubSGD iter. 225/499: loss=1.9559795910973605, w0=74.20000000000014, w1=16.68309642247216\n",
      "SubSGD iter. 226/499: loss=5.625510175359423, w0=73.50000000000014, w1=17.266455930928736\n",
      "SubSGD iter. 227/499: loss=103.22375980897436, w0=74.20000000000014, w1=15.772278840812344\n",
      "SubSGD iter. 228/499: loss=1.3359197945845978, w0=73.50000000000014, w1=16.51996351786865\n",
      "SubSGD iter. 229/499: loss=0.4536014484570927, w0=72.80000000000014, w1=15.690130668064462\n",
      "SubSGD iter. 230/499: loss=2.889568934052761, w0=73.50000000000014, w1=16.54214235843948\n",
      "SubSGD iter. 231/499: loss=3.4404766507937126, w0=74.20000000000014, w1=17.07235085510008\n",
      "SubSGD iter. 232/499: loss=5.195558073624412, w0=73.50000000000014, w1=16.46426839767061\n",
      "SubSGD iter. 233/499: loss=2.068022835771451, w0=72.80000000000014, w1=17.0787040724409\n",
      "SubSGD iter. 234/499: loss=6.190304756879804, w0=72.10000000000014, w1=17.202330755567793\n",
      "SubSGD iter. 235/499: loss=1.68467313613111, w0=71.40000000000013, w1=16.937576163484707\n",
      "SubSGD iter. 236/499: loss=3.191000458878001, w0=72.10000000000014, w1=16.61753928042553\n",
      "SubSGD iter. 237/499: loss=1.018598318515103, w0=72.80000000000014, w1=16.800423748891408\n",
      "SubSGD iter. 238/499: loss=3.5593379265029057, w0=72.10000000000014, w1=16.19234129146194\n",
      "SubSGD iter. 239/499: loss=2.4901630209102166, w0=71.40000000000013, w1=17.03157575235383\n",
      "SubSGD iter. 240/499: loss=3.1390539992177224, w0=70.70000000000013, w1=17.544336716073207\n",
      "SubSGD iter. 241/499: loss=0.5277863340701643, w0=70.00000000000013, w1=17.867872060853063\n",
      "SubSGD iter. 242/499: loss=4.267390303119072, w0=70.70000000000013, w1=18.487443824049567\n",
      "SubSGD iter. 243/499: loss=2.9005389118778737, w0=70.00000000000013, w1=18.838657202236774\n",
      "SubSGD iter. 244/499: loss=2.5383012177572652, w0=70.70000000000013, w1=19.021541670702653\n",
      "SubSGD iter. 245/499: loss=4.6130989468167485, w0=71.40000000000013, w1=17.987540594727417\n",
      "SubSGD iter. 246/499: loss=1.3528597844951662, w0=70.70000000000013, w1=18.1438321332647\n",
      "SubSGD iter. 247/499: loss=9.165532943640912, w0=71.40000000000013, w1=16.99195351719208\n",
      "SubSGD iter. 248/499: loss=2.0122121905469754, w0=72.10000000000014, w1=16.916959153081066\n",
      "SubSGD iter. 249/499: loss=5.90730409893996, w0=71.40000000000013, w1=16.721915667184227\n",
      "SubSGD iter. 250/499: loss=2.6078872114277374, w0=72.10000000000014, w1=16.389468639076732\n",
      "SubSGD iter. 251/499: loss=5.353163129116794, w0=71.40000000000013, w1=16.74068201726394\n",
      "SubSGD iter. 252/499: loss=13.843423716495494, w0=72.10000000000014, w1=17.09671953415046\n",
      "SubSGD iter. 253/499: loss=4.664903164788107, w0=71.40000000000013, w1=17.348364406047676\n",
      "SubSGD iter. 254/499: loss=1.8716336560849953, w0=72.10000000000014, w1=17.33693396812123\n",
      "SubSGD iter. 255/499: loss=2.988656202148391, w0=72.80000000000014, w1=17.963387944196963\n",
      "SubSGD iter. 256/499: loss=0.3935659285886075, w0=73.50000000000014, w1=17.383676884994856\n",
      "SubSGD iter. 257/499: loss=4.770601586939669, w0=74.20000000000014, w1=16.83957203492399\n",
      "SubSGD iter. 258/499: loss=0.9775412462046091, w0=74.90000000000015, w1=17.459143798120493\n",
      "SubSGD iter. 259/499: loss=3.3339344015678165, w0=74.20000000000014, w1=16.923585097882793\n",
      "SubSGD iter. 260/499: loss=7.187240497839589, w0=73.50000000000014, w1=17.235928101200486\n",
      "SubSGD iter. 261/499: loss=1.3906867634408613, w0=72.80000000000014, w1=17.850363775970774\n",
      "SubSGD iter. 262/499: loss=5.187685379471915, w0=72.10000000000014, w1=16.950908473873273\n",
      "SubSGD iter. 263/499: loss=3.3341236389172906, w0=72.80000000000014, w1=17.577362449949007\n",
      "SubSGD iter. 264/499: loss=5.256868397091296, w0=72.10000000000014, w1=17.671960564526522\n",
      "SubSGD iter. 265/499: loss=1.3875162758564414, w0=71.40000000000013, w1=16.947105203150635\n",
      "SubSGD iter. 266/499: loss=4.715888399144539, w0=72.10000000000014, w1=16.31186539270404\n",
      "SubSGD iter. 267/499: loss=9.042311429540476, w0=71.40000000000013, w1=15.758189566929934\n",
      "SubSGD iter. 268/499: loss=9.712346535690486, w0=72.10000000000014, w1=15.98503357857711\n",
      "SubSGD iter. 269/499: loss=2.648485874364617, w0=71.40000000000013, w1=16.308568923356965\n",
      "SubSGD iter. 270/499: loss=2.458926667403034, w0=70.70000000000013, w1=16.880281068877043\n",
      "SubSGD iter. 271/499: loss=1.7271332426784198, w0=70.00000000000013, w1=16.005486159120622\n",
      "SubSGD iter. 272/499: loss=0.017135444904937458, w0=69.30000000000013, w1=15.10603085702312\n",
      "SubSGD iter. 273/499: loss=4.958787721218101, w0=68.60000000000012, w1=14.831166797097847\n",
      "SubSGD iter. 274/499: loss=3.107189655870698, w0=67.90000000000012, w1=15.339266917841975\n",
      "SubSGD iter. 275/499: loss=1.9022651787050364, w0=67.20000000000012, w1=14.906594565111794\n",
      "SubSGD iter. 276/499: loss=1.4790176397015031, w0=67.90000000000012, w1=14.41807669314584\n",
      "SubSGD iter. 277/499: loss=3.64757727586813, w0=68.60000000000012, w1=14.24174984506216\n",
      "SubSGD iter. 278/499: loss=3.5350222622963656, w0=67.90000000000012, w1=14.749849965806288\n",
      "SubSGD iter. 279/499: loss=9.346351494711953, w0=68.60000000000012, w1=14.932391865607059\n",
      "SubSGD iter. 280/499: loss=9.559767206227065, w0=69.30000000000013, w1=15.462600362267658\n",
      "SubSGD iter. 281/499: loss=6.666509620414885, w0=70.00000000000013, w1=16.314612052642676\n",
      "SubSGD iter. 282/499: loss=4.605129284702414, w0=69.30000000000013, w1=15.881939699912495\n",
      "SubSGD iter. 283/499: loss=0.3132177833597609, w0=70.00000000000013, w1=15.110714935566662\n",
      "SubSGD iter. 284/499: loss=5.90554744308065, w0=70.70000000000013, w1=15.759129342178715\n",
      "SubSGD iter. 285/499: loss=0.873422523023109, w0=71.40000000000013, w1=14.866906083365699\n",
      "SubSGD iter. 286/499: loss=2.2442993184705955, w0=72.10000000000014, w1=14.54686920030652\n",
      "SubSGD iter. 287/499: loss=1.021289208831547, w0=72.80000000000014, w1=13.822004765817645\n",
      "SubSGD iter. 288/499: loss=8.759440968225107, w0=72.10000000000014, w1=13.213078120984532\n",
      "SubSGD iter. 289/499: loss=6.703599018803729, w0=72.80000000000014, w1=13.383448236530617\n",
      "SubSGD iter. 290/499: loss=1.7481104007371897, w0=73.50000000000014, w1=14.72637073092962\n",
      "SubSGD iter. 291/499: loss=9.794383702623357, w0=72.80000000000014, w1=15.167487845096833\n",
      "SubSGD iter. 292/499: loss=7.911734887542181, w0=73.50000000000014, w1=15.832375240136336\n",
      "SubSGD iter. 293/499: loss=4.06795889312891, w0=74.20000000000014, w1=14.542357571467212\n",
      "SubSGD iter. 294/499: loss=1.4464537164039513, w0=73.50000000000014, w1=13.808532046308157\n",
      "SubSGD iter. 295/499: loss=1.59420389283396, w0=74.20000000000014, w1=14.324208188583016\n",
      "SubSGD iter. 296/499: loss=3.1830544892943635, w0=73.50000000000014, w1=14.045396825985065\n",
      "SubSGD iter. 297/499: loss=2.4577366125640907, w0=74.20000000000014, w1=14.503886289362447\n",
      "SubSGD iter. 298/499: loss=1.213507751646489, w0=74.90000000000015, w1=14.787885925887624\n",
      "SubSGD iter. 299/499: loss=98.7437200409545, w0=75.60000000000015, w1=12.70945319999715\n",
      "SubSGD iter. 300/499: loss=2.404116039039394, w0=74.90000000000015, w1=12.720883637923595\n",
      "SubSGD iter. 301/499: loss=5.5174822685634695, w0=75.60000000000015, w1=14.066343463315086\n",
      "SubSGD iter. 302/499: loss=7.749582730514945, w0=74.90000000000015, w1=14.122475157645429\n",
      "SubSGD iter. 303/499: loss=1.920960170496656, w0=75.60000000000015, w1=14.770889564257482\n",
      "SubSGD iter. 304/499: loss=3.015493119782022, w0=74.90000000000015, w1=14.148656039059144\n",
      "SubSGD iter. 305/499: loss=5.486689605691566, w0=75.60000000000015, w1=14.281157530893628\n",
      "SubSGD iter. 306/499: loss=2.8833234225806663, w0=74.90000000000015, w1=14.844116074982061\n",
      "SubSGD iter. 307/499: loss=3.3825235682733705, w0=74.20000000000014, w1=13.96932116522564\n",
      "SubSGD iter. 308/499: loss=7.081188986150188, w0=73.50000000000014, w1=14.730326438327364\n",
      "SubSGD iter. 309/499: loss=4.737101377864917, w0=72.80000000000014, w1=15.091379416998178\n",
      "SubSGD iter. 310/499: loss=1.757799874041197, w0=72.10000000000014, w1=15.028167226385314\n",
      "SubSGD iter. 311/499: loss=1.9829739329074982, w0=71.40000000000013, w1=15.345510974495005\n",
      "SubSGD iter. 312/499: loss=7.179742618313028, w0=72.10000000000014, w1=14.362178678694711\n",
      "SubSGD iter. 313/499: loss=4.177946479435768, w0=71.40000000000013, w1=14.8923736640967\n",
      "SubSGD iter. 314/499: loss=8.121287253292529, w0=70.70000000000013, w1=15.288661419496474\n",
      "SubSGD iter. 315/499: loss=101.80208088334575, w0=71.40000000000013, w1=13.794484329380083\n",
      "SubSGD iter. 316/499: loss=5.724780139473623, w0=72.10000000000014, w1=14.442898735992136\n",
      "SubSGD iter. 317/499: loss=0.4992418140859627, w0=72.80000000000014, w1=14.101084618321115\n",
      "SubSGD iter. 318/499: loss=2.5111635651795297, w0=72.10000000000014, w1=13.59854995762201\n",
      "SubSGD iter. 319/499: loss=5.745127596123723, w0=72.80000000000014, w1=14.072936042881413\n",
      "SubSGD iter. 320/499: loss=5.6848387683914865, w0=72.10000000000014, w1=14.644648188401492\n",
      "SubSGD iter. 321/499: loss=0.440365737708305, w0=71.40000000000013, w1=15.3923328654578\n",
      "SubSGD iter. 322/499: loss=1.746388012747758, w0=70.70000000000013, w1=15.564701657960159\n",
      "SubSGD iter. 323/499: loss=4.294782632072653, w0=70.00000000000013, w1=15.940910828453479\n",
      "SubSGD iter. 324/499: loss=7.8026671959927825, w0=69.30000000000013, w1=15.331984183620365\n",
      "SubSGD iter. 325/499: loss=10.053086224436925, w0=70.00000000000013, w1=16.335627923809714\n",
      "SubSGD iter. 326/499: loss=2.7829993303490284, w0=70.70000000000013, w1=16.339812100626304\n",
      "SubSGD iter. 327/499: loss=2.7847459499041065, w0=70.00000000000013, w1=15.159721594606939\n",
      "SubSGD iter. 328/499: loss=0.9299395847713825, w0=69.30000000000013, w1=15.483256939386797\n",
      "SubSGD iter. 329/499: loss=1.3538957559519673, w0=70.00000000000013, w1=16.3827122414843\n",
      "SubSGD iter. 330/499: loss=0.2908254088230393, w0=69.30000000000013, w1=16.871230113450252\n",
      "SubSGD iter. 331/499: loss=8.471414295933663, w0=70.00000000000013, w1=15.719351497377636\n",
      "SubSGD iter. 332/499: loss=5.399120278435802, w0=70.70000000000013, w1=16.722762634449108\n",
      "SubSGD iter. 333/499: loss=0.9465826482955748, w0=70.00000000000013, w1=17.516045742985416\n",
      "SubSGD iter. 334/499: loss=9.749283663751143, w0=70.70000000000013, w1=17.648547234819898\n",
      "SubSGD iter. 335/499: loss=0.11127986948848445, w0=71.40000000000013, w1=17.711759425432763\n",
      "SubSGD iter. 336/499: loss=0.03147706270745232, w0=72.10000000000014, w1=16.872524964540872\n",
      "SubSGD iter. 337/499: loss=5.8129413308445095, w0=72.80000000000014, w1=17.042895080086957\n",
      "SubSGD iter. 338/499: loss=0.24717361584355046, w0=73.50000000000014, w1=16.305034462785812\n",
      "SubSGD iter. 339/499: loss=0.3099001864618316, w0=74.20000000000014, w1=15.972587434678317\n",
      "SubSGD iter. 340/499: loss=3.931973417911564, w0=74.90000000000015, w1=16.142957550224402\n",
      "SubSGD iter. 341/499: loss=11.743813646640973, w0=74.20000000000014, w1=16.780380097635042\n",
      "SubSGD iter. 342/499: loss=9.69586623419702, w0=74.90000000000015, w1=16.202520549951974\n",
      "SubSGD iter. 343/499: loss=10.617146105789416, w0=75.60000000000015, w1=16.558558066838494\n",
      "SubSGD iter. 344/499: loss=4.683783164353002, w0=76.30000000000015, w1=15.5752257710382\n",
      "SubSGD iter. 345/499: loss=9.96169596961488, w0=75.60000000000015, w1=15.926439149225406\n",
      "SubSGD iter. 346/499: loss=4.450176871041307, w0=76.30000000000015, w1=16.058940641059888\n",
      "SubSGD iter. 347/499: loss=11.364161403523767, w0=75.60000000000015, w1=16.2473079025063\n",
      "SubSGD iter. 348/499: loss=1.14472479182065, w0=76.30000000000015, w1=15.095429286433683\n",
      "SubSGD iter. 349/499: loss=5.036257973405185, w0=75.60000000000015, w1=13.75250679203468\n",
      "SubSGD iter. 350/499: loss=8.332884206023593, w0=74.90000000000015, w1=14.523731556380513\n",
      "SubSGD iter. 351/499: loss=4.901221246084418, w0=74.20000000000014, w1=15.41595481519353\n",
      "SubSGD iter. 352/499: loss=7.589073929785883, w0=73.50000000000014, w1=15.22091132929669\n",
      "SubSGD iter. 353/499: loss=4.072159651126626, w0=72.80000000000014, w1=14.87382908802311\n",
      "SubSGD iter. 354/499: loss=3.6136714436499346, w0=72.10000000000014, w1=15.907830163998344\n",
      "SubSGD iter. 355/499: loss=1.9916537269130288, w0=71.40000000000013, w1=15.008374861900842\n",
      "SubSGD iter. 356/499: loss=2.81030344357535, w0=72.10000000000014, w1=15.524051004175702\n",
      "SubSGD iter. 357/499: loss=0.11949477651526763, w0=72.80000000000014, w1=16.361494309714857\n",
      "SubSGD iter. 358/499: loss=0.38549353047478974, w0=73.50000000000014, w1=16.544378778180736\n",
      "SubSGD iter. 359/499: loss=5.903909993611222, w0=74.20000000000014, w1=17.20926617322024\n",
      "SubSGD iter. 360/499: loss=10.87664405972982, w0=73.50000000000014, w1=17.450184672192588\n",
      "SubSGD iter. 361/499: loss=2.4369669538933607, w0=72.80000000000014, w1=16.99316902610437\n",
      "SubSGD iter. 362/499: loss=7.048257870940063, w0=73.50000000000014, w1=17.12567051793885\n",
      "SubSGD iter. 363/499: loss=4.789291400750564, w0=72.80000000000014, w1=17.513668851361516\n",
      "SubSGD iter. 364/499: loss=0.90684898170786, w0=73.50000000000014, w1=16.662028644383255\n",
      "SubSGD iter. 365/499: loss=0.2395736558013084, w0=72.80000000000014, w1=16.6734590823097\n",
      "SubSGD iter. 366/499: loss=2.941651514491035, w0=73.50000000000014, w1=15.5481556527537\n",
      "SubSGD iter. 367/499: loss=1.309342761461906, w0=72.80000000000014, w1=14.710712347214546\n",
      "SubSGD iter. 368/499: loss=7.480299178083612, w0=73.50000000000014, w1=14.84321383904903\n",
      "SubSGD iter. 369/499: loss=6.458814460335894, w0=72.80000000000014, w1=15.426573347505602\n",
      "SubSGD iter. 370/499: loss=6.192032609153387, w0=72.10000000000014, w1=15.231529861608763\n",
      "SubSGD iter. 371/499: loss=4.058447064592542, w0=72.80000000000014, w1=15.866499223218245\n",
      "SubSGD iter. 372/499: loss=3.203793756640124, w0=72.10000000000014, w1=16.637723987564076\n",
      "SubSGD iter. 373/499: loss=2.139721492875452, w0=71.40000000000013, w1=16.810092780066437\n",
      "SubSGD iter. 374/499: loss=1.3925639356404602, w0=70.70000000000013, w1=17.29861065203239\n",
      "SubSGD iter. 375/499: loss=2.0680810509079492, w0=71.40000000000013, w1=16.264609576057154\n",
      "SubSGD iter. 376/499: loss=0.14326836064117288, w0=70.70000000000013, w1=15.642376050858816\n",
      "SubSGD iter. 377/499: loss=1.4068599048956898, w0=70.00000000000013, w1=15.965911395638674\n",
      "SubSGD iter. 378/499: loss=10.33017756177702, w0=70.70000000000013, w1=15.648954346424974\n",
      "SubSGD iter. 379/499: loss=2.488165863146371, w0=71.40000000000013, w1=15.307140228753953\n",
      "SubSGD iter. 380/499: loss=5.783231754532338, w0=72.10000000000014, w1=16.181467841819735\n",
      "SubSGD iter. 381/499: loss=0.038313276573731514, w0=72.80000000000014, w1=16.254210602173842\n",
      "SubSGD iter. 382/499: loss=8.422052319311632, w0=73.50000000000014, w1=17.42079662281288\n",
      "SubSGD iter. 383/499: loss=0.2657042338504425, w0=74.20000000000014, w1=18.069211029424935\n",
      "SubSGD iter. 384/499: loss=3.785429304345655, w0=74.90000000000015, w1=16.943907599868933\n",
      "SubSGD iter. 385/499: loss=10.21274126413234, w0=74.20000000000014, w1=17.193014395119086\n",
      "SubSGD iter. 386/499: loss=96.38687498103346, w0=74.90000000000015, w1=16.28309294077678\n",
      "SubSGD iter. 387/499: loss=1.4639836763074072, w0=75.60000000000015, w1=16.92838902358165\n",
      "SubSGD iter. 388/499: loss=6.647314574719502, w0=76.30000000000015, w1=17.287817456531723\n",
      "SubSGD iter. 389/499: loss=0.12507980950828568, w0=77.00000000000016, w1=16.652577646085128\n",
      "SubSGD iter. 390/499: loss=4.601522709171277, w0=76.30000000000015, w1=16.297272455605345\n",
      "SubSGD iter. 391/499: loss=7.028774322188234, w0=75.60000000000015, w1=17.090555564141653\n",
      "SubSGD iter. 392/499: loss=2.147777310569168, w0=74.90000000000015, w1=16.29771368800552\n",
      "SubSGD iter. 393/499: loss=6.410340713557801, w0=74.20000000000014, w1=15.93144252822022\n",
      "SubSGD iter. 394/499: loss=7.559961923572885, w0=74.90000000000015, w1=17.098028548859258\n",
      "SubSGD iter. 395/499: loss=4.384098682660934, w0=74.20000000000014, w1=16.475795023660922\n",
      "SubSGD iter. 396/499: loss=5.373600643208803, w0=73.50000000000014, w1=15.403149548091037\n",
      "SubSGD iter. 397/499: loss=4.317428803543486, w0=72.80000000000014, w1=15.726684892870894\n",
      "SubSGD iter. 398/499: loss=3.3578341480319907, w0=72.10000000000014, w1=16.497909657216727\n",
      "SubSGD iter. 399/499: loss=12.68823445995929, w0=71.40000000000013, w1=16.591913927224525\n",
      "SubSGD iter. 400/499: loss=2.787752907722634, w0=70.70000000000013, w1=17.05916898097398\n",
      "SubSGD iter. 401/499: loss=0.3170398990311867, w0=70.00000000000013, w1=16.221725675434826\n",
      "SubSGD iter. 402/499: loss=0.34711968522134384, w0=69.30000000000013, w1=16.378017213972107\n",
      "SubSGD iter. 403/499: loss=0.43742586298603925, w0=70.00000000000013, w1=15.617011940870384\n",
      "SubSGD iter. 404/499: loss=7.520909106162577, w0=69.30000000000013, w1=15.00808529603727\n",
      "SubSGD iter. 405/499: loss=5.955779963441991, w0=68.60000000000012, w1=15.404373051437044\n",
      "SubSGD iter. 406/499: loss=10.062092956771735, w0=69.30000000000013, w1=15.009501237085102\n",
      "SubSGD iter. 407/499: loss=17.945577505772093, w0=70.00000000000013, w1=16.232794686702544\n",
      "SubSGD iter. 408/499: loss=1.9758590478680844, w0=69.30000000000013, w1=16.288926381032887\n",
      "SubSGD iter. 409/499: loss=1.0530847302086954, w0=68.60000000000012, w1=16.676924714455552\n",
      "SubSGD iter. 410/499: loss=2.99990633603349, w0=69.30000000000013, w1=17.210888983652705\n",
      "SubSGD iter. 411/499: loss=1.889926877535629, w0=70.00000000000013, w1=17.935744345028592\n",
      "SubSGD iter. 412/499: loss=4.271941378429439, w0=69.30000000000013, w1=18.376861459195805\n",
      "SubSGD iter. 413/499: loss=3.580095357327778, w0=70.00000000000013, w1=19.02527586580786\n",
      "SubSGD iter. 414/499: loss=3.7077482852628236, w0=69.30000000000013, w1=18.15048095605144\n",
      "SubSGD iter. 415/499: loss=0.280371072438129, w0=70.00000000000013, w1=18.772714481249775\n",
      "SubSGD iter. 416/499: loss=12.881120593948765, w0=70.70000000000013, w1=18.129384425900884\n",
      "SubSGD iter. 417/499: loss=0.4611821877685287, w0=70.00000000000013, w1=17.404529064524997\n",
      "SubSGD iter. 418/499: loss=1.491551215772006, w0=70.70000000000013, w1=17.648776671019903\n",
      "SubSGD iter. 419/499: loss=2.730366955186497, w0=70.00000000000013, w1=18.16750020481325\n",
      "SubSGD iter. 420/499: loss=3.3626032640143393, w0=70.70000000000013, w1=17.587789145611143\n",
      "SubSGD iter. 421/499: loss=2.575543269833048, w0=71.40000000000013, w1=17.576358707684697\n",
      "SubSGD iter. 422/499: loss=4.7024141970256395, w0=70.70000000000013, w1=17.69998539081159\n",
      "SubSGD iter. 423/499: loss=0.1793806561108795, w0=70.00000000000013, w1=16.185219233630896\n",
      "SubSGD iter. 424/499: loss=5.756710558847104, w0=70.70000000000013, w1=16.8047909968274\n",
      "SubSGD iter. 425/499: loss=1.3386381154512605, w0=71.40000000000013, w1=15.770789920852163\n",
      "SubSGD iter. 426/499: loss=5.204114999402464, w0=72.10000000000014, w1=16.645117533917944\n",
      "SubSGD iter. 427/499: loss=5.831559903206511, w0=71.40000000000013, w1=16.450074048021104\n",
      "SubSGD iter. 428/499: loss=4.719429370856865, w0=70.70000000000013, w1=16.664385698088243\n",
      "SubSGD iter. 429/499: loss=8.4012563092367, w0=71.40000000000013, w1=15.374368029419118\n",
      "SubSGD iter. 430/499: loss=1.6792607109765783, w0=72.10000000000014, w1=15.991543496354552\n",
      "SubSGD iter. 431/499: loss=2.0497336385733576, w0=72.80000000000014, w1=17.097562017996605\n",
      "SubSGD iter. 432/499: loss=12.185714727323777, w0=72.10000000000014, w1=17.020797928448808\n",
      "SubSGD iter. 433/499: loss=0.10319103567016441, w0=72.80000000000014, w1=16.844471080365125\n",
      "SubSGD iter. 434/499: loss=0.5964116883078532, w0=73.50000000000014, w1=16.76947671625411\n",
      "SubSGD iter. 435/499: loss=1.422791564339235, w0=72.80000000000014, w1=16.696733955900005\n",
      "SubSGD iter. 436/499: loss=7.585084906733883, w0=73.50000000000014, w1=16.909215126478664\n",
      "SubSGD iter. 437/499: loss=2.599711440188429, w0=72.80000000000014, w1=16.758780970543835\n",
      "SubSGD iter. 438/499: loss=3.1254699302419198, w0=73.50000000000014, w1=17.404077053348708\n",
      "SubSGD iter. 439/499: loss=6.665307998882625, w0=72.80000000000014, w1=16.061154558949703\n",
      "SubSGD iter. 440/499: loss=7.23719911980271, w0=73.50000000000014, w1=15.849795869218935\n",
      "SubSGD iter. 441/499: loss=0.7677681427734981, w0=74.20000000000014, w1=15.196880654124902\n",
      "SubSGD iter. 442/499: loss=2.408375792997859, w0=73.50000000000014, w1=14.149394804086095\n",
      "SubSGD iter. 443/499: loss=4.099938433602915, w0=72.80000000000014, w1=14.76383047885638\n",
      "SubSGD iter. 444/499: loss=2.361588028395552, w0=72.10000000000014, w1=15.159360449955555\n",
      "SubSGD iter. 445/499: loss=4.343075235574645, w0=72.80000000000014, w1=15.59588604236752\n",
      "SubSGD iter. 446/499: loss=0.5774341058816503, w0=72.10000000000014, w1=14.835814343415233\n",
      "SubSGD iter. 447/499: loss=5.598159690028467, w0=72.80000000000014, w1=15.481110426220106\n",
      "SubSGD iter. 448/499: loss=3.5385201710918324, w0=72.10000000000014, w1=14.435091965990589\n",
      "SubSGD iter. 449/499: loss=6.3893842971949795, w0=72.80000000000014, w1=14.073516169708634\n",
      "SubSGD iter. 450/499: loss=5.73337652438196, w0=72.10000000000014, w1=14.733138172607442\n",
      "SubSGD iter. 451/499: loss=5.318882243561092, w0=72.80000000000014, w1=15.359592148683175\n",
      "SubSGD iter. 452/499: loss=0.6472540221534899, w0=73.50000000000014, w1=15.542476617149054\n",
      "SubSGD iter. 453/499: loss=1.034729736508865, w0=72.80000000000014, w1=15.008512347951902\n",
      "SubSGD iter. 454/499: loss=2.4010698697788655, w0=73.50000000000014, w1=15.27260533549381\n",
      "SubSGD iter. 455/499: loss=6.786140082547028, w0=72.80000000000014, w1=15.769421830255316\n",
      "SubSGD iter. 456/499: loss=2.2907516449303458, w0=72.10000000000014, w1=16.80342290623055\n",
      "SubSGD iter. 457/499: loss=0.22191142813025522, w0=72.80000000000014, w1=17.158728096710334\n",
      "SubSGD iter. 458/499: loss=2.0315557216021176, w0=72.10000000000014, w1=17.997962557602225\n",
      "SubSGD iter. 459/499: loss=8.073005463480584, w0=71.40000000000013, w1=18.2521531129462\n",
      "SubSGD iter. 460/499: loss=3.1143850221184124, w0=72.10000000000014, w1=18.688678705358164\n",
      "SubSGD iter. 461/499: loss=2.645260059458309, w0=72.80000000000014, w1=18.587771425834333\n",
      "SubSGD iter. 462/499: loss=99.59990261795144, w0=73.50000000000014, w1=17.677849971492027\n",
      "SubSGD iter. 463/499: loss=5.778947226831036, w0=72.80000000000014, w1=17.17531531079292\n",
      "SubSGD iter. 464/499: loss=5.897389868397717, w0=72.10000000000014, w1=17.38962696086006\n",
      "SubSGD iter. 465/499: loss=1.2628022060099298, w0=71.40000000000013, w1=18.150632233961783\n",
      "SubSGD iter. 466/499: loss=0.7127490189959147, w0=72.10000000000014, w1=17.379407469615952\n",
      "SubSGD iter. 467/499: loss=3.9839143075170114, w0=71.40000000000013, w1=17.435539163946295\n",
      "SubSGD iter. 468/499: loss=9.279672235364657, w0=70.70000000000013, w1=16.49563364439978\n",
      "SubSGD iter. 469/499: loss=2.4504478148687667, w0=71.40000000000013, w1=16.67851811286566\n",
      "SubSGD iter. 470/499: loss=4.472149524400216, w0=72.10000000000014, w1=16.043278302419065\n",
      "SubSGD iter. 471/499: loss=6.014768258904368, w0=72.80000000000014, w1=16.21364841796515\n",
      "SubSGD iter. 472/499: loss=3.716693210005829, w0=72.10000000000014, w1=16.873270420863957\n",
      "SubSGD iter. 473/499: loss=5.893069864616578, w0=72.80000000000014, w1=16.931598748691176\n",
      "SubSGD iter. 474/499: loss=9.160233476031763, w0=72.10000000000014, w1=17.18578930403515\n",
      "SubSGD iter. 475/499: loss=6.860930836796314, w0=71.40000000000013, w1=17.374156565481563\n",
      "SubSGD iter. 476/499: loss=0.21942386163009786, w0=70.70000000000013, w1=17.691500313591252\n",
      "SubSGD iter. 477/499: loss=3.7972644034216927, w0=71.40000000000013, w1=17.85260213731712\n",
      "SubSGD iter. 478/499: loss=5.425785748258875, w0=70.70000000000013, w1=16.509679642918115\n",
      "SubSGD iter. 479/499: loss=5.172271288852215, w0=71.40000000000013, w1=16.98406572817752\n",
      "SubSGD iter. 480/499: loss=3.2122552808480975, w0=72.10000000000014, w1=16.66402884511834\n",
      "SubSGD iter. 481/499: loss=4.748154863843368, w0=72.80000000000014, w1=17.19423734177894\n",
      "SubSGD iter. 482/499: loss=3.8089184726275533, w0=73.50000000000014, w1=17.37677924157971\n",
      "SubSGD iter. 483/499: loss=2.031546997059621, w0=72.80000000000014, w1=16.03131941618822\n",
      "SubSGD iter. 484/499: loss=0.29995968742785806, w0=72.10000000000014, w1=15.41414394925279\n",
      "SubSGD iter. 485/499: loss=3.4231487737050728, w0=71.40000000000013, w1=16.25337841014468\n",
      "SubSGD iter. 486/499: loss=4.991376062334965, w0=72.10000000000014, w1=16.898674492949553\n",
      "SubSGD iter. 487/499: loss=1.988882689145214, w0=72.80000000000014, w1=17.357163956326936\n",
      "SubSGD iter. 488/499: loss=0.5842701427966048, w0=72.10000000000014, w1=17.971599631097224\n",
      "SubSGD iter. 489/499: loss=5.944050893100645, w0=72.80000000000014, w1=17.456768876604688\n",
      "SubSGD iter. 490/499: loss=11.654234396873171, w0=73.50000000000014, w1=16.87890932892162\n",
      "SubSGD iter. 491/499: loss=3.1959127456543683, w0=72.80000000000014, w1=16.145083803762564\n",
      "SubSGD iter. 492/499: loss=8.186968477452922, w0=73.50000000000014, w1=16.371927815409737\n",
      "SubSGD iter. 493/499: loss=0.4503464587176609, w0=72.80000000000014, w1=16.15354220074144\n",
      "SubSGD iter. 494/499: loss=3.2857556390798237, w0=72.10000000000014, w1=14.962521975776156\n",
      "SubSGD iter. 495/499: loss=1.4181281346784402, w0=72.80000000000014, w1=15.687377337152043\n",
      "SubSGD iter. 496/499: loss=4.7915376120264455, w0=72.10000000000014, w1=16.154632390901497\n",
      "SubSGD iter. 497/499: loss=5.215274343812403, w0=72.80000000000014, w1=15.61052754083063\n",
      "SubSGD iter. 498/499: loss=0.9618515455727419, w0=72.10000000000014, w1=14.988294015632292\n",
      "SubSGD iter. 499/499: loss=2.458430426756621, w0=72.80000000000014, w1=14.832676046463087\n",
      "SubSGD: execution time=0.022 seconds\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = 500\n",
    "gamma = 0.7\n",
    "batch_size = 1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start SubSGD.\n",
    "start_time = datetime.datetime.now()\n",
    "subsgd_losses, subsgd_ws = stochastic_subgradient_descent(\n",
    "    y, tx, w_initial, batch_size, max_iters, gamma\n",
    ")\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SubSGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c9e1ae6f2884a949be3261ea7de242b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=501, min=1), Output()), _dom_classes=('widg…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ipywidgets import IntSlider, interact\n",
    "\n",
    "\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        subsgd_losses,\n",
    "        subsgd_ws,\n",
    "        grid_losses,\n",
    "        grid_w0,\n",
    "        grid_w1,\n",
    "        mean_x,\n",
    "        std_x,\n",
    "        height,\n",
    "        weight,\n",
    "        n_iter,\n",
    "    )\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(subsgd_ws)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
